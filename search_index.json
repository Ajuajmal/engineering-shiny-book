[
["index.html", "Engineering Production-Grade Shiny Apps Introduction Motivation Book structure About the authors Want to help? Other resources Disclaimer Software information and conventions Acknowledgments", " Engineering Production-Grade Shiny Apps Colin Fay, Sébastien Rochette, Vincent Guyader, Cervan Girard 2020-04-24 Introduction img.left { float: left; margin-right: 1em; } This book is currently under development. It will be published in 2020 in the R Series by Chapman &amp; Hall. Motivation This book will not get you started with Shiny, nor talk about how to deploy into production and scale your app. What we’ll see is the process of building the app. Why? Lots of blog posts and books talk about starting to use shiny or putting apps in production. Very few (if any) talk about this grey area between getting started and pushing into production. So this is what this book is going to talk about: building Shiny application. We’ll focus on the process, the workflow, and the tools we use at ThinkR when building big Shiny Apps. Hence, if you are starting to read this book, we assume you have a working knowledge of how to build a small application, and want to know how to go one step further. Book structure Part 1 (Building big Shiny Apps) gives a general context about what is a successful Shiny Apps and what challenges arise when you are dealing with a large scale application. It also introduces the {golem} package and more general concept about organising your workflow. This first part will be light on code: you will read more about conceptual ideas and organisation strategy. Part 2 to 6 describes a series of steps you can take when you want to build a Shiny App, and the tooling associated with each step. Part 7 (Optimizing) tackles the question of optimization, first by presenting some common caveats of Shiny Apps, and then showing how to optimize R code, and use JavaScript to lighten R work. About the authors Colin Fay Colin FAY works at ThinkR, a french agency focused on everything R-related. During the day, he helps companies to take full advantage of the power of R, by providing training (from beginner to expert), tools (packages, web apps…) and infrastructure. His main areas of expertise are data &amp; software engineering, web applications (frontend and backend), and R in production. During the night, Colin is also an hyperactive open source developer and an open data advocate. You can find a lot of his work on his GitHub account (https://github.com/ColinFay) and on ThinkR’s account (https://github.com/thinkr-open). He is also active in the Data Science community in France, especially in his home town Rennes, where he founded the data-blogging website Data-Bzh.fr, co-founded the Breizh Data Club association, and organises the Breizh Data Club Meetups. Sébastien Rochette Sébastien is a data scientist at ThinkR, where he teaches anything R related from beginner to expert level, guides R developers towards implementation of best practices, and creates tailor-made R solutions for the needs of his customers. Vincent Guyader ThinkR Founder, with more than 10 years of experience, and with a scientific and technical background, Vincent is an R-enthusiast. He still has his hands in the code, whether to develop applications, analyze data or build packages. When he’s not coding, he plays with Docker and manages servers. Meeting highly technical challenges is not incompatible with pedagogy: he passionately trains very diverse learner profiles at R. Cervan Girard Want to help? Any feedbacks on the book is very welcome. Feel free to open an issue, or to make a PR if you spot a typo (I’m not a native english speaker, so there might be some waiting to be found ;) ). Other resources Getting started with Shiny Learn Shiny with RStudio Getting Started with Shiny (WIP) Mastering Shiny A gRadual intRoduction to Shiny Shiny into production Shiny in production: Principles, practices, and tools Shiny in Production Suggest a Resource! Disclaimer Open source is moving (very) fast, and some of the tools described in this book are still under active development. But good news! A large portion of this book is about the methodology, and not purely the technology, so even if some of the packages and code described in this book become obsolete, a significant part of what is described in this book will still be relevant. When reading this book, remember that they are a “snapshot” of various technologies and packages, which might some day become out of date, have breaking changes, or even disappear. We will try to update the online version whenever changes are made to the codebase of the packages used in this book, so feel free to browse the online version. The current version of this book has been compiled on: Sys.Date() [1] &quot;2020-04-24&quot; With the following configuration: See Session Information xfun::session_info() R version 3.6.3 (2020-02-29) Platform: x86_64-apple-darwin15.6.0 (64-bit) Running under: macOS Catalina 10.15.4 Locale: en_US.UTF-8 / en_US.UTF-8 / en_US.UTF-8 / C / en_US.UTF-8 / en_US.UTF-8 Package version: askpass_1.1 assertthat_0.2.1 attempt_0.3.0 backports_1.1.6 base64enc_0.1.3 BH_1.72.0.3 bookdown_0.18 brew_1.0.6 callr_3.4.3 cli_2.0.2 clipr_0.7.0 commonmark_1.7 compiler_3.6.3 config_0.3 crayon_1.3.4 curl_4.3 desc_1.2.0 digest_0.6.25 dockerfiler_0.1.3 ellipsis_0.3.0 evaluate_0.14 fansi_0.4.1 fastmap_1.0.1 fs_1.4.1 gh_1.1.0 git2r_0.26.1 glue_1.4.0 golem_0.2.1 graphics_3.6.3 grDevices_3.6.3 here_0.1 highr_0.8 htmltools_0.4.0 httpuv_1.5.2 httr_1.4.1 ini_0.3.1 jsonlite_1.6.1 knitr_1.28 later_1.0.0 lifecycle_0.2.0 magrittr_1.5 markdown_1.1 methods_3.6.3 mime_0.9 openssl_1.4.1 pillar_1.4.3 pkgbuild_1.0.6 pkgconfig_2.0.3 pkgload_1.0.2 praise_1.0.0 prettyunits_1.1.1 processx_3.4.2 promises_1.1.0 ps_1.3.2 purrr_0.3.4 R6_2.4.1 Rcpp_1.0.4.6 rematch2_2.1.1 remotes_2.1.1 rlang_0.4.5 rmarkdown_2.1 roxygen2_7.1.0 rprojroot_1.3-2 rstudioapi_0.11 shiny_1.4.0.2 sourcetools_0.1.7 stats_3.6.3 stringi_1.4.6 stringr_1.4.0 sys_3.3 testthat_2.3.2 tibble_3.0.1 tinytex_0.22 tools_3.6.3 usethis_1.6.0 utf8_1.1.4 utils_3.6.3 vctrs_0.2.4 whisker_0.4 withr_2.2.0 xfun_0.13 xml2_1.3.2 xtable_1.8-4 yaml_2.2.1 Software information and conventions The book has been built with the knitr package (???) and the bookdown package (Xie 2020) to compile my book. Package names are in curly brackets in code format (e.g., {rmarkdown}), and inline code and filenames are formatted in a typewriter font (e.g., knitr::knit('foo.Rmd')). Function names are followed by parentheses (e.g., bookdown::render_book()). Acknowledgments Contributors to this book Liz Roten, for proofreading chapter 1 and helping us with writing advices. Dan S. Reznik, for proof-reading the JavaScript chapter. References "],
["successfulshinyapp.html", "Chapter 1 About Successful Shiny Apps 1.1 A (very) short introduction to Shiny 1.2 What is a complex Shiny Application? 1.3 What is a successful Shiny App?", " Chapter 1 About Successful Shiny Apps I Too Like to Live Dangerously .right{ text-align: right;} Austin Powers 1.1 A (very) short introduction to Shiny If you are reading this page, chances are you already know what a Shiny application (shorten “Shiny app”) is—a web application that communicates with R, built in R, and working with R. The beauty of {shiny} is that it makes it easy for someone already familiar with R to create a small app in a matter of hours. With small and minimal Shiny apps, no knowledge of HTML, CSS or JavaScript is required, and you do not have to think about technical elements that usually come with web applications—for example, you do not have to think about the port the application is served on: {shiny} picks one for you1. Same goes for serving external dependencies: the application comes with its set of CSS and JavaScript dependencies that a common Shiny developer does not need to worry about. And that is probably one of the main reason why this package has become so successful over the years—with very little training, you can rapidly create a proof-of-concept (PoC) for a data-product, showcase an algorithm, or present your results in an elegant and accessible user interface. The first version of {shiny} was published in 2012. Since then, it has been one of the top project of the RStudio team. At the time of writing these lines (April 2020), there are more than 4700 commits in the master branch of the GitHub repository, made by 46 contributors. It is now downloaded around 400K times a month, according to cranlogs, and has 733 reverse dependencies (i.e packages that depend on it), according to devtools::revdep(\"shiny\"). If you are very new to Shiny, this book might feel a little bit overwhelming: we will be discussing some advance {shiny} production methods, best practices and structural idea for sending Shiny applications to production. This books relies on the assumption that you already know how to build basic Shiny applications, and that you want to push your Shiny skills to the next level: in other words, you are ready to move from the Proof of Concept to the production-grade application. If you are very new to Shiny, we suggest you start with the Mastering Shiny book before reading the present book. Ready to start engineering production-grade Shiny Apps? 1.2 What is a complex Shiny Application? One of the unfortunate things about reality is that it often poses complex problems that demand complex solutions .right{ text-align: right;} The Art of Unix Programming 1.2.1 Reaching the cliff of complexity Building a Shiny application seems quite straightforward when it comes to small prototypes or proof of concepts: after a few hours of practice and documentation reading, most R developers can have a small working application. But things change when your application reaches “the cliff of complexity”2, i.e that moment when the application reaches a states when it can be qualified as “complex”. But what do we mean by complexity? Getting a clear definition is not an easy task3 as it very much depends on who is concerned and who you are talking to. But a good definition can be found in The DevOps Handbook: “One of the defining characteristics of a complex system is that it defies any single person’s ability to see the system as a whole and understand how all the pieces fit together. Complex system typically have a high degree of interconnectedness of tightly coupled components, and system-level behavior cannot be explained merely in terms of the behavior of the system components.” (we underlined). Building on top of this quote, let’s try to come up with a definition that will serve us in the context of engineering Shiny applications. When building software, we can think of complexity from two points of view: the complexity as it is seen by the developer, and the complexity as it is seen by the customer/end-user.4 For the code, bugs are harder to anticipate: it is hard to think about all the different paths the software can follow and difficult to identify bugs because they are deeply nested in the numerous routines the app is doing. It is also hard to think about what the state of your app is at a given moment because of the numerous inputs and outputs your app contains. From the user perspective, the more complex an app is, the steeper the learning curve. Indeed, the user will have to invest more time learning how the app works, and will be even more disappointed if ever they realize this time has been a waste. Let’s dive a little bit more in these two type of complexity. 1.2.1.1 Developers complexity An app is to be considered complex when it is so large in terms of size and functionality that it makes it impossible to reason about it at once, and developer must rely on tools and methods to understand and handle this complexity: for example, when it comes to {shiny}, you will rely on tools like the {golem} framework, introduced all along this book, to handle some implementation, development and deployment complexity. This book will introduce a clear methodology that comes with a series of conventions, which are crucial when it comes to building and maintaining complex systems: by imposing a formalized structure for a software, it enhances its readability, lower the learning curve for new-comers, and reduces the risk of errors inherent to repetitive tasks. This type of complexity is called implementation complexity. One of the goal of this book is to present you a methodology and toolkit that will help you reduce this form of complexity. 1.2.1.2 Customers and users complexity Customers and end user see complexity as interface complexity. Interface complexity can be driven by a lot of elements, for example the probability of making an error while using the app, the difficulty to understand the logical progression in the app, the presence of unfamiliar behaviour or terms, visual distractions… This book will also bring you strategy to help you cope with the need for simplification when it comes to designing interface. 1.2.2 Balancing complexities There is an inherent tension between these two source of complexity, as designing an app means finding a good balance between implementation and interface complexity. Lowering one source of complexity usually means increasing the other, and managing an application project means knowing where to draw the line. This usually requires restraining yourself from implementing too much features, and still create an application that is easy to use, and that fits the requirements you have received. For example, there is something common in Shiny application: what we can call the “too much reactivity pattern”. In some cases, developers try to make everything reactive: e.g., three sliders and a dropdown input, all updating a single plot. This behavior lowers the interface complexity: users do not have to really think about what they are doing, they move sliders, change the inputs, and boom! the plot updates. But this kind of pattern can make the application perform too much computation, for example because users rarely go to the value they need on their first try: they usually miss what value they actually want to select. One solution can be to delay reactivity or to cache things so that R computes fewer things. But that comes with a cost: handling delayed reactivity and caching elements increases implementation complexity. Another solution is to add an “update plot” button, which updates the plot only when the user clicks on it. This pattern makes it easier to control reactivity from an implementation side. But this can make the interface a little bit more complex for the users, who have to perform another action, on top of changing their inputs. We will argue somewhere else in the book that not enough reactivity is better than too much reactivity, as the latter increases computation time, and relies on the assumption that the user makes the right action on the first try. Another good example is {shiny}’s dateRangeInput() function. This function requires the user to choose a starting date and an ending date. However, the function allows to choose a start date which is posterior to the end (that is the behavior of the JavaScript plugin used in {shiny} to create this input). But allowing this behavior leads to bugs, notably in a context of full reactivity. Handling this special case is completely doable: with a little bit of craft, you can watch what the user inputs and throw an error if the start is after the end.5 On one hand that solution increases implementation complexity, while on the other leaving this naive behavior requires the user to think carefully about what they are selecting, thus increasing the interface complexity. So what should we do? It’s up to you: deciding where to draw the line between interface &amp; implementation complexity very much depends of the project, but that is something that you should keep in mind all along the project’s life. 1.2.3 Assessing Code Complexity On the developer side, you will want to reduce code complexity so that everybody involved in the coding process is able to create a mental model of how the application work. On the user side, you will want to reduce interface complexity so that everybody comes out of using your application with a good user-experience. Reducing complexity first comes with being able to identify its potential sources, be it in your application code base or in the specifications describing how the application should work. Finding these sources of complexity is not an easy task, as it requires some programming knowledge to identify bottlenecks, basic UX (User Experience) skills to implement a good interface, and of course a project management methodology to structure the whole life of your application. All these points will be addressed in this book. But before that, let’s dive into code complexity. 1.2.3.1 Codebase size The total number of lines of code, or the number of files, can be good clue of potential complexity, but only if used as order ofs magnitude (for example a 10.000-line codebase is potentially more complex than a 100-line codebase), but should not be relied on if used strictly: even more if you try to reduce this number of line by sacrificing code readability. R is very permissive when it comes to indentation and line breaks, and, unlike JavaScript or CSS, it is generally not minified6. In R, the number of lines of code depends on your coding style and the packages you are using. For example, the {tidyverse} style guide encourage the use of the pipe (%&gt;%)7 with one function by line, producing more lines in the end code. So you can expect a “tidyverse-centric” package to contain more line of code, yet the pipe itself has been thought as a tool to lower code complexity by enhancing its readability8. For example, the two following pieces of code do the same thing. Both have a different number of lines, and a different level of reading complexity. library(dplyr, warn.conflicts = FALSE) # With pipe iris %&gt;% group_by(Species) %&gt;% summarize(m_sp = mean(Sepal.Length)) [90m# A tibble: 3 x 2[39m Species m_sp [3m[90m&lt;fct&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [90m1[39m setosa 5.01 [90m2[39m versicolor 5.94 [90m3[39m virginica 6.59 # Without summarize(group_by(iris, Species), m_sp = mean(Sepal.Length)) [90m# A tibble: 3 x 2[39m Species m_sp [3m[90m&lt;fct&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [90m1[39m setosa 5.01 [90m2[39m versicolor 5.94 [90m3[39m virginica 6.59 Also, there is also no limit in the way you can indent your code. iris[ 1 : 5, &quot;Species&quot; ] [1] setosa setosa setosa setosa setosa Levels: setosa versicolor virginica Six lines of code for something that could also be written in one line. iris[1:5, &quot;Species&quot;] [1] setosa setosa setosa setosa setosa Levels: setosa versicolor virginica In other words, using this kind of writing style can make the code base larger in term of lines, without really adding complexity to the general program. Another drawback of this metric is that it focuses on numbers instead of readability, and in the long run, yes, readability matters As noted in The Art of Unix Programming, “Chapter 13: Speaking of Complexity”, “Pressure to keep the codebase size down by using extremely dense and complicated implementation techniques can cause a cascade of implementation complexity in the system, leading to an un-debuggable mess”. Still, this metric can be useful to reinforce what you have learned from other metrics. It is rather unlikely that you will find this “extreme” coding style we showed above, and even if it might not make sense to compare two code base that just differ by 1 or 2 % of lines of code, it is very likely that a code base which is ten, one hundred, one thousand times larger is a more complex software. Another good metric related to this metric is the number of files in the project: R developers tend to split their functions into several files, so the more files you will find in a project, the larger the code base is. And numerous files can also be a sign of maintenance complexity, as it may be harder to reason about an app logic split into several files than about something that fits into one linear code inside one file9. On the other hand, one big 10.000-lines file which is standing alone in the project is not a good sign either. If you want to use the number of lines metric, you can do it from R with the {cloc} package, available at https://github.com/hrbrmstr/cloc. if (!requireNamespace(&quot;cloc&quot;)){ remotes::install_github(&quot;hrbrmstr/cloc&quot;) } For example, let’s compare a rather big package ({shiny}) with a small one ({attempt}): library(cloc) library(dplyr) shiny_cloc &lt;- cloc_cran(&quot;shiny&quot;, .progress = FALSE, repos = &quot;http://cran.irsn.fr/&quot; ) attempt_cloc &lt;- cloc_cran(&quot;attempt&quot;, .progress = FALSE, repos = &quot;http://cran.irsn.fr/&quot; ) clocs &lt;- bind_rows( shiny_cloc, attempt_cloc ) # Counting lines of code clocs %&gt;% group_by(pkg) %&gt;% summarise( loc = sum(loc) ) [90m# A tibble: 2 x 2[39m pkg loc [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;int&gt;[39m[23m [90m1[39m attempt [4m4[24m011 [90m2[39m shiny [4m7[24m[4m1[24m493 # Counting files clocs %&gt;% group_by(pkg) %&gt;% summarise( files = sum(file_count) ) [90m# A tibble: 2 x 2[39m pkg files [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;int&gt;[39m[23m [90m1[39m attempt 35 [90m2[39m shiny 269 Here, with these two metrics, we can safely assume that {shiny} is a more complex package than {attempt}. 1.2.3.2 Cyclomatic complexity Cyclomatic complexity is a software engineering measure which allows to define the number of different linear paths a piece of code can take. The higher the number of paths, the harder it can be to have a clear mental model ot this function. Cyclomatic complexity is computed based on a control-flow graph10 representation of an algorithm. (The following paragraph details the algorithm implementation, feel free to skip it) The complexity number is then computed by taking the number of nodes, subtracting the number of edges, plus two times the number of connected components of this graph. The algorithm is then \\(M = E − N + 2P\\), where \\(M\\) is the measure, \\(E\\) the number of edges, \\(N\\) the number of nodes and \\(2P\\) is 2 time the number of connected components. We will not go deep into this topic, as there are a lot things going on in this computation and you can find many documentation about this online. Please refer to the bibliography for further readings about the theory behind this measurement. In R, the cyclomatic complexity can be computed using the {cyclocomp} package. You can get it from CRAN with: install.packages(&quot;cyclocomp&quot;) The {cyclocomp} package comes with three main functions: cyclocomp(), cyclocomp_package(), and cyclocomp_package_dir(). The one we will be interested in is cyclocomp_package_dir(): building successful shiny apps with the {golem} framework (we will get back on that later) means you will be building your app as a package. Then, you will be able to use cyclocomp_package_dir() to compute the complexity of your app. Here is for example the cyclomatic complexity of the default golem template: library(cyclocomp) cyclocomp_package(&quot;golex&quot;) %&gt;% head() name cyclocomp 1 app_server 1 2 app_ui 1 3 golem_add_external_resources 1 4 run_app 1 And the one from another small application: cyclocomp_package(&quot;tidytuesday201942&quot;) %&gt;% head() name cyclocomp 24 mod_dataviz_ui 8 23 mod_dataviz_server 7 35 rv 6 14 display 4 39 undisplay 4 37 tagRemoveAttributes 3 And, finally, the one for {shiny}: cyclocomp_package(&quot;shiny&quot;) %&gt;% head() name cyclocomp 494 untar2 75 115 diagnoseCode 54 389 runApp 50 150 find_panel_info_non_api 37 371 renderTable 37 102 dataTablesJSON 34 And, bonus, this cyclocomp_package() function can also be used to retrieve the number of functions inside the package. So, as The Clash said, “what are we gonna do now?” You might have heard this saying: “if you copy and paste a piece of code twice, you should write a function”. And indeed, splitting code into smaller pieces lower the local cyclomatic complexity, as smaller functions have lower cyclomatic complexity, but that is just at a local level, and it can be a suboptimal option: having a very large number of functions can make it harder to navigate in the codebase. But that is not a magic solution: (A) because the global complexity of the app is not lowered by splitting things into pieces (just local complexity), (B) because the deeper the call stack, the harder it can be to debug. 1.2.3.3 Other measures for code complexity Complexity can come from other sources: insufficient code coverage, dependencies that breaks the implementation, relying on old packages, or a lot of other things. We can use the {packageMetrics2} package to get some of these metrics: for example, the number of dependencies, the code coverage, the number of releases and the date of the last one, etc., … and the number of lines of code and the cyclomatic complexity. library(packageMetrics2) frame_metric &lt;- function(pkg){ metrics &lt;- package_metrics(pkg) tibble::tibble( n = names(metrics), val = metrics, expl = list_package_metrics()[names(metrics)] ) } {golem} frame_metric(&quot;golem&quot;) %&gt;% knitr::kable() n val expl ARR 0 Number of times = is used for assignment ATC 85.1549755301794 Author Test Coverage DWL 8417 Number of Downloads DEP 27 Num of Dependencies DPD 1 Number of Reverse-Dependencies CCP 2.44444444444444 Cyclomatic Complexity FLE 3.08991228070175 Average number of code lines per function FRE 2019-08-05T14:50:02+00:00 Date of First Release LIB 0 Number of library and require calls LLE 40 Number of code lines longer than 80 characters LNC 0 Number of lines of compiled code LNR 1464 Number of lines of R code LRE 2019-08-05T14:50:02+00:00 Date of Last Release NAT 0 Number of attach and detach calls NTF 0 Number of times T/F is used instead of TRUE/FALSE NUP 0 Updates During the Last 6 Months OGH 1 Whether the package is on GitHub SAP 1 Number of sapply calls SEM 0 Number of trailing semicolons in the code SEQ 0 Number of 1:length(vec) expressions SWD 18 Number of setwd calls VIG 0 Number of vignettes {shiny} frame_metric(&quot;shiny&quot;) %&gt;% knitr::kable() n val expl ARR 14 Number of times = is used for assignment ATC 29.2341145204156 Author Test Coverage DWL 12238097 Number of Downloads DEP 22 Num of Dependencies DPD 722 Number of Reverse-Dependencies CCP 3.70817843866171 Cyclomatic Complexity FLE 2.34588807785888 Average number of code lines per function FRE 2012-12-01T07:16:17+00:00 Date of First Release LIB 1 Number of library and require calls LLE 495 Number of code lines longer than 80 characters LNC 0 Number of lines of compiled code LNR 24737 Number of lines of R code LRE 2019-10-10T11:50:02+00:00 Date of Last Release NAT 0 Number of attach and detach calls NTF 0 Number of times T/F is used instead of TRUE/FALSE NUP 1 Updates During the Last 6 Months OGH 1 Whether the package is on GitHub SAP 10 Number of sapply calls SEM 1 Number of trailing semicolons in the code SEQ 0 Number of 1:length(vec) expressions SWD 6 Number of setwd calls VIG 0 Number of vignettes If you are building you Shiny application with {golem}, you can use the DESCRIPTION file, which contains the list of dependencies, as a starting point to explore these metrics for your dependencies: desc::desc_get_deps(&quot;golex/DESCRIPTION&quot;) type package version 1 Imports config * 2 Imports golem * 3 Imports shiny * # See also attachment::att_from_description(&quot;golex/DESCRIPTION&quot;) [1] &quot;config&quot; &quot;golem&quot; &quot;shiny&quot; 1.2.4 Production Grade Software Engineering The use of R has evolved since its initial version released in 1995, using it 25 years later, as a tool to build softwares released in production, is a challenge. Complexity is still frowned upon by a lot of developers, notably because it has been seen as something to avoid according to the Unix philosophy. But there are dozens of reasons why an app can become complex: for example, the question your app is answering is quite complicated and involves a lot of computation and routines. The resulting app is rather ambitious and implements a lot of features, etc. So yes, there is a chance that if you are reading this page, you are working or are planning to work on a complex Shiny app. And this is not necessarily a bad thing! Shiny apps can definitely be used to implement production-grade11 software, but production-grade software implies production-grade software engineering. To make your project a success, you need to use tools that reduce the complexity of your app and ensure that your app is resilient to aging. In other words, production-grade Shiny apps requires working with a software engineering mindset, which is not always an easy task in the R world: many R developers have learned this language as a tool for doing data analysis, building model, making statistics, not really as a tool for building software. And still today, for a lot of R users, the software is still used as an “experimentation tool”, where production quality is one of the least concerns. But the rise of {shiny} (among other packages) has drastically changed the potential of R as a language for production software engineering: its ease of use is also one of the reason why the language is now used outside academia, in more “traditional” software engineering teams. This changing context requires different mindsets, skills, and tools. With {shiny}, as we said before, it is quite easy to prototype a simple app, without any “hardcore” software engineering skills. And when we are happy with our little proof of concept, we are tempted to add something new. And another. And another. And without any structured methodology, we are almost certain to reach the cliff of complexity very soon and end up with a codebase that is hardly (if ever) ready to be refactored to be sent to production. The good news is that building a complex app with R (or with any other language) is not an impossible task. But this requires planning, rigor, and correct engineering. This is what this book is about: how to organize your Shiny App in a way that is time and code efficient, and how to use correct engineering to make your app a success. 1.3 What is a successful Shiny App? Your application does not have to be complex to be successful. Even more, in a world where “less is more”, the more you can reduce your application complexity, the more you will be prepared for success. So what is a successful Shiny app? Defining such a metric is not an easy task, but we can extract some common patterns when it comes to applications that would be considered successful. 1.3.1 It exists First of all, an app is successful if it was delivered. In other words, the developer team was able to move from specification to implementation to testing and to delivering. This is a very engineering-oriented definition of success, but it is a pragmatic one: an app that never reaches the state of usability is not a successful app, as something along the way has blocked the process of finishing the code. This condition implies a lot of things, butt mostly it implies that the team was able to organize itself in an efficient way, so that they were able to work together in making the project a success. Anybody that has already worked on a code base as a team knows it is not an easy task. 1.3.2 It is accurate The app was delivered, and it answers the question it is supposed to answer, or serves the purpose it is supposed to serve. Delivering is not the only thing to keep in mind: you can deliver a working app but it might not work the way it is supposed to work. Just as before, accuracy means that between the moment the idea appears in someone’s mind and the moment the app is actually ready to be used, everybody was able to work together toward a common goal, and that now this goal is reached, we are also certain that the answers we get from the application are accurate, and that users can rely on the application to make decision. 1.3.3 It is usable being usable means that the app was delivered, it serves the purpose, and it is user-friendly. Unless you are coding for art sake, there will always be one or more end users. And if these people cannot use the application because it is too hard to use, too hard to understand, because it is too slow or there is no inherent logic in how the user experience is designed, then it is inappropriate to call the app is a success. 1.3.4 It is immortal Of course “immortal” is a little bit far fetched, but when designing the application, you should aim for robustness along the years, by engineering a (theoretical) application immortality. Planning for the future is a very important component of a successful Shiny App project. Once the app is out, it is successful if it can exist in the long run, with all the hazards that this implies: new package versions that could potentially break the code base, sudden call for the implementation of new features in the global interface, changing key features of the UI or the back-end, and not to mention passing the code base along to someone who has not worked on the first version, and who is now in charge of developing the next version12. And this, again, is hard to do without effective planning and efficient engineering. from The Art of Unix Programming, “Chapter 13: Speaking of Complexity”↩︎ see shiny/issues/2043#issuecomment-525640738 for an example↩︎ By production-grade, we mean a software that can be used in a context where people use it for doing their job, and where failures or bugs have real-life consequences↩︎ "],
["planning.html", "Chapter 2 Planning Ahead 2.1 Working with a “long-term” mindset 2.2 Working as a team: tools &amp; organisation", " Chapter 2 Planning Ahead 2.1 Working with a “long-term” mindset “Rome ne fut pas faite toute en un jour” French proverb 2.1.1 Prepare for success Whatever are your ambitions for your Shiny application, you should take a little time to set robust foundations that will save a lot of time in the future. A common thing you will hear about Shiny is that it is a good prototyping tool. This can not be denied. Building a Proof of Concept (POC) for an app is relatively easy if you compare to what it needs when building applications in other languages. With Shiny, you can build a “it works on my machine” web application in a couple of hours, and show it to your team, your boss, your investors… Thanks to the way Shiny is built, you do not have to care about websocket, ports, html, JavaScript, libraries, and all the things that are elegantly bundled straight into Shiny. Hence, you can have a quick, hacky application that will work on your machine, and that, very rapidly. But that is not the way you should start. Indeed, starting with hacky foundations will lead to two possibilities: You will have to rewrite everything from scratch to have a robust application for production. If you do not want to do that, you will get stucked with a legacy code base for the app that is built on top of hacky functions. Either way, that is an heavy technical debt. The idea there is that even if Shiny is a good tool for prototyping, there is no harm in starting your application on solid ground, even for a prototype: the sooner you start with a robust framework the better, and the longer you wait the harder it gets to convert your application to a production-ready application: the larger the codebase, the harder it is to untangle everything and make it work. In this book, we will present a framework called {golem}, which is a toolbox for building production-grade Shiny applications. Even if {golem} is focused on production, there is no reason not to use it for your proof of concepts: starting a new {golem} project is relatively straightforward, and even if you do not use the advanced features, you can use it for very small apps. That way, you are getting ready for complexity, and if, one day, you need to turn this small app into a production app, the foundations are there. 2.1.2 Develop with the KISS principle The KISS principle, as “Keep It Simple, Stupid”, should drive the implementation of features in the application to allow anyone in the future, including original developers, to take over on the development. Deeply rooted in the Unix Philosophy, the KISS principle states that most systems work best if they are kept simple rather than made complicated; therefore, simplicity should be a key goal in design, and unnecessary complexity should be avoided. .right{ text-align: right;} KISS principle The story behind this principle is supposed to be that Kelly Johnson, lead engineer at the Lockheed Skunk Works, gave his workers a set of very common tools and said that the airplanes should be repairable with these tools, and these tools only. The idea behind this being that repairing an aircraft should be possible for any average engineer. This should be a principle to keep in mind when building application. Indeed, large scale Shiny projects can lead to many people working on the code base, for a long period of time. Many people means a variety of skills, with some common ground in Shiny development. So when choosing how and what to implement, try to make a rule to go for the simplest solution13, i.e. the one that any common Shiny Developer would be able to understand and maintain. If you go for an exotic solution or a complex technology, be sure that you are doing it for a good reason: unknown or hard to grasp technology reduce the chance of finding someone that will be able to maintain that piece of code in the future. 2.2 Working as a team: tools &amp; organisation Working as a team, whatever the coding project, requires adequate tools, discipline and organisation. Complex Shiny Apps usually imply that several people will work on the application. For example, at ThinkR, 3 to 4 people usually work in parallel on the same application, but there might be more people involved on larger projects. The choice of tools and organisation is crucial for a successful application. 2.2.1 From the tools point of view 2.2.1.1 Version Control &amp; Test all the things To get informed about a code break during development, you will need to write tests for your app, and use continuous integration (CI) so that you are sure this is automatically detected14. When you are working on a complex application, chances are that you will be working on it for a significant period of time, meaning that you will write code, modify it, use it, go back to it after a few weeks, change some other things, and probably break things. Breaking things is a natural process of software engineering, notably when working on a piece of code during a long period. Remember the last chapter where we defined that complex applications are too large to be understood fully? Adding code that breaks the codebase will happen with complex app, so the sooner you take measure to solve code break, the better. As you can not prevent code to break, you should at least get the tooling to: Be informed that the code has broken: this is the role of tests combined with CI Be able to identify changes between versions, and potentially, get back in time to a previous code base: this is the role of version control We will go deeper into testing and version control in chapter 14. 2.2.1.2 Small is beautiful Building an application with multiple small and independant pieces will lighten your development process and your mental load. Previous chapter introduced the notion of complexity in size, where the app grows so large that it is very hard to have a good grasp of it. A large code base implies that the safe way to work is to split the app into pieces as much as possible. Splitting a Shiny project is made possible by following two techniques: Split your app into Shiny Modules, so that your app can be though of as a tree, making it possible for every developer to concentrate on one node, and only one, instead of having to think about the global infrastructure when implementing features Extract your core “non-reactive” functions, that we will also call the “business logic”, and include them in external files, so that you can work on these outside of the app. Working on independent static files to implement features will prevent you to relaunch the whole application every time you need to add something new. We will get back to Shiny module and how to organized your project in the next chapter. 2.2.2 From the organisational point of view We recommend to define two kind of developers: a unique person (or maybe two) to be in charge of supervising the whole project and developers of single functionalities. Management of the project and the communication between the two developers types will be defined around the tools, like git and its graphical interfaces. 2.2.2.1 A person in charge As the person in charge of the development, you will have an overview of the entire project and manage the team so that all developers create pieces of the application that correctly fit together. It is hard to have the complete picture of what the app is doing everywhere. Most of the time, it is not necessary for all developers to have this complete picture. Defining a unique person in charge mentally loads a unique developer. You will have to get the whole picture: what each part of the software is doing, how to make everything working together, avoid development conflicts, and of course check that, at the end of the day, the results returned by the complete application are the correct ones. You will be the one that kicks off the project and write the first draft of the application. If you follow the workflow we will describe in this book, you will create a {golem} project, fill the information, and define the application structure by providing the main modules and the prototype of the app. Once the skeleton of the app is created, list a series of small tasks to be accomplished. We strongly suggest to use git with a graphical interface (Gitlab, Github, Bitbucket, …). Versionning with git can easily be integrated with R. The graphical interface of these services will help you manage the project. The small tasks are defined as issues in the git graphical interface and will be closed during development, while referred to specific modifications of the code (through commits). You will also be able to set continuous integration, as recommended above, to test developers work before integration in the main development branch. If you follow a git flow (described in chapter \\@(ref: secure)) version control workflow, you will also be in charge of reviewing and accepting the pull/merge requests to the main dev branch if they solve the associated issues. Note that you can also be part of the developers. This may indeed be a recommendation as you will directly face difficulties if they arise. However, you will need to be careful in the code review process by assigning someone else to check your work. Do not worry if this sounds like a foreign language to you, we will get back to this method later in this book (chapter \\@(ref: secure)). Note that other management tools can be used there: for example redmine, jira, or any issue tracking software can play this role. 2.2.2.2 Developers As a developer, you will focus on small functionnalities, make sure to protect your work with tests and verify that your implementations do not break others work. If the person in charge correctly splitted the work between developers of the team, you will probably be focusing on one or more parts of the application, but you do not need to know every single bit of what the application is doing. In a perfect world, the application is split in various Shiny Modules, one module equals one file and each member of the team will be assigned a safe development of one or more modules. It is simpler to work in this context where one developer is assigned to one module, although we know that in reality it may be a little more complex, and several members of the team might go back and forth working on a common module. But, the person in charge will be there to help make all the pieces to fit together. "],
["structure.html", "Chapter 3 Structuring your Project 3.1 Shiny App as a Package 3.2 Using Shiny Modules 3.3 Structuring your app", " Chapter 3 Structuring your Project 3.1 Shiny App as a Package Building a robust, maintenable and shareable Shiny application will be made possible by building an R package. In the next chapter you will be introduced to the {golem} package, which is an opinionated framework for building production-ready Shiny Applications—this framework will be used a lot through this book, and it relies on the idea that every Shiny application should be built as an R package. But in a world where Shiny Applications are mostly created as a series of files, why bother with a package? 3.1.1 What is in a production-grade Shiny App? You probably do not have realized it yet, but it you have built a significant (in term of code base) Shiny application, chances are you have been using a package-like strucutre without even seeing it. Think about your last Shiny which was created as a single-file (app.R) or two files app (ui.R and server.R), and what is needed there to make it a production-ready application. First of all, you wîll need to add metadata to your application. In other words, all the information that are necessary for something that runs in production; the name of the app, the version number (which is crucial to any serious, production-level project), what the application does, who to contact if something goes wrong… etc. Second, you need to find a way to handle the dependencies. Because you know, when you want to push your app into production, you can not have this conversation with I.T. people: IT: Hey, I tried to source(\"app.R\") as you said, but I got an error. R-dev: What is the error? IT: It says “could not find package ‘shiny’”. R-dev: Ah yes, you need to install {shiny}. Try to run install.packages(\"shiny\"). IT: OK nice. What else? R-dev: Let me think, try also install.packages(\"DT\")… good? Now try install.packages(\"ggplot2\"), and … […] IT: Ok, now I source the ‘app.R’, right? R-dev: Sure! IT: Ok so it says ‘could not find function runApp()’ R-dev: Ah, you got to do library(shiny) at the beginning of your script. And library(purrr), and library(jsonlite)*, and… * Which will lead to a NAMESPACE conflict on the flatten() function that can cause you some debugging headache (trust us, we have been there before). So, hey, it would be cool if we could have a Shiny app that only imports specific functions from a package, right? We can not stress it enough but dependencies matter. You need to handle them, and handle them correctly if you want to ensure a smooth deployment to production. Third, let’s say you are building a big appplication. Something with thousands of lines of code. While doing that, you can not built this large application by writting all the codebase into one, maybe two files, that is simply impossible to maintain in the long run, nor to use on a daily basis while developing. So, what to do? Let us split everything into smaller files that we can call! Og, and maybe we can store these files in a specific directory. We can name this directory R/ for instance. Last but not least, we want our app to live long and prosper, which means we need to document it: it would be nice to find a way to comment each small piece of codet to explain what these specific lines do, so that either the end users or the developers that take over the code will be able to maintain it. The other thing we need for our application to be successful on the long term is tests, so that we are sure we are not introducing any regression along development. Oh, and that would be nice if people could get a tar.gz and install it on their computer and have access to a local copy of the app! OK, so let’s sum it up what we need for our application: This app needs to have metadata and to handle dependencies correctly, which is what you get from the DESCRIPTION + NAMESPACE files of a package. Even more practical is the fact that you can do “selective namespace extraction” inside a package, i.e. you can say “I want this function from this package”. Also, this app needs to be split up in smaller .R files, stored in a specific directory, which is the way a package is organized. And I do not need to emphasize how documentation is a vital part of any package, so we solved this question too here. So is the testing toolkit. And of course, the “install everywhere” wish comes to life when a Shiny App is in a package. 3.1.2 Document and test As for any software in production, assuring the usability and sustainability of your app will be done through documentation and testing. 3.1.2.1 Documenting your app Documentatin your Shiny app is explaining features to the end users, and also to the future developers (chances are it will be you). The good news is that using the R package structure helps you leverage the common tools for documentation in R: A README file that you will put at the root of your package, which will document how to install the package, and some information about how to use the package. Note that in many cases developers go for a .md file (short for markdown) because this format is automatically rendered on services like GitHub, GitLab, or any other main version control system. Vignettes are longer form documentation that explain in more depth how to use your app. There are also useful if you need to detail the core functions of the application using a static document. In a perfect world, you will be creating one vignette per shiny page/tab. Functions documentation. Every function in your package should come with its own documentation, even if only for your future self. “Exported” functions, the one which are available once you do library(myapp), should be fully documented: they are the one that will be listed in the package help page. The internal functions need less documentation, but documenting them is the best way to be sure you can come back to the app in a few months and still know why things are the way they are, what the pieces of the apps are used for, and how to use these functions15. If needed, you can build a {pkgdown} website, that can either be deployed on the web or kept internally. It can contain installation steps for I.T., internal features use for developers, user guide…. 3.1.2.2 Testing Nothing should go to production without being tested. Nothing. Testing production apps is a wide question and we will come back to this question in another chapter, but let’s talk briefly about why using a package structure helps with testing. Frameworks for package testing are robust and widely documented in the R world, and if you choose to embrace the “Shiny App as a Package” strucutre, you do not have to put any extra-effort for testing your application back-end: use a canonical testing framework like {testthat}. Learning how to use it is not the subject of this chapter, so feel free to refer to the documentation, and see also Chapter 5 of the workshop: “Building a package that lasts”. We will come back to testing in the “Build Yourself a Safety Net” chapter. 3.1.3 Deploy Once your application is ready, you will want to send it to production. Most of the time, if not always, that means running it on someone else’s computer. 3.1.3.1 Local deployment When adopting the package strucutre, you can use classical tools to locally install your Shiny application. A Shiny App as a package can be built as a tar.gz, sent to your colleagues, friends, and family, and even to the CRAN. It can also be installed in any R-package repository. You can then install your packaged application along with its dependencies using the appropriate remotes::install_*() command. And, if you built your app with {golem}, you will launch the app using: library(myuberapp) run_app() 3.1.3.2 RStudio Connect, Shiny Server, shinyapps.io Sending a Shiny application to a Rstudio product currently requires placing an R script at the root of your package directory. To run your application on these platforms, you will need to use a “standard” Shiny formation, i.e. an app.R file or ui.R / server.R, and send the whole thing to the server. To integrate your “Shiny App as a package” into Connect or Shiny Server, you can adopt two strategies: Use an internal package manager like RStudio Package Manager, where the package app is installed. Once the package is available in your internal repository, you can create an app.R file with only this small piece of code: library(myuberapp) run_app() Upload the complete content of the package directory to the server. You will need an app.R file at the root of the package: # Load all R scripts and functions pkgload::load_all() # Launch the application shiny::shinyApp(ui = app_ui(), server = app_server) This is the file you will get if you run one of the three RStudio related functions from golem, for example golem::add_rconnect_file(). 3.1.3.3 Docker containers Docker containers can be used to embed a frozen OS that will launch your application in a safe environnement. In order to dockerize your app, create a Dockerfile that list your package to be installed as in the local deployment with the appropriate remotes::install_*() function. Then, use as a CMD R -e 'options(\"shiny.port\" = 80, shiny.host = \"0.0.0.0\"); myuberapp::run_app()' so that your app will be launched when starting the Docker container. Change the output port to the one you need. Note that {golem} provides you the Dockerfile you need with golem::add_dockerfile(). We will be back to Docker container in a few chapters, notably in the context of building Dockerfile for {golem}-based applications. 3.1.4 Resources In the rest of this book, we will assume you are comfortable with building an R package. If you need to read some resources before continuing, feel free to have a look at these links: R packages “Building a package that lasts” Writing R Extensions R package primer - a minimal tutorial 3.2 Using Shiny Modules Modules are one of the most powerful tool for building Shiny Application in a maintenable and sustainable way. 3.2.1 Why are we using Shiny modules? Small is beautiful. Being able to properly cut a code in small modules helps the developers build their mental model of the application (Remember “What is a complex Shiny Application?”). But what are Shiny modules? Shiny modules address the namespacing problem in Shiny UI and server logic, adding a level of abstraction beyond functions .right{ text-align: right;} Modularizing Shiny app code Let us first untangle this quote with an example about what is the Shiny namespace problem. 3.2.1.1 One million “Validate” buttons A big Shiny application usually requires using several times the same pieces of ui/server, which makes it hard to name and identify similar inputs and outputs. Indeed, Shiny requires its outputs and inputs to have a unique id. And unfortunately, we can not bypass that: when you send a plot from R to the browser, i.e from the server to the ui, the browser needs to know exactly where to put this element. This “exactly where” is handled through the use of an id. Ids are not Shiny specific: they are at the very root of the way web pages work. Understanding all of this is not the purpose of this chapter: just remember that Shiny inputs and outputs ids have to be unique, just as any id on a webpage, so that the browser knows where to put what it receives from R, and R knows what to listen to from the browser. The need to be unique is made a little bit complex by the way Shiny handles the names, as it shares a global pool for all the id names, with no native way to use namespaces. Namespaces? Namespaces are a computer science concept which has been created to handle a common issue: how to share the same name for a variable in various places of your program without them conflicting. In other words, how to use an object called plop several times in the program, and still be sure that it is correctly used depending on the context. R itself has a system for namespaces ; this is what packages do and why you can have purrr::flatten and jsonlite::flatten on the same computer and inside the same script: the function names rethe same, but the two live in different namespaces, and the behavior of both functions can be totally different as the symbol is evaluated inside two different namespaces. If you want to learn more about namespaces, please refer to the 7.4 Special environments chapter from Advanced R, or turn to any computer science book: namespaces are pretty common in any programming language. So, that is what modules are made for: creating small namespaces where you can safely define ids without conflicting with other ids in the app. Why do we need to do that? Think about the number of times you created a “OK” or “validate” button. How have you been handling that so far? By creating validate1, validate2, and so on and so forth. But if you think about it, you are mimicking a namespacing process: a validate in namespace 1, another in namespace 2. Consider the following Shiny application: library(shiny) ui &lt;- function() { fluidPage( sliderInput( inputId = &quot;choice1&quot;, label = &quot;choice 1&quot;, min = 1, max = 10, value = 5 ), actionButton( inputId = &quot;validate1&quot;, label = &quot;Validate choice 1&quot; ), sliderInput( inputId = &quot;choice2&quot;, label = &quot;choice 2&quot;, min = 1, max = 10, value = 5 ), actionButton( inputId = &quot;validate2&quot;, label = &quot;Validate choice 2&quot; ) ) } server &lt;- function(input, output, session) { observeEvent( input$validate1 , { print(input$choice1) }) observeEvent( input$validate2 , { print(input$choice2) }) } shinyApp(ui, server) This, of course, is an approach that works. Well, it works as long as your code base is small. But how can you be sure that you are not creating validate6 on line 55 and another on line 837? How can you be sure that you are deleting the correct combination of UI/server components if they are named that way? Also, how do you work smoothly in a context where you have to scroll from sliderInput(\"choice1\" to observeEvent( input$choice1 , { which might be separated by thousands of lines? 3.2.1.2 A bite-sized code base Build your application through multiple smaller application, easier to understand, develop and maintain, using shiny modules. We assume that you know the R saying that “if you copy and paste something more than twice, you should make a function”. Then, how do we refactor the partially repetitive piece of code so that it is reusable? Yes, you guessed right: using shiny modules. Shiny modules aim at three things: simplifying “id” namespacing, split the code base into a series of functions, and allow UI/Server parts of your app to be reused. Most of the time, modules are used to do the two first. In our case, we could say that 90% of the module we write are never reused16 ; they are here to allow us to split the code base into smaller, more manageable pieces. With Shiny modules, you will be writing a combination of UI and server functions. Think of them as small, standalone Shiny apps, which handle a fraction of your global application. If you develop R packages, chances are you have split your functions into series of smaller functions ; with shiny modules, you are doing the exact same thing: with just a little bit of tweaking, you are splitting your application into series of smaller applications. 3.2.2 When should you modularize? No matter how big your applicatoin is, it is always a safe to start modularising from the very beginning, The effort of writing a module from start is pretty low compared to converting an app that starts to grow to a module structure. It is even easier if you are working with {golem}, which promotes the use of modules from the very beginning of your application. \"Yes, but I just want to write a small app, nothing fancy Production apps almost always started as a small Proof Of Concept. Then, the small PoC becomes an interesting idea. Then, this idea becomes a strategical asset. And before you know it, your ‘not-that-fancy’ app needs to become larger and largerr. So you will be better build solid foundations for your application from the very beginning. 3.2.3 A practical walk through As an example is worth a thousand words, let’s explore together the code of a very small Shiny application that is split into modules. 3.2.3.1 Your first Shiny Module Let’s try to transform the above example (the one with two sliders and two action buttons) into an application with a module. The module is small shiny application separated from the main code: # Re-usable module mod_ui &lt;- function(id) { ns &lt;- NS(id) tagList( sliderInput( inputId = ns(&quot;choice&quot;), label = &quot;Choice&quot;, min = 1, max = 10, value = 5 ), actionButton( inputId = ns(&quot;validate&quot;), label = &quot;Validate Choice&quot; ) ) } mod_server &lt;- function(input, output, session) { ns &lt;- session$ns observeEvent( input$validate , { print(input$choice) }) } # Main application library(shiny) app_ui &lt;- function() { fluidPage( mod_ui(id = &quot;mod_ui_1&quot;), mod_ui(id = &quot;mod_ui_2&quot;) ) } app_server &lt;- function(input, output, session) { callModule(mod_server, id = &quot;mod_ui_1&quot;) callModule(mod_server, id = &quot;mod_ui_2&quot;) } shinyApp(app_ui, app_server) Let’s stop for a minute and decompose what we have here. The server function of the module (mod_server()) is pretty much the same as before: you use the same code as the one you would use in any server part of a Shiny application. The ui function of the module (mod_ui()) requires specific things. There are two new things: ns &lt;- NS(id) and ns(inputId). That is where the namespacing happens. Remember the previous version where we identified out two “validate” buttons with slightly different namespaces: validate1 and validate2? Here, we create namespaces with the ns() function, built with ns &lt;- NS(id). This line, ns &lt;- NS(id), is added on top of all module ui functions and will allow building namespaces with the module id. To understand what it does, let us try and run it outside Shiny: id &lt;- &quot;mod_ui_1&quot; ns &lt;- NS(id) ns(&quot;choice&quot;) [1] &quot;mod_ui_1-choice&quot; And here it is, our namespaced id. Each call to a module with callModule() requires a different id argument that will allow creating various internal namespaces, preventing from id conflicts17. Then you can have as many validate input as you want in your app, as long as this validate has a unique id inside your module. 3.2.3.2 Passing arguments to your modules Shiny modules will potentially be reused and may need specific user interface and inputs. This requires using extra arguments to generate the UI and server. As UI and server are functions, you can set parameters that will be used to configure the internals of the result. As you can see, the app_ui contains a series of call to mod_ui(unique_id, ...) function, allowing additional arguments like any other function: mod_ui &lt;- function(id, button_label) { ns &lt;- NS(id) tagList( actionButton(ns(&quot;validate&quot;), button_label) ) } mod_ui(&quot;mod_ui_1&quot;, button_label = &quot;Validate Choice&quot;) mod_ui(&quot;mod_ui_2&quot;, button_label = &quot;Validate Choice, again&quot;) &lt;button id=&quot;mod_ui_1-validate&quot; type=&quot;button&quot; class=&quot;btn btn-default action-button&quot;&gt;Validate Choice&lt;/button&gt; &lt;button id=&quot;mod_ui_2-validate&quot; type=&quot;button&quot; class=&quot;btn btn-default action-button&quot;&gt;Validate Choice, again&lt;/button&gt; The app_server side contains a series of callModule(mod_server, unique_id, ...), also allowing additional parameters, just like any other function. As a live example, we can have a look at mod_dataviz.R from the {tidytuesday201942} Shiny application. This application contains 6 tabs, 4 of them being pretty much alike: a side bar with inputs, an a main panel with a button and the plot. This is a typical case where you should reuse modules: if two or more parts are relatively similar, it is easier to bundle it inside a reusable module, and condition the ui/server with function arguments. {tidytuesday201942} Here, are some examples of how it works in the module UI: mod_dataviz_ui &lt;- function( id, type = c(&quot;point&quot;, &quot;hist&quot;, &quot;boxplot&quot;, &quot;bar&quot;) ) { h4( sprintf( &quot;Create a geom_%s&quot;, type ) ), if (type == &quot;boxplot&quot; | type ==&quot;bar&quot;) { selectInput( ns(&quot;x&quot;), &quot;x&quot;, choices = names_that_are(c(&quot;logical&quot;, &quot;character&quot;)) ) } else { selectInput( ns(&quot;x&quot;), &quot;x&quot;, choices = names_that_are(&quot;numeric&quot;) ) } } And in the module server: mod_dataviz_server &lt;- function( input, output, session, type ) { if (type == &quot;point&quot;) { x &lt;- rlang::sym(input$x) y &lt;- rlang::sym(input$y) color &lt;- rlang::sym(input$color) r$plot &lt;- ggplot( big_epa_cars, aes(!!x, !!y, color = !!color) ) + geom_point() + scale_color_manual( values = color_values( 1:length(unique(pull(big_epa_cars, !!color))), palette = input$palette ) ) } } Then, the UI of the entire application is: app_ui &lt;- function() { # [...] tagList( fluidRow( id = &quot;geom_point&quot;, mod_dataviz_ui(&quot;dataviz_ui_1&quot;, &quot;point&quot;) ), fluidRow( id = &quot;geom_hist&quot;, mod_dataviz_ui(&quot;dataviz_ui_2&quot;, &quot;hist&quot;) ) ) } And the app_server() of the application: app_server &lt;- function(input, output, session) { #callModule(mod_raw_server, &quot;raw_ui_1&quot;) callModule(mod_dataviz_server, &quot;dataviz_ui_1&quot;, type = &quot;point&quot;) callModule(mod_dataviz_server, &quot;dataviz_ui_2&quot;, type = &quot;hist&quot;) callModule(mod_dataviz_server, &quot;dataviz_ui_3&quot;, type = &quot;boxplot&quot;) callModule(mod_dataviz_server, &quot;dataviz_ui_4&quot;, type = &quot;bar&quot;) } 3.2.4 Communication between modules One of the hardest part about modules is sharing data across them. There are at least three approaches: (i) returning a reactive function, (ii) the “stratégie du petit r” (to be pronounced with a french accent of course) or (iii) the “stratégie du grand R6”. 3.2.4.1 Returning values from the module One common approach is to return a reactive function from one module, and pass it to another, in the general app_server() function. Here is an example that illustrate this pattern. # Module 1 mod_ui &lt;- function(id) { ns &lt;- NS(id) tagList( sliderInput(ns(&quot;choice&quot;), &quot;Choice&quot;, 1, 10, 5) ) } mod_server &lt;- function(input, output, session) { return( reactive({ input$choice }) ) } # Module 2 mod_b_ui &lt;- function(id) { ns &lt;- NS(id) tagList( actionButton(ns(&quot;validate&quot;), &quot;Print&quot;) ) } mod_b_server &lt;- function(input, output, session, react) { observeEvent( input$validate , { print(react()) }) } # Application library(shiny) app_ui &lt;- function() { fluidPage( mod_ui(&quot;mod_ui_1&quot;), mod_b_ui(&quot;mod_ui_2&quot;) ) } app_server &lt;- function(input, output, session) { res &lt;- callModule(mod_server, &quot;mod_ui_1&quot;) callModule(mod_b_server, &quot;mod_ui_2&quot;, react = res) } shinyApp(ui, server) This strategy works well, but for large Shiny Apps it might be hard to handle large list of reactive outputs / inputs and to keep track of how things are organised. It might also create some reactivity issues, as a lot of reactive function calls is harder to control. 3.2.4.2 The “stratégie du petit r” In this strategy, we create a global reactiveValues list that is passed along other modules. The idea is that it allows to be less preoccupied about what your module takes as input and what it outputs. You can think of this approach as creating a small, internal database that is passed along all the modules of your application. Below, we create a “global” (in the sense that it is initiated at the top of the module hierarchy) reactiveValues() object in the app_server() function. It will then go through all modules, passed as a function argument. # Module 1 mod_ui &lt;- function(id) { ns &lt;- NS(id) tagList( sliderInput(ns(&quot;choice&quot;), &quot;Choice&quot;, 1, 10, 5) ) } mod_server &lt;- function(input, output, session, r) { observeEvent( input$choice , { r$choice &lt;- input$choice }) } # Module 2 mod_b_ui &lt;- function(id) { ns &lt;- NS(id) tagList( actionButton(ns(&quot;validate&quot;), &quot;Print&quot;) ) } mod_b_server &lt;- function(input, output, session, r) { ns &lt;- session$ns observeEvent( input$validate , { print(r$choice) }) } # Application library(shiny) ui &lt;- function() { fluidPage( mod_ui(&quot;mod_ui_1&quot;), mod_b_ui(&quot;mod_ui_2&quot;) ) } server &lt;- function(input, output, session) { r &lt;- reactiveValues() callModule(mod_server, &quot;mod_ui_1&quot;, r) callModule(mod_b_server, &quot;mod_ui_2&quot;, r) } shinyApp(ui, server) The good thing about this method is that whenever you add something in one module, it is immediately available in all other modules where r is present. The downside is that it can make it harder to reason about the app, as the input/content of the r is not specified anywhere unless you explicitely document it: the parameter to your server function being “r” only, you need to be a little bit more zealous when it comes to documenting it. Note that if you want to share your module, for example in a package, you should document the structure of the r. For example: #&#39; @param r a `reactiveValues()` list with a `choice` element in it. #&#39; This `r$choice` will be printed to the R console. 3.2.4.3 The “stratégie du grand R6” Similarly to the “stratégie du petit r”, we can create an R6 object, which is passed along inside the modules. As this R6 object is not a reactive object and is not meant to be used as such, this reduces uncontrolled reactivity of the application, thus reduces the complexity of handling chain reactions across modules. Of course, you need to have another special tool in your app to trigger elements. All this will be explained in details in chapter Reactivity anti-patterns of this book, and you can find an example of this pattern inside the {hexmake} application. 3.3 Structuring your app 3.3.1 Business logic &amp; application logic A shiny application has two main components: the application logic and the business logic. Application logic is what makes your Shiny app interactive: structure, button, table, interactivity, etc. These components are not specific to your core business: you could use them for any other line of work or professional context. This has no other use case than your interactive application: it is not meant to be used outside your app, you would not use them in a markdown report for instance. Business logic is the components with the core algorithms and functions that make your application specific to your area of work. You can recognize these elements as the ones that can be run outside any interactive context. This is the case for specific computation and algorithm, custom plot or geom for {ggplot2}, specific calls to a database, etc. These two components do not have to live together. They should not live together if you want to keep your sanity when you build an app. You will end up having to rerun the app from scratch and spend five minutes clicking everywhere just to be sure you have correctly set the color palette for the graph on the last tabPanel(). Trust us, we have been there, and it is not pretty. So what is the way to go? Extract the business function from the reactive functions. Literally. Compare this pattern: # Application library(shiny) library(dplyr) ui &lt;- function() { tagList( tableOutput(&quot;tbl&quot;) ) } server &lt;- function(input, output, session) { output$tbl &lt;- renderTable({ mtcars %&gt;% # [...] %&gt;% # [...] %&gt;% # [...] %&gt;% # [...] %&gt;% # [...] %&gt;% top_n(10) }) } shinyApp(ui, server) To this one: library(shiny) library(dplyr) # Business logic top_this &lt;- function(tbl) { tbl %&gt;% # [...] %&gt;% # [...] %&gt;% # [...] %&gt;% # [...] %&gt;% top_n(10) } # Application ui &lt;- function() { tagList( tableOutput(&quot;tbl&quot;) ) } server &lt;- function(input, output, session) { output$tbl &lt;- renderTable({ top_this(mtcars) }) } shinyApp(ui, server) Both scripts do the exact same thing. The difference is that the second code can be easily explored without having to relaunch the app. You will be able to build a reproducible example to explore, illustrate and improve function named top_this(). This function can be tested, documented and reused outside the application. Moreover, this approach lowers the cognitive load when debugging: you either debug an application issue, or a business logic issue. You never debug both at the same time. Even more, think about the future: how likely are the colors or the UI subject to change, compare to how likely the core algorithms are to change? As said in The Art of Unix Programming, “Fashions in the look and feel of GUI toolkits may come and go, but raster operations and compositing are forever”. In other words, the core back-end, once consolidated, will potentially stay unchanged forever. On the other hand, the front-end might change: new colors, new graphic designs, new interactions, new visualization libraries… Whenever this will happen, you will be happy you have separated the business logic from the application logic, as you will have to change less code. How to do that? Add you application logic inside a file (typically, a module), and the business logic in another R script (typically starting with fct_ or utils_). You can even write the business logic inside another package, making these functions really reusable outside your application. 3.3.2 Small is beautiful (bis repetita) There are a lot of reasons for splitting your application into smaller pieces, including the fact that it is easier to maintain, easier to decipher, and it facilitates collaboration. There is nothing harder to maintain than a Shiny app only made of a unique 1000-line long app.R file. Well, there still is the 10000-line long app.R file, but you got the idea. Long scripts are almost always synonym of complexity when it comes to building a software. Of course, small and numerous scripts do not systematically prevent from codebase complexity but they simplify collaboration and maintenance, and divide the application logic into smaller, easier to understand bits of code. So yes, big files are complex to handle and make development harder. Here is what happens when you work on a application for production: You will work during a long period of time (either in one run or split across several months) on your codebase. Hence, you will have to get back to pieces of code you wrote a long time ago. You will possibly develop with other developers. Maintaining a code base when several people work on the same directory is already a complex thing: from time to time you might work on the same file separately, a situation where you will have to be careful about what and how to merge things when changes are implemented. It is almost impossible to work together on one same file all along the project without losing your mind: even more if this file is thousands of lines long. You will implement numerous features. Numerous features imply a lot of UI &amp; server interactions. In an app.R file of thousands of lines, it is very hard to match the UI element with its server counterpart: when the UI is on line 50 and the server on line 570, you will be scrolling a lot when working on these elements. 3.3.3 Conventions matter In this section you will find a suggestion for a naming convention for your app files that will help you and your team be organized. Splitting files is good. Splitting files using a defined convention is better. Why? Because using a common convention for your files helps the other developers (and potentially you) to know exactly what is contained in a specific file. Using a convention allows everyone to be able to know where to look for debugging and implementing new features. For example, if you follow {golem}’s convention (which is the one developed in this section), you will know immediatly that a file starting with mod_ contains a module. If you take over a project, look in the R/ folder and see files starting with these three letters, yoy will know immediately that these files contain modules. Here is our proposition for a convention defining how to split your application into smaller pieces. First of all, put everything into an R/ folder. If you build your app using the {golem} framework, this is already the case. We use the package convention to hold the functions of our application. The naming convention in {golem} is the following: app_*.R (typically app_ui.R and app_server.R) contain the top level functions defining your user interface and your server function. fct_* files contains the business logic, potentially large functions. They are the backbone of the application and may not be specific to a given module. They can be added using {golem} with the add_fct(\"name\") function. mod_* files contain a unique module. Many Shiny apps contain a series of tabs, or at least a tab-like pattern, so we suggest that you number then according to their step in the application; Tabs are almost always named in the user interface, so that you can use this tab-name as the file name. For example, if you build a dashboard where the first tab is called “Import”, you should name your file mod_01_import.R. You can create this file with a module skeleton using golem::add_module(\"01_import\"). utils_* are files that contain utilities, which are small helper functions. For example, you might want to have a not_na, which is not_na &lt;- Negate(is.na), a not_null, or small tools that you will be using application-wide. Note that you can also create utils for a specific module. *_ui_*, for example utils_ui.R, relates to the user interface. Anything back-end can be noted using *_server_*. For example fct_connection_server.R will contain functions that are related to the connection to a database, and which are specifically used from the server side. Note that when building a module file with {golem}, you can also create fct_ and utils_ specific files that will hold functions and utilities for this specific module. For example, golem::add_module(\"01_import\", fct = \"readr\", utils = \"ui\") will create R/mod_01_import.R, R/mod_01_import_fct_readr.R and R/mod_01_import_utils_ui.R. Of course, as with any convention, you might be deviating from time to time from this pattern. Your app may not have that many functions, or maybe the functions can all fit into one utils_ file. But be it one or thousands of files, it is always a good practice to stick to a formalized pattern. "],
["golem.html", "Chapter 4 Introduction to {golem} 4.1 What is {golem}? 4.2 Understanding {golem} app structure", " Chapter 4 Introduction to {golem} The {golem} package is a framework for building production-grade Shiny Application. Lot of the patterns and methodologies described in this book are linked to {golem} and packages from the golemverse. Of course, all the advices developed in this book will still be valid even if you are not planning on using {golem}. We have quickly introduced {golem} in the last chapter, and we will come back to this package from time to time in the following chapters. So, let’s start with an introduction to this package. Note that the version used at the time of writing this book is 0.2.1. 4.1 What is {golem}? {golem} can be thought as a toolkit for simplifying the creation, development and deployment of a Shiny application. It’s focused on building applications that will be sent to production, but of course starting with {golem} from the very beginning is also possibl, even recommended: it is easier to start with {golem} than to refactor your all codebase to fit into the framework. The stable release can be found on CRAN and is installed with: install.packages(&quot;golem&quot;) {golem} development version can be found on GitHub and is installed with: remotes::install_github(&quot;Thinkr-open/golem&quot;) The version of the package on CRAN at the time of writting this book is: library(dplyr, warn.conflicts = FALSE) tools::CRAN_package_db() %&gt;% filter(Package == &quot;golem&quot;) %&gt;% select(Version) Version 1 0.2.1 While the current version of the dev version is: x &lt;- tempfile() download.file(&quot;https://raw.githubusercontent.com/ThinkR-open/golem/dev/DESCRIPTION&quot;, x) desc::desc_get_version(x) [1] &#39;0.2.1.9000&#39; The motivation behind {golem} is that building a proof-of-concept application is easy, but things change when the application becomes larger and more complex, and especially when you need to send that app to production. Until recently there has not been any real framework for building and deploying production-grade Shiny Apps. This is where {golem} comes into play: offering Shiny developers a toolkit for making a stable, easy-to-maintain, and robust production web application with R. {golem} has been developed to abstract away the most common engineering tasks (for example, module creation, addition and linking of external CSS or JavaScript file, …), so you can focus on what matters: building the application. Once your application is ready to be deployed, {golem} guides you through testing and brings tools for deploying to common platforms. Some things to keep in mind before using {golem}: A {golem} application is contained inside a package. Knowing how to build a package is heavily recommended. The good news is also that everything you know about package development can be applied to {golem}. A {golem} app works better if you are working with shiny modules. Knowing how modules work is also recommended but not necessary. 4.2 Understanding {golem} app structure A {golem} application is a, R package, as having an R package architecture is perfectly suited for production-ready programs, as we developed in the previous chapter. Let’s focus on the architecture of the default {golem} app, and present what part each file plays and how you can use (or not use) each of them. You can create a {golem} project, here called golex, with Rstudio “New project” creation or with command line: golem::create_golem(&quot;golex&quot;) The project will start with this specific architecture: fs::dir_tree(&quot;golex&quot;) [01;34mgolex[0m ├── DESCRIPTION ├── NAMESPACE ├── [01;34mR[0m │ ├── [32mapp_config.R[0m │ ├── [32mapp_server.R[0m │ ├── [32mapp_ui.R[0m │ ├── [32mmod_my_first_module.R[0m │ └── [32mrun_app.R[0m ├── [01;34mdev[0m │ ├── [32m01_start.R[0m │ ├── [32m02_dev.R[0m │ ├── [32m03_deploy.R[0m │ └── [32mrun_dev.R[0m ├── [01;34minst[0m │ ├── [01;34mapp[0m │ │ └── [01;34mwww[0m │ │ ├── custom.css │ │ ├── favicon.ico │ │ ├── plop.js │ │ └── script.js │ └── golem-config.yml └── [01;34mman[0m └── run_app.Rd If you are familiar with building R packages, this structure will look familiar to you. And for a good readon: a {golem} app IS a package. 4.2.1 DESCRIPTION &amp; NAMESPACE The DESCRIPTION and NAMESPACE are standard package files (i.e. they are not {golem}-specific). In DESCRIPTION, you will add a series of metadata about your package, for example who wrote the package, what is the package version, what is its goal, who to complain to if things go wrong, and also information about external dependencies, the license, the encoding… This DESCRIPTION file will be filled automatically by the first function you will run in dev/01_start.R, and by other functions from the dev/ scripts. In other words, most of the time you will not interact with it directly, but through wrappers from {golem} and {usethis} which are listed in the dev scripts. The NAMESPACE file is the file you will NEVER edit by hand! The NAMESPACE file defines how to interact with the rest of the package: what functions to import and from which package and what functions to export, i.e. what functions are available to the user when you do library(golex). This file will be built when running the documenting process in your R package: {roxygen2} will scan all your .R files, and build the man/ + the NAMESPACE, by scanning the roxygen tags there. Explaining how these files are to be filled and how to document your functions is out of the scope of this book, as they are deeply linked to how you would do that for any other package. If you ant to learn more about these, here are some resources you can refer to: Writing R Extensions - The DESCRIPTION file Writing R Extensions - Package namespaces R Packages - Package metadata R Packages - Namespace Building a package that lasts — eRum 2018 workshop 4.2.2 R/ The R/ folder is the standard folder where you will store all your app functions. When you start your project with {golem}, this folder is pre-populated with three .R files: app_server.R, app_ui.R and run_app.R. During the process of building your application, all the core functionalities of your app will be stored in the R/ directory. Note that these files are the “core” functionalities of your application itself, and that other .R files also exists. For example, when you will need to deploy your application on RStudio platforms, {golem} will create an app.R at the root of your directory18. This file should not go into the R/ folder, as it is not in the core of the package mechanic. The dev/ folder also contains .R scripts, and they are inside this folder as they should not live inside the R/ folder: they are utilitarian files for development, not core functionalities of your application. Inside these .R files, you will find the content of your modules (the one added with golem::add_modules()) and the utilitarian / business logic functions, built with golem::add_utils() and golem::add_fct(). If you want to add a standard file (that is to say out of {golem} nomenclature), you can also call usethis::use_r(\"name\"), which will create a R/name.R file. If you have built a “classic” Shiny Apps, i.e before {golem} was available, or if you started with the standard RStudio template, you might have a series of source() and library() calls all other the place. This is not to be done with a {golem}-based application: you are leveraging the package infrastructure so that everything inside the R/ folder is made available internally, without having to source() scripts. Note also that this folder can not contain sub-folders. 4.2.2.1 app_server.R #&#39; The application server-side #&#39; #&#39; @param input,output,session Internal parameters for {shiny}. #&#39; DO NOT REMOVE. #&#39; @import shiny #&#39; @noRd app_server &lt;- function( input, output, session ) { # List the first level callModules here } The app_server.R file contains the function for the server logic. If you are familiar with the classic ‘ui.R / server.R’ methodology, this function can be seen as a replacement for the contents of the function you have in your server.R. Building a complex Shiny application commonly implies using Shiny modules. If so, you will be adding there a series of callModule(), the ones you will get on the very bottom of the file created with golem::add_module(). You will also find global elements from your server-logic: top-level reactiveValues(), connections to databases, options setting… 4.2.2.2 app_ui.R #&#39; The application User-Interface #&#39; #&#39; @param request Internal parameter for `{shiny}`. #&#39; DO NOT REMOVE. #&#39; @import shiny #&#39; @noRd app_ui &lt;- function(request) { tagList( # Leave this function for adding external resources golem_add_external_resources(), # List the first level UI elements here fluidPage( h1(&quot;golex&quot;) ) ) } This piece of the app_ui.R is designed to received the counterpart of what you put in your server. Everything here is to be put after the # List the first level UI elements here line. Just as with their server counterparts, the UI side of these elements are the one from the bottom of the file you are creating with golem::add_module(). By default, {golem} uses a fluidPage(), which is {shiny} most commonly used template. If ever you want to use navBarPage(), this is where you will define this: replace one with the other, and you will be good to go. You can also define any other template page, for example with an htmlTemplate(). Keep in mind that removing the fluidPage() here implies that there is no available CSS/JS template to be used anymore, and you will need to be adding your own there. #&#39; Add external Resources to the Application #&#39; #&#39; This function is internally used to add external #&#39; resources inside the Shiny application. #&#39; #&#39; @import shiny #&#39; @importFrom golem add_resource_path activate_js favicon bundle_resources #&#39; @noRd golem_add_external_resources &lt;- function(){ add_resource_path( &#39;www&#39;, app_sys(&#39;app/www&#39;) ) tags$head( favicon(), bundle_resources( path = app_sys(&#39;app/www&#39;), app_title = &#39;golex&#39; ) # Add here other external resources # for example, you can add shinyalert::useShinyalert() ) } The second part of this file contains the golem_add_external_resources() function, which is used to add, well, external resources. You may have noticed that this function is to be found above in the file, in the app_ui() function. This function is used for linking to external files inside your applications: notably the files you will create with golem::add_css_file() and friends. In golem_add_external_resources(), you can also define custom resourcesPath. The first line (the one with addResourcePath()) is the one allowing the inst/app/www folder to mounted and be available at www with your app when you launch it. That link makes it possible for {golem} to bundle the CSS and JavaScript files automatically. The other part of this function, starting with tags$head, creates a &lt;head&gt; tag for your application. This &lt;head&gt; tag is a pretty standard tag, which is used in HTML to define a series of metadata about your app. We encourage you to add any new external file (e.g pictures) in this inst/app/www folder, so that you can later use it in the UI with the common www prefix. An other common pattern would be: Adding images in inst/app/img Calling addResourcePath( 'img', system.file('app/img', package = 'golex') ) Adding elements to your UI with tags$img(src = \"img/name.png\"). 4.2.2.3 run_app.R #&#39; Run the Shiny Application #&#39; #&#39; @param ... A series of options to be used inside the app. #&#39; #&#39; @export #&#39; @importFrom shiny shinyApp #&#39; @importFrom golem with_golem_options run_app &lt;- function( ... ) { with_golem_options( app = shinyApp( ui = app_ui, server = app_server ), golem_opts = list(...) ) } The run_app() function is the one that you will use to launch the app19. The body of this function is wrapped inside with_golem_options(), which allows you to pass arguments to the run_app() function, which will later be callable with golem::get_golem_options(). Some examples of passing arguments include run_app(prod = FALSE) to run a verbose development version or run_app(user = \"admin) to bypass authentication during development tests. 4.2.3 golem-config 4.2.3.1 app_config.R Inside the R/, the app_config.R #&#39; Access files in the current app #&#39; #&#39; @param ... Character vector specifying directory and or file to #&#39; point to inside the current package. #&#39; #&#39; @noRd app_sys &lt;- function(...){ system.file(..., package = &quot;golex&quot;) } #&#39; Read App Config #&#39; #&#39; @param value Value to retrieve from the config file. #&#39; @param config R_CONFIG_ACTIVE value. #&#39; @param use_parent Logical, scan the parent directory for config file. #&#39; #&#39; @importFrom config get #&#39; #&#39; @noRd get_golem_config &lt;- function( value, config = Sys.getenv(&quot;R_CONFIG_ACTIVE&quot;, &quot;default&quot;), use_parent = TRUE ){ config::get( value = value, config = config, # Modify this if your config file is somewhere else: file = app_sys(&quot;golem-config.yml&quot;), use_parent = use_parent ) } This file is designed to handle two things: app_sys() is a wrapper around system.file(package = \"golex\"), and allows you to quickly reference to the files inside the inst/ folder. For example, app_sys(\"x.txt\") points to inst/x.txt file inside your package. get_golem_config() helps you manipulate the config file located at inst/golem-config.yml. 4.2.3.2 Manipulating golem-config.yml Here is what the default config file looks like: default: golem_name: golex golem_version: 0.0.0.9000 app_prod: no production: app_prod: yes dev: golem_wd: !expr here::here() It is based on the {config} format, and allows you to define contexts, with values associated with these specific contexts. For example, in the default example: default.golem_name, default.golem_version, default.app_prod are usable across the whole life of your golem app: while developing, and also when in production. production.app_prod might be used for adding elements that are to be used once the app is in production. dev.golem_wd is in a dev config because the only moment you might reliably use this config is while developing your app. Use the app_sys() function if you want to rely on the package path once the app is deployed. These options are globally set with: set_golem_options() The functions reading the options in this config file are: get_golem_name() get_golem_wd() get_golem_version() You can set these with: set_golem_name(&quot;this&quot;) set_golem_wd(&quot;.&quot;) set_golem_version(&quot;0.0.1&quot;) If you are already familiar with the {config} package, you can use this file just as any config file. {golem} comes with an amend_golem_config() function to add elements to it. amend_golem_config( key = &quot;where&quot;, value = &quot;indev&quot; ) amend_golem_config( key = &quot;where&quot;, value = &quot;inprod&quot;, config = &quot;production&quot; ) In R/app_config.R, you will find a get_golem_config() function that allows you to retrieve config from this config file: get_golem_config( &quot;where&quot; ) get_golem_config( &quot;where&quot;, config = &quot;production&quot; ) You can also use en environment variable (default {config} behavior): Sys.setenv(&quot;R_CONFIG_ACTIVE&quot; = &quot;production&quot;) get_golem_config(&quot;where&quot;) The good news is that if you don’t want/need to use {config}, you can safely ignore this file, just leave it where it is: it is used internally by the {golem} functions. 4.2.3.3 golem_config vs golem_options There is two ways to configure golem apps: The golem_opts in the run_app() function The golem-config.yml file The big difference between these two is that the golem options from run_app() are meant to be configured during runtime: you will be doing run_app(val = \"this\"), whereas the golem-config is meant to be used in the back-end, and will not be linked to the parameters passed to run_app() (even if this is technically possible, this is not the main objective),. It is also linked to the R_CONFIG_ACTIVE environment variable, just as any {config} file. The idea is also that the golem-config.yml file is shareable across {golem} projects (golem_opts are application specific), and will be tracked by version control systems. 4.2.4 inst/app/www/ The inst/app/www/ folder contains all files that are made available at application run time. Any web application has external files that allow it to run20. For example, {shiny} and its fluidPage() function bundles a series of CSS and JavaScript files, notably the Boostrap library, or jQuery. These external files enhance your app: CSS for the design part and JavaScript for the interactive part (more or less). On top of that, you can add your own files: your own design with CSS or your own JavaScript content (as we will see in the last chapters of this book). In order to work, you have to include, somewhere in the UI, a link to these files. This is what golem_add_external_resources() is made for: linking the external resources that you will build with the following functions. golem::add_css_file() golem::add_js_file() golem::add_js_handler() golem::use_favicon() Be aware that these files are available under the www/ at application run time, i.e. that the www/ folder is available by your browser, not by R when it runs/generates your application. In other words, you can use the www prefix in the HTML generated in your UI, which is read by your browser, not from the R/server side. If you want to link to a file that is read during application generation, you will need to use the app_sys() function, with for example includeMarkdown( app_sys(\"app/www/plop.md\") ). 4.2.5 dev/ The dev/ folder is to be used as a notebook for your development process: you will find here a series of functions that can be used all along your project. The content of these files are specific to {golem} here, but the concept of using a script to store all development steps is not restricted to a Shiny application: it could easily be done for any package, and this is something we recommend to do. The functions inside these files are the ones used to do some setup, like usethis::use_mit_license() or usethis::use_vignette(\"my-analysis\"), add testing infrastrucutre like usethis::use_test(\"my-function\") or devtools::check(). You will also find functions to populate the application like golem::add_module(\"my-module\") or golem::add_js_file(\"my-script\"). And finally, there are functions you will need once your application is ready: pkgdown::build_site(), rhub::check_for_cran() or golem::add_dockerfile(). We will come back to these files later in this book when we describe in more depth the {golem} workflow. 4.2.6 man/ The man/ folder includes the package documentation. It is a common folder automatically filled when you document your app, notably when running the dev/run_dev.R script and the document_and_reload() function. As with the NAMESPACE and DESCRIPTION files, explaining this file is out of scope of this book (and to be honest, you will probably never have to interact with these files directly). To know more about documentation and how to build it, here are some external links: R Packages - Object documentation Building a package that lasts — eRum 2018 workshop "],
["workflow.html", "Chapter 5 The workflow 5.1 Part 1: Design 5.2 Part 2: Prototype 5.3 Part 3: Build 5.4 Part 4: Strengthen 5.5 Part 5: Deploy", " Chapter 5 The workflow Building a robust, production-ready web application will be made easier by following a given workflow. The one we are advocating for is divided in five steps: Design, Prototype, Build, Strenghten and Deploy. In this section, we will give an overview of the different steps of this workflow: the rest of the book will cover in more depth each of these steps. 5.1 Part 1: Design The first part of the workflow is the design part. This first step of the process is the one that happens, before actually writing any line of code. This first step is not Shiny nor R specific, but something software engineers do for any software or web application: discuss with the clients, the end-users, and the developers who will work on the project. During the process of designing, you will define how the application will be build: somewhere between users’ dreams, what is technically possible, and the time you have to build the application. 5.2 Part 2: Prototype The Prototype part is the one during which you will build the front-end and the back-end, but separately. As you may know, a Shiny application is an interface (the front-end) used to communicate information to the end-users that are computed on the server side (the back-end). To start on solid ground, you need to build the two (front and back) seperately: On one hand, work on the general appearance, without working on any actual algorithmic implementation: position of the inputs and outputs, general design, interactions…. everything that does not rely on computation on the back end. This “UI first” approach will be made possible for Shiny with notably one package, {shinipsum}, and tools like basic CSS, and some {golem} functions. On the other hand, you (or someone from your team), will be working on building the back-end logic, which are the actual outputs that are going to be displayed, the algorithm that will compute results, and all the elements that do not need an interactive runtime to work. For this point, you can use what we call a “Rmd-first” approach, by combining R functions with the writing of Vignettes that describes the internal of the application. 5.3 Part 3: Build The Build part is the one where you will combine the business (or back-end) logic with the front-end. In this third part, you will work on the core engine of the application, making the business logic work inside the interactive logic of your application. 5.4 Part 4: Strengthen Strengthening your app is ensuring your application is immortal, in the sense that we defined in the first chapter of this book. In this part, we will go through unit tests, reproducible development environment, version control and continuous integration in the context of Shiny applications. 5.5 Part 5: Deploy To Deploy is to send your application into production once it is built. Being exhaustive here would be an impossible task: there are countless ways to make your application accessible to its targeted users, but we will try to cover some basics about this part. In this part, we will quickly present a series of methods to deploy your application on various environments, notably sharing your application as a package, sending it to an RStudio platform, or building a Docker image to serve your app on a cloud provider. "],
["matters.html", "Chapter 6 UX Matters 6.1 Simplicity is Gold 6.2 The danger of feature-creep 6.3 Web Accessibility", " Chapter 6 UX Matters Let’s state the truth: no matter how complex and innovative your back-end is, your application is bad if your User Experience (UX) is bad. That’s the hard truth. We have a natural tendency, as R-coders, to be focused on the back-end, i.e the server part of the application21. Which is perfectly normal—chances are you did not come to R to design front-ends22. However, if people can not understand how to use your application, or if your application front-end does not work at all, your application is not successful, no matter how innovative and incredible the computation algorithms in the back-end are. As you are building a complex, production-grade Shiny application, do not under-estimate the necessity for a successful front-end: it is after all the first thing (and probably the only thing) that the end-users of your web application will see, and our natural taste, as R developers, for back-end/server logic can play against us in the long run: by neglecting the UI and the UX, you will make your application less likely to be adopted among your users ; which is a good way to fail your application project. 6.1 Simplicity is Gold “Simplify, then add lightness” Colin Chapman CBE, Founder of Lotus Cars Aiming for simplicity is a hard thing, but some rules will help you build a better UX, paving the way for a successful application. There are mainly two contexts where you will be building a web app with R: for professional use (i.e people will rely on this app to do their job), or for fun (i.e people will just use the app as a distraction) But both cases have something in common: people will want the app to be usable, and easily usable. If people use your app in a professional context, they do not want to fight with your interface, read complex manuals, or lose time understanding what they are supposed to do and how they are supposed to use your application. In other words, they want an efficient tool: something that, beyond being accurate, is easy to grasp. In a professional context, when it comes to “Business applications”, remember that quicker you understand the interface, the better the user experience. Think about all the professional applications and software that you have been ranting about during your professional life, all these cranky user interfaces you have not understand and/or you need to relearn every time you use them. You do not want your app to be one of these applications. And on the other hand, if users open your app for fun, they are not going to fight against your application: they are just going to give up if the app is too complex to be used. Even a game has to appear easy to use when the users open it. In this section, we will review two general principles: the “do not make me think” principle, which states that interfaces should be as self-explanatory as possible, and the “Rule of least surprise”, stating that elements should behave the way they are commonly expected to behave. These two rules aim at solving one issue: the bigger the cognitive load of your app, the harder it will be for the end-user to use it on a daily basis. 6.1.1 How we read the web: scanning content One big lie we tell ourselves as developer is that end-user will use the app the way we designed it to be used (and to be honest, this is true of any software). We love to think that when faced to our app, the users will carefully read the instructions, make a rational decision based on careful examination of the inputs, before doing what we expect them to do. But the harsh truth is that it is not how what happens. First of all, user rarely read carefully all the instructions: they scan, and perform the first action that more or less match what they need to do, i.e they satisfice (a portmanteau of satisfy and suffice). Navigating the web, users try to optimize their decision but not by making the decision that would be “optimal”, but by doing the first that will satisfy a sufficient amount of relevance. They are behaving like that for a lot of reason, but notably because they want to be as quick as possible on the web, and because the cost of being wrong is most of the time very low: even if you make the wrong decision on a website, chances are that you are just a “return” or “cancel” button away from canceling your last action. .right{ text-align: right;} Don't make me think - Steve Krug For example, let’s have a look at the user interface of hexmake, a Shiny app for building hex stickers, available at https://connect.thinkr.fr/hexmake/ [](img/hexmake.png What will be your reading pattern for this application? What is the first thing you will do when using this app? There is an inherent logic in the application: each sub-menu is designed to handle one specific part of your sticker. The last-but-two menu is the one used to download the sticker, and the last one the menu to open the “how to” of the app. When opening this app, will your fist move be to open the “How to”? Will you open all the sub-menu and select the most “logical” one to start with? Chances are that reading this line, you think you will do that. But in reality, we behave less rationally that we’d like to think. What we most of the time do is click on the first thing that matches what we are here to do. For example, most of the time we will first change the package name, or upload an image, before even opening the about section of this app. Once user have scanned the page, they perform the first action that seems reasonable. Or as coined in “Rational Choice and the Structure of the Environment” by Herbert A. Simon, “organisms adapt well enough to “satisfice”; they do not, in general, “optimize.””. In other words, “As soon as we find a link that seems like it might lead to what we’re looking for, there’s a very good chance that we’ll click it” (‘Don’t make me think’, Steve Krug). What that also means is that user might perform what you’d expect to be “irrational” choices. As they are scanning your application, they might do something unexpected, or use a part of your app in a way that you would not expect it to be used. For example, if you are creating an app that is designed to take as input data that has to be filled following a specific form, you need to check that this requirement is fulfill, or you will end up debugging errors on uncommon entries. This is a pretty common thing about apps and about software in general: you have to expect users to use your product in ways you would not have expect, in way that might seem absurd to you. This is what is called “defensive programming”: you prevent the application to be used in an unexpected way, and instead of relying on the end user being rational with their choice, we “defend” our function from unexpected inputs. For example, consider this small app: library(shiny) ui &lt;- function(request){ tagList( selectInput( &quot;species&quot;, &quot;Choose one or more species&quot;, choices = unique(iris$Species), multiple = TRUE, selected = unique(iris$Species)[1] ), plotOutput(&quot;plt&quot;) ) } server &lt;- function( input, output, session ){ output$plt &lt;- renderPlot({ plot( iris[ iris$Species %in% input$species, ] ) }) } shinyApp(ui, server) What is wrong with this app? Probably nothing from a developer point of view: there is a label telling that one should select one or more element from the dropdown, and then something is plotted below. Pretty standard. But what happen if the dropdown is empty? Our first conception would be that this would never happen, as it is explicitly specified that there should be one or more elements selected. In fact, chances are that even with this label, users will eventually end up with an empty selectInput(), leading to the printing of a red error where the plot should be. And here, we are lucky, the error only prevents the plot from being displayed: other errors would make the application crash. What should we do? Adopt a defensive programming mindset. Every time you create interactive elements, inputs and outputs, or things the user might interact with, ask yourself: “what if [that crazy thing] happens? How do I handle the case where the minimal viable requirements for my app are not met?”. And in fact, you should not be focusing on that only for the user side: the back-end should also be examined for potential unexpected behaviors. For example, if your Shiny app relies on a database connection, you should check gracefully that the connection is possible, and if it is not, send a message to your user that the database is not reachable, and that they should either restart the app or come back in a few minutes. In fact, this is a crucial thing when it comes to making your app successful: you should always fail gracefully and informatively. That means that when your R code fails, the whole app should not fail. If the R code fails for some reason, the user should either get nothing back or an informative bug message, not be faced with a grayish version of the application. Because of the way Shiny is designed, a lot of R errors will make the Shiny app fail completely. If you have not think about this upfront, that means that a user might use the app for say 10 minutes, do a series of specifications, enter parameters and data, and at some point the app completely crashes. Then the user has to restart from scratches, because there is no native way, from there, to restart from where the app has crashed. This is a very important thing to keep in mind when building Shiny app: once the app has failed, there is no easy way to natively get it back to the moment just before it crashed, meaning that your users might lose a significant amount of time they have spent configuring the app. So, one good practice: try, as much as possible, to wrap all server calls in some form of try-catch pattern. That way, you can for example send a notification to the user if the process fails, either using {shiny} notification function, an external package like {shinyalert}, or a custom JavaScript alert like notify.js. Here is a pseudo-code pattern for this using the {attempt} package: library(shiny) ui &lt;- function(request){ tagList( # [...] ) } server &lt;- function( input, output, session ){ conn &lt;- attempt::attempt({ connect_db() }) if (attempt::is_try_error(conn)){ send_notification(&quot;Could not connect&quot;) } else { continue_computing() } } shinyApp(ui, server) 6.1.2 A self-evident app (or at least self-explanatory) One of the goal of a usable app is to make it self-evident, and fall back to a self explanatory app if the first option is too complex a goal. What is the difference between the two? self-evident : “Not needing to be demonstrated or explained; obvious.” lexico.com self-explanatory : “Easily understood; not needing explanation.” https://www.lexico.com/en/definition/self_explanatory So the first is that the app is designed in such a way that there is no learning curve to using it. A self-explanatory app has a small learning curve, but it is designed in a way that will make the user understand it in a matter of seconds. Let’s for example get back to our {tidytuesday201942} application available at connect.thinkr.fr/tidytuesday201942 By itself, this application is not self-evident: you need to have a series of background knowledge before understanding what this application was designed for. For example, you might need to have a vague sense of what tidytuesday is. If you do not, you will have to read the home text, which will help you understand what this is. Then, if we have a look at the menu elements, we see that these are a series of functions from {ggplot2}: without any background about the package, you might find it difficult understanding what this app actually does. Yet, if you want to understand what this app is designed for, you will find enough information either on the home page or in the About section, with external links if needed. And of course, when building apps, context matters. The {tidytuesday201942} app is one that has been developed in the context of tidytuesday, an online weekly event for learning data analysis, mainly through the use of {tidyverse} packages. So there is a good chance visitors of the app will already know what is {ggplot2} when visiting the app. 6.1.2.1 The “Rule of Least Surprise” Also know as “Principle of Least Astonishment.” Rule of Least Surprise: In interface design, always do the least surprising thing. .right{ text-align: right;} 'Basic of the Unix Philosophy', Eric Steven Raymond When we are browsing the web, we have a series of pre-conception about what things are and what they do. For example, we expect an underline text to be clickable: so there is a good chance that if you use underline text inside your app, the user will try to click on it. Usually, the link is also colored differently from the rest of the text. Same goes for the pointer of the mouse, which usually switch from an arrow to a small hand with a finger up. A lot of other conventions exist on the web, and you should endeavor to follow them: a clickable link should have at least one of the properties we just described—and if it is neither underlined nor colored nor changing the pointer when it is hovered, chances are that the user will not click on it. Just imagine for a second if our “Download” button in the {tidytuesday201942} app did not actually download the graph you had generated. Even more, imagine if this button did not download the graph but something else. How would you feel about this experience? And it is not just about links: almost every visual elements on a web page is surrounded by conventions. Buttons should have borders. Links should appear clickable. Bigger texts are headers, the bigger the more important. Elements “visually nested” are related. Etc. Weirdly enough, that is an easy thing to spot when we arrive on a webpage/an app: it can either feel “natural”, or you can immediately see that something is off. The hard thing is that it is something you spot when you are a new-comer: developing the app makes us so familiar with the app that we might miss when something is not used the way it is conventionally used23. Let’s exemplify this with the “Render” button from the {tidytuesday201942} application. This app is built on top of Bootstrap 4, which has no CSS class for {shiny} action button24. Result: without any further CSS, the buttons do not come out as buttons, making it harder to decipher they are actually buttons. Compare this native design: To the one with a little bit of CSS (which is the one online): Yes, it is subtle, yet the second version of the button is clearer to understand. Least surprise is crucial to make the user experience a good one: users rarely think that if something is behaving unexpectedly on an app, it is because of the app: they will usually think it is their fault. Same goes for the application failing or behaving in an unexpected way: most users think they are “doing it wrong”, instead of blaming the designer of the software. When users are astonished they usually assume that they have made a mistake; they are unlikely to realize that the page has astonished them. They are more likely to feel that they are at fault for not anticipating the page. Don’t take advantage of this; making users feel stupid is not endearing. .right{ text-align: right;} The cranky user: The Principle of Least Astonishment 6.1.2.2 Think about the progression If there is a progression in your app, you should have designed a clear pattern of moving forward. If you need to bring your user from step 1 to step 7, you need to guide them through the whole process, and it can be as simple as putting “Next” buttons on the bottom of each page. Inside your app, this progression has to be clear, even more if step n+1 relies on the inputs from n. A good and simple way to do that is to hide elements at step n+1 until all the requirements are fulfilled at step n. Indeed, you can be sure that if step 2 relies on step 1 and you did not hide step 2 until you have everything you need, users will go to step 2 too soon. Another way to help this readability is to ensure some kind of linear logic through the app: step 1, data upload, step 2, data cleaning, step 3, data visualization, step 4, exporting the report. And organized your application around this logic, from left to right / right to left, or from top to bottom. Let’s compare {tidytuesday201942} to {hexmake} — one has a clear progression, {hexmake}, and has been designed as such: the upper menus design the stickers, and then once they are filled you can download them. So there is a progression here, from top to bottom. On the other hand, the {tidytuesday201942} does not have a real progression inside it: you can navigate from one tab to the other indifferently. Hence there is no visual clues of progression on that app. 6.1.2.3 Inputs and errors You’re the one developing the app, so of course you are conscious of all the inputs that are needed to complete a specific task. But your users might be new to the app, distracted while reading, they might not clearly understand what they are doing, maybe they do not really want to use your app but are forced to by their boss… Or maybe your app is a little bit hard to understand, so it is hard to know what to do at first. When building your app, you should make sure that if an input is necessary, it is made clear inside the app that it is. One way to do this is simply by hiding UI elements that can not be used until all the necessary inputs are there: for example, if a plot fails at rendering unless you have provided a selection, do not try to render this plot unless the selection is done. If you are building a dashboard and tab 2 needs specific inputs from tab 1, then tab 3 specific inputs from tab 2, then be sure that tab 2 and 3 are not clickable/available until all the required inputs are filled. That way, you can help the user navigate through the app, by reducing the cognitive load of having to be sure that everything is correctly set-up: if it is not clickable, that is because something is missing. And do this for all the elements in your app: for example with {hexmake}, we start with filled fields and an hex sticker which is ready, so that even if you start with the download part, the application would still work. If we had chosen another pattern, such as making the user fill everything before being able to download, we would have needed to make downloading impossible until all fields are filled. Another example from this application is the use of a MongoDB back-end to store the hex stickers: if the application is launched with with_mongo set to FALSE, the user will not see any buttons or field that refers to this options. Think about all the time when you are ordering something on the internet, and need to fill specific fields before being able to click on the “Validate” button. Well, apply that approach to your app, that will prevent from unwanted mistakes. Note that when using the golem::use_utils_ui() function, you will end with a script of UI tools, one being with_red_star, which adds a little red star at the end of the text you are entering, a common pattern for signifying that a field is mandatory: with_red_star(&quot;Enter your name here&quot;) Enter your name here* Also, be generous when it comes to errors: it is rather frustrating for a user to see an app crash without any explanation about what went wrong. So, if something fails or behaves unexpectedly, error messages are a key feature to help your user get on the right track. And, at the same time, helping them correct themselves after an error is the best way to save you time answering angry emails! Let’s refactor our app from before: library(shiny) ui &lt;- function(request){ tagList( selectInput( &quot;species&quot;, &quot;Choose one or more species&quot;, choices = unique(iris$Species), multiple = TRUE, selected = unique(iris$Species)[1] ), plotOutput(&quot;plt&quot;) ) } server &lt;- function( input, output, session ){ output$plt &lt;- renderPlot({ if (length(input$species) == 0){ shiny::showNotification( type = &quot;error&quot;, &quot;Species can not be empty&quot; ) } req(input$species) plot( iris[ iris$Species %in% input$species, ] ) }) } shinyApp(ui, server) Here, as a user, it is way easier to understand what went wrong: we have moved from a red error Error: need finite 'xlim' values to a pop-up explaining what went wrong in the way the user configured the app. Perfect way to reduce your bug tracker incoming tickets! This is a way to do it natively in Shiny, but note that you can also use the {shinyAlert} package to implement alerts. It is also possible to build your own with a little bit of HTML, CSS and JavaScript, as shown on the notifyjsexample repo. 6.2 The danger of feature-creep 6.2.1 What is feature-creep? Even more often (at least in the commercial software world) excessive complexity comes from project requirements that are based on the marketing fad of the month rather than the reality of what customers want or software can actually deliver. Many a good design has been smothered under marketing’s pile of “checklist features” — features that, often, no customer will ever use. And a vicious circle operates; the competition thinks it has to compete with chrome by adding more chrome. Pretty soon, massive bloat is the industry standard and everyone is using huge, buggy programs not even their developers can love. .right{ text-align: right;} The Art of Unix Programming Feature-creep is the process of adding features to the app that complexify the usage and the maintenance of the product, to the point that extreme feature-creep can lead to the product being entirely unusable and completely impossible to maintain. This movement always starts well-intentioned: easier navigation, more information, more visualizations, modifiable elements, and so on and so forth. It can come from project managers or dev, but users can also be responsible for asking more and more features in the app. If you are working in a context where the app specifications where designed by the users, or where you regularly meet the users for their feedback, they will most of the time be asking for more than what is efficiently implementable. Behind feature-creep, there is always a will to make the user experience better, but adding more and more things most of the time leads to a slower app, worst user experience, steeper learning curve, and all these bad states you do not want your app to be into. Let’s take a rather common data analytic process: querying data, cleaning them, then plotting and summarizing them. And let’s say that we want to add to this a simple admin dashboard, that tracks what the users do in the app. It’s pretty tempting to think of this as a unity and throw the whole code base into one big project and hope for the best. But let’s decompose what we have got there for a minute: one task is querying and cleaning, one other is analyzing, and one other is administration. What is the point of having one big app for these three different tasks? Splitting this project into three smaller apps will keep you from having a large app which is harder to maintain, and that might be less performing. Indeed, if you put everything into the same app, you will have to add extra mechanisms to prevent the admin panel from loading if your user simply wants to go to the extraction step, and inversely: a user visiting the admin panel probably does not need the extraction and analysis back-end to be loaded when they simply want to browse the way other users have been using the app. Or, as simply put in The Art of Unix Programing: Rule of Parsimony: Write a big program only when it is clear by demonstration that nothing else will do. But let’s focus on a smaller scope, and think about some things that can be thought of as feature-creeping your Shiny app. 6.2.2 Too much reactivity When designing an app, you will be designing the way users will navigate through the app. And most of the time, we design with the idea that the users will perform a “correct selection” pattern. Something like: “The user will select 40 on the sliderInput() and the plot will update automatically. Then the user will select the element they need in the selectInput() and the plot will update automatically”. When in reality what will happen is: “The user will click on the slider, aim at 40 but will reach 45, then 37, then 42, before having the right amount of 40. Then they will select something in the selectInput(), but chances are not the correct one from the first time.” In real life usage, people make mistakes using the app, they do not move the sliders to the right place, so if the application reacts to their every moves, the experience using the app can be bad: in the example above, full reactivity means that you will get 4 “wrong” computations of the plot before getting it right. In the {tidytuesday201942} application example, let’s imagine all the elements on the left automatically update the plot: especially in a context of a learning tool, reacting to any configuration change will launch a lot of useless computation, slowing the app in the long run, and making the user experience poorer. So what should we do? Prevent ourselves from implementing “full reactivity”: instead, we will add a user input that will launch the computation. The simplest solution being a button so that the user signals to the application than now they are ready for the application to compute what they have parametrized. 6.2.3 Too much interactivity Users love interactive elements. Maybe too much. If you present a user with a choice between a simple graph and a dynamic one, chances are that they will spontaneously go for the dynamic graph. Yet, dynamic is not always the solution, and for several reasons. 6.2.3.1 Speed Dynamic elements are slower to render than fixed one. Most of the time (if not always), rendering dynamic elements means that you will bind some external libraries, and maybe you will have to make R convert data from one format to another. For example, rendering a {ggplot2} plot will be faster than rendering a ggplotly() plot, which has to convert from one format to another25. That being said, not all visualization libraries are created equal, and choosing interactive visualization will not automatically lead to poorer performance: just keep in mind that this can happen. 6.2.3.2 Visual noise More interactivity can lead to an element being less straightforward to understand. Think for a minute about the {plotly} outputs. They are awesome if you need this kind of interactivity, but for a common plot there might be too many things to understand. Instead of focusing on the data, a lot of things show: buttons to zoom, to do selection, to export in png, and things like that. With this kind of graphs, users might lose some time focusing on understanding what the buttons do and why they are there, instead of focusing on what matters: getting insights from the data. Of course these features are awesome if you need them: exploring data interactively is a fundamental strength for an application when the context is right. But if there is no solid reason for using an interactive table, use a standard HTML table. In other words, do not make things interactive if there is no value in adding interactivity ; for example, if you have a small table and the users do not need to sort the table, filter, navigate in pages, DT::datatable() will add more visual noise than adding value to the application. Adding interactivity widgets (in most cases) means adding visual elements to your original content: in other words, you are adding visual components that might distract the user from focusing on the content of the information. To sum up, a good rule to live by is that you should not add a feature for the sake of adding a feature. Less is more. .right{ text-align: right;} Ludwig Mies van der Rohe 6.3 Web Accessibility 6.3.1 About Accessibility When building professional Shiny applications, you have to keep in mind that, potentially, this app will be consume by a large audience. A large audience means that there is a chance that your app will be used by people with visual, mobility, or maybe cognitive disabilities26. Web Accessibility deals with the process of making the web available to people with disabilities. The Web is fundamentally designed to work for all people, whatever their hardware, software, language, location, or ability. When the Web meets this goal, it is accessible to people with a diverse range of hearing, movement, sight, and cognitive ability. .right{ text-align: right;} Accessibility in Context - The Web Accessibility Initiative When learning to code a web app through “canonical” courses, you will be introduced to web Accessibility very early. For example, you can learn about this straight from the first chapter of learn.freecodecamp.org. The first course, “Responsive Web Design Certification”, has a chapter on web accessibility just after the one on HTML and CSS. 6.3.2 Making your App Accessible 6.3.2.1 Hierarchy Headers are not just there to make your application more stylish. &lt;h1&gt; to &lt;h6&gt; are there so they can create a hierarchy inside your webpage: &lt;h1&gt; being more important (hierarchically speaking) than &lt;h2&gt;. In a perfectly designed website, you would only have one header of level 1, a small amount of level 2 headers, more header of level 3, etc. These elements are used by screen readers (devices used by blind people) to understand how the page is organized. Hence, you should not rely on the header level for styling: do not use an &lt;h1&gt; because you need a larger title somewhere in your app. If you want to increase the size of an header, use CSS, which we will see in an upcoming chapter. 6.3.2.2 HTML element: Semantic tags, and tags metadata In HTML, there are two kind of elements: the one without “meanings” like &lt;div&gt; or &lt;span&gt;, and the one which are considered meaningful, like &lt;title&gt; or &lt;article&gt;. The second ones are called “semantic tags”, as they have a specific meaning in the sense that they define what they contain. Same thing as with headers, these elements are crucial for the screen readers to understand what the page contains. library(htmltools) tags$article( tags$h2(&quot;Title&quot;), tags$div(&quot;Content&quot;) ) One other HTML method you can use is tags attributes as metadata. Tags attributes are complementary elements you can add to a tag to add information: most of the time, you will be using it to add a CSS class, and identifier, or maybe some events like onclick27. But these can also be used to add, for example, an alternate text to an image: this alt being the one which is read when the image is not available, either because the page could not reach the resource, or because the person navigating the app is using a screen to speech technology. library(shiny) ui &lt;- function(request){ tagList( plotOutput(&quot;plot&quot;) %&gt;% tagAppendAttributes(alt = &quot;Plot of iris&quot;) ) } server &lt;- function( input, output, session ){ output$plot &lt;- renderPlot({ plot(iris) }) } shinyApp(ui, server) What makes these two things similar (semantic tags and tags metadata) is that they are both unseen by user without any impairment: if the image is correctly rendered and the user is capable of reading images, chances are that this user will see the image. But these elements are made for people with disabilities, and especially users who might be using screen to speech technologies: these visitors use a software that scans the textual content of the page and reads it, and that helps navigate through the page. This navigation is also crucial when it comes to screen to speech technology: these software will be able to read the &lt;title&gt; tag, jump to the &lt;nav&gt;, or straight to the &lt;article&gt; on the page. Hence the importance of structuring the page: these technologies need the app to be built in a structured way, so that it is possible to jump from one section to another, and other common tasks a fully capable user will commonly do. Some other tags exists and can be used for semantic purpose: for example &lt;address&gt;, &lt;video&gt;, or &lt;label&gt;. 6.3.2.3 Navigation Your app user might also have mobility impairment. For example, some with Parkinson might be using your app, or someone with a handicap making it harder for them to move their hand and click. For these users, moving an arm to grab the mouse might be challenging, and they might be navigating the web using their keyboard only. When building your app, thinking about how these users will be able to use it is crucial: maybe there are so may button they need to move their mouse to and click that they will not be able to use it. So, as much as possible, make everything doable with a keyboard: for example, if you have a textInput() with a validation button below, allow the user to validate by pressing the ENTER on their keyboard. Here is a small example of how to implement that: library(shiny) ui &lt;- function(request){ tagList( textInput(&quot;text&quot;, &quot;title&quot;) %&gt;% tagAppendAttributes( onKeyPress = &quot;Shiny.setInputValue(&#39;keypress&#39;, {value : event.key}, {priority: &#39;event&#39;})&quot; ), actionButton(&quot;go&quot;, &quot;Go&quot;) ) } server &lt;- function( input, output, session ){ observeEvent( input$keypress , { if (input$keypress$value == &quot;Enter&quot;){ # ... } }) } shinyApp(ui, server) Note that if you need a more systemic way to do this, you can do it with the {nter} package, which is available only on GitHub at the time of writing these lines: # Taken from https://github.com/JohnCoene/nter library(nter) library(shiny) ui &lt;- fluidPage( textInput(&quot;text&quot;, &quot;&quot;), actionButton(&quot;send&quot;, &quot;Do not click hit enter&quot;), verbatimTextOutput(&quot;typed&quot;), nter(&quot;send&quot;, &quot;text&quot;) # trigger &#39;send&#39; button when &#39;text&#39; is active. ) server &lt;- function(input, output) { txt &lt;- eventReactive(input$send, { input$text }) output$typed &lt;- renderPrint(txt()) } shinyApp(ui, server) 6.3.2.4 Color choices Color blindness is also a common impairment when it comes to web accessibility. And it is a rather common deficiency: according to colourblindawareness.org, “color (color) blindness (color vision deficiency, or CVD) affects approximately 1 in 12 men (8%) and 1 in 200 women in the world”. Keeping in mind this prevalence of color blindness is even more important in the context of Shiny, where we are developing data science products, which most of the time include data visualization. If designed wrong, dataviz can be unreadable for some specific type of color blindness. That is why we recommend using the viridis palette, which has been created to be readable by the most common types of color blindness. Here are for example a visualization through the lens of various type of color blindness: # Function to generate the graph with_palette &lt;- function(palette) { x &lt;- y &lt;- seq(-8 * pi, 8 * pi, len = 40) r &lt;- sqrt(outer(x^2, y^2, &quot;+&quot;)) filled.contour(cos(r^2) * exp(-r / (2 * pi)), axes = FALSE, color.palette = palette, asp = 1 ) } With the jet.colors palette from {matlab} with_palette(matlab::jet.colors) with_palette(viridis::viridis) Even without color-blindness, it’s already way more readable. But let’s now use the {dichromat} package to simulate color blindness. library(dichromat) library(purrr) Attaching package: &#39;purrr&#39; The following object is masked from &#39;package:magrittr&#39;: set_names Deutan with jet.colors and viridis graph &lt;- partial(dichromat, type = &quot;deutan&quot;) with_palette( compose( graph, matlab::jet.colors ) ) with_palette( compose( graph, viridis::viridis ) ) Protan with jet.colors and viridis graph &lt;- partial(dichromat, type = &quot;protan&quot;) with_palette( compose( graph, matlab::jet.colors ) ) with_palette( compose( graph, viridis::viridis ) ) Tritan with jet.colors and viridis graph &lt;- partial(dichromat, type = &quot;tritan&quot;) with_palette( compose( graph, matlab::jet.colors ) ) with_palette( compose( graph, viridis::viridis ) ) par(mfrow=c(2,1)) with_palette( compose( partial(dichromat, type = &quot;deutan&quot;), matlab::jet.colors ) ) with_palette( compose( partial(dichromat, type = &quot;deutan&quot;), viridis::viridis ) ) As you can see, the viridis palette always gives a more readable graph than the jet.colors one. And, the plus side, it looks fantastic. So do not hesitate to try and use it! 6.3.3 Evaluating your App Accessibility &amp; Further reading There are several tools on the web that can evaluate the accessibility of your webpage. You can also use a Google Chrome built-in tool called LightHouse (we will come back to it in the Testing chapter). Evaluating Web Accessibility, with lengthy reports and advice about checking the accessibility of your website https://www.webaccessibility.com/ has an online checker for webpage accessibility, and allows you to freely test 5 pages. {hexmake} accessibility results aka what happens in the server side of a Shiny App↩︎ "],
["step-design.html", "Chapter 7 Don’t rush into coding 7.1 Designing before coding 7.2 Ask questions", " Chapter 7 Don’t rush into coding 7.1 Designing before coding You have to believe that software design is a craft worth all the intelligence, creativity, and passion you can muster. Otherwise you will not look past the easy, stereotyped ways of approaching design and implementation; you will rush into coding when you should be thinking. You’ll carelessly complicate when you should be relentlessly simplifying — and theyou willll wonder why your code bloats and debugging is so hard. .right{ text-align: right;} The Art of Unix Programming - Attitude Matters Too 7.1.1 The Urge to Code At the moment you receive the specifications for your app, it is tempting to rush into coding. And that is perfectly normal: we’re Shiny developer because we love building software, so as soon as a problem emerges, our brain starts thinking about technical implementation, packages, pieces of code, and all these things that we love to do when we are building an app. But rushing into coding from the very beginning is not the safest way to go. Focusing on technical details from the very beginning can make you miss the big picture, be it for the whole app if you are in charge of the project, or for the piece of the whole app you have been assigned to. Have you ever faced a situation in a coding project where you tell yourself “Oh, I wish I had realized this sooner, because now I need to refactor a lot of my code for this specific thing”? Yes, we all have been in this situation: realizing too late that the thing we have implemented does not work with another feature we discover along the road. And what about “Oh I wish I had realized sooner that this package existed before trying to implement my own functions to do that!”. Same thing: we’re jumping straight into solving programming problem when someone else has open-sourced a solution to this very same problem. Of course, implementing your own solution might be a good thing in specific cases: avoiding heavy dependencies, incompatible licensing, the joy of intellectual challenge… But when building production software, it is safer to go for an existing solution if it exists and fits in the project: existing packages/software that are widely used by the community and by the industry benefit from wider testing, wider documentation, and a larger audience if you need to ask questions. And of course, it saves time, be it immediately or in the long run: re-using an existing solution allows you to save time re-implementing it, so you save time today, but it also prevent you from having to detect and correct bugs, saving you time tomorrow28. So, before rushing into coding, take some time to conceptualize your application / module on a piece of paper. That will help you get the big picture for the piece of code you will be writing: what are the inputs, what are the outputs, what packages / services you can use inside your application, how it will fit in the rest of the project. 7.1.2 Knowing where to search Being a good developer is knowing where to search, and what to search for. Here are a non-exhaustive list of places you can look for if you are stuck/looking for existing packages. 7.1.2.1 R &amp; Shiny CRAN Task View: Web Technologies and Services and CRAN Task View: Databases with R, which will be useful for interacting with web technologies and databases. The cloudyr project, which focuses on cloud services and R. The METACRAN, which is a search engine for R packages. GitHub search using language:R: when doing a search on GitHub, do not forget to add the language specific tag. RStudio Community has a series of post about Shiny: question, announcement, best practices… 7.1.2.2 Web Mozilla developer center is one of the most comprehensive resource platform when it comes to web technologies (HTML, CSS, and JavaScript) Google Developer Center also has a series of resources that can be helpful when it comes to web technologies. FreeCodeCamp contains more that 2000 hours of free courses about web technologies, plus a blog and forum. 7.1.3 About Concept Map Using concept map to think about your app can be a valuable tool to help you grasp the big picture of your application. Concept maps are a widely used tool, be it in the software engineering world and in many other fields. The idea with concept maps is to take a piece of paper (or a digital tool), where you draw all the concepts that come to mind for a specific topic, and all the relationships that link these concepts together. Drawing a concept map is a way to organize the knowledge of a specific topic. When doing this for a piece software, we are not trying to add technical details about the way things are implemented: we are listing down the various “actors” (the concepts) around our app, with the relationships they have. For example, here is a very simple concept map of the {hexmake} app. Built with XMind (https://www.xmind.net) As you can see, we are not detailing the technical implementations: we are not writing the external database specification, the connection process, how the different modules interact with each other… The goal of a concept map is to think about the big picture, to see the “who and what” of the application. Here, creating this concept map helps us list the flow of the app: there is a user, that wants to configure an hex, built with default image or with an uploaded one, and once this hex is finished, the user can either download it or register it in a database. This database can be browsed, and restore hex. The user can also export a .hex file, that can restore an app configuration. Once this general flow is written down, you can get back to it several times during the process of building the app, but it is also a perfect tool at the end to see if everything is in place: once the application is finished, we can question it: Can we point to any concept and see that it is there? Can we look at every relationship and see they all work as expected? Deciding which level of details you want to put in your concept map depends: “simple” applications probably do not need complex maps. And that also depends on how precise the specifications are, and how many people are working on the project: the concept map is a valuable tool when it comes to communication, as it allows people involved in the project to have visual clues of the conceptual architecture of the application. But beware: too complex maps are also unreadable! 7.2 Ask questions Before starting to code, the safe call will be to ask your team/client (depending on the project) a series of question just to get a good grasp of the whole project. Here is a (non-exhaustive) list of information you might need along the way. Side note: of course, these questions do not cover the core functionalities of the application: I’m pretty sure you have thought about covering this already. These are more contextual questions which are not directly linked to the application itself 7.2.1 About the end users Who are the end users of your app? Are they tech-literate? In which context will they be using your app? On what machines, and in what context? Will they be using the app in their office, on their phone while driving a tractor, in a plant while wearing lab coats? That might seems like weird questions if you are just focusing on the very technical side of the app implementation, but think about where the app will be used: the application used while driving agricultural machines might need less interactive things, bigger fonts, simpler interface, less details and more direct information. If you are building a Shiny app for a team of sellers who are always on the road, chances are they will need an app that they can browse from their mobile. And developing for mobiles requires a different kind of mindset29. Another good reason why talking to the users is an important step is that most of the time, people writing specifications are not the end users and will ask either too much features or not enough. Do the users really need that much interactive plots? Do they actually need that much granularity in the information? Will they really see a datatable of 15k lines? Do they really care about being able to zoom in the dygraph so that they can see the point at a minute scale? To what extent does the app has to be fast? Asking these questions is important, because building interactive widgets makes the app a little bit slower, and shoving a series of unnecessary widgets will make the user experience worse, adding more cognitive load than necessary. The speed of execution of your app is also an important parameter for your application: getting a sense about the need for speed in your application will allow you to judge whether or not you will have to focus on optimizing code execution. On top of that, remember all these things we have seen in the last chapter about accessibility: some of your end users might have specific accessibility requirements. 7.2.2 Pre-existing code-base From time to time, you are building a Shiny app on top of an existing code-base: either scripts with business logic, a package if you are lucky, or a POC for a Shiny app. These kind of projects are often referred to as “brownfield projects”, in opposition to “greenfield projects”, borrowing the terminology from urban planning: a greenfield project being one where you are building on “evergreen” lands, while a brownfield project is building on lands that were, for example, industrial lands, and which will need to be sanitized, as they potentially contain waste or pollution, constructions need to be destroyed, roads needs to be deviated, and all these things that can make the urban planning process more complex. Then, you can extend this to software engineering, where a greenfield project is the one that start from scratch, and a brownfield project is a one where you need to build on top of an existing code-base, implying that you will need to do some extra work before actually working on the project: “When transforming brownfield projects, we may face significant impediments and problems, especially when no automated testing exists, or when there is a tightly-coupled architecture that prevents small teams from developing, testing, and deploying code independtly.” .right{ text-align: right;} The DevOps Handbook Depending on how you chose to handle it, starting from an code base that is already written can either be very much helping, or you can be shooting yourself in the foot. Most of the time, Shiny projects are not built as reproducible infrastructures: you will find a series of library() calls, no functions structure per se, no documentation, and no tests. In that case, we would advise to do it “the hard way”, or at least what seems to be the hard way: throw the app out away and start from scratch… Well, not really from scratch: extract the core business logic of the app and make it a package. Take some time with the developer(s) that has built the current app, so that you can make them extract the core business logic, i.e. all the pieces of code that do not need a reactive context to run. Write documentation for this package, work on test, and once you are done, call it a day: you now have a solid ground for building the back-end, and it is built outside of any reactivity, is not linked to any application, and most of the time it can be used outside of the app. And it might actually be more useful than you think: it can serves analysts and data scientists that will benefit from these functions outside of the application, as they can use the business logic functions that are now packaged, and so reusable. Existing Shiny projects, in most cases, have not been build by software engineers nor web developers—they have been built by data analyst/scientists who wanted to create an interactive PoC for their work The good news then is that you can expect the core algorithms to be pretty solid and innovative. But web development is not their strength: and that is perfectly normal, as it is not their core job. What that implies is that most Shiny PoCs take shortcuts and rely on hacks, especially when it comes to managing reactivity, which is a beautiful concept for small project but can be very complex to scale if you are not a software engineer by training. Even more given that R is by nature sequential. That’s why it is better to split the business and app logic from the very beginning (as we have explained in chapter 3): it simplifies the process of refactoring a Shiny PoC into a production-grade Shiny application. 7.2.3 Deployment There are so many considerations about deployment that it will be very hard to list them all, but keep in mind that if you do not ask questions about where your application will be deployed from the start, you might have bad surprises when you will send your app to production. Of course, it is more or less solved if you are deploying with Docker: if it works in a container on your machine, it should work in production, but that is not as simple as that: for example, building a Shiny application that will be used by 10 people is not the same as building an application that needs to scale to 50.000 users. Learning at the end of the project that “now we need to scale to a very large user base” might prevent the deployment from being successful, as this kind of scale implies specific consideration while building. But that is just the tip of the iceberg. One time, we built an app that had to do some API requests. So far so good, nothing too complicated… until we discovered that the server where the app will be deployed does not have access to the internet, making it impossible to issue API requests from the server. Here, the containers worked on our machine, as they had access to the internet. Once deployed, the app stopped working, and we lose a couple of days of exchanges with the client, trying to debug our API calls, until we realized that the issue was not with our app, but with the server itself. It’s even more important to think about the IT side of your application, as the people writing specs and interacting with you might come from the Data Science team, and they might or might not have discussed with the IT team about deploying the app. There is a chance that they do not have in mind all what is needed to deploy a Shiny App on their company server. For example, maybe your application has a database back-end. For that, you will need to have access to this database, the correct port should be set, and the permission given to the process that executes the Shiny app to read, and maybe write, to the database. But, and for good reason, database managers do not issue read and write permissions to a DB without having examined what the app wants to read, and how and where it will write. So, if you do not want to have weeks of delay for your app deployment, start the discussion from the very beginning of the project. That way, even if the process of getting permission to write on the company database takes time, you might have it by the end of the coding marathon. "],
["css.html", "Chapter 8 A Gentle Introduction to CSS 8.1 What is CSS 8.2 Getting started with CSS 8.3 Integrate CSS files to your Shiny App 8.4 External resources", " Chapter 8 A Gentle Introduction to CSS 8.1 What is CSS 8.1.1 About CSS CSS, for Cascading Style Sheets, is one of main technologies that power the web today, along with HTML and JavaScript. HTML is a series of tags that define your web page structure, and JavaScript is a programming language that allows you to manipulate the page (well, it can do a lot more than that, but we are simplifying to make it understandable). CSS is what handles the design, i.e. the visual rendering of the web page: the color of the header, the font, the background, and everything that makes a web page looks like it is not from 1983 (again, we are simplifying for the sake of clarity). On every browser, each HTML element has a default style: for example all &lt;h1&gt; have the size 2em and are in bold, and &lt;strong&gt; is in bold. But we might not be happy with what a “standard page” (with no CSS) looks like: that is the very reason of CSS, modifying the visual rendering of the page. If you want to get an idea of the importance of CSS, try installing extensions like Web Developer for Google Chrome. Then, if you go on the extension and choose CSS, click “Disable All Style”, to see what a page without CSS looks like. For example, here is what rtask.thinkr.fr looks like: And here is what it looks like without CSS: CSS now seems pretty useful right? 8.1.2 Shiny’s default: fluidPage() In Shiny, there is a default CSS: the one from Bootstrap 3. As you can see if you have created a fluidPage() before, there is already styling applied. Compare: (no fluidPage) library(shiny) ui &lt;- function(request){ tagList( h1(&quot;Hey&quot;), h2(&quot;You&quot;), p(&quot;You rock!&quot;), selectInput(&quot;what&quot;, &quot;Do you&quot;, unique(iris$Species)) ) } server &lt;- function( input, output, session ){ } shinyApp(ui, server) Shiny applications not supported in static R Markdown documents to (with fluidPage): library(shiny) ui &lt;- function(request){ fluidPage( h1(&quot;Hey&quot;), h2(&quot;You&quot;), p(&quot;You rock!&quot;), selectInput(&quot;what&quot;, &quot;Do you&quot;, unique(iris$Species)) ) } server &lt;- function( input, output, session ){ } shinyApp(ui, server) Shiny applications not supported in static R Markdown documents Yes, that is subtle, but you can see how it makes the difference on larger apps. 8.2 Getting started with CSS CSS is a descriptive language, meaning that you will have to declare the style either on a tag or inside an external file. We will see how to integrate CSS inside your Shiny application in the next section, but before that, let’s start with a short introduction to CSS30. 8.2.1 About CSS syntax CSS syntax is composed of two elements: a selector, and a declaration block. The CSS selector describes how to identify the HTML tags that will be affected by the style declared with key-value pairs in the declaration block that follows. And because an example will be easier to understand, here is a simple CSS rule: h2 { color:red; } Here, the selector is h2, meaning that the HTML tags aimed by the style are the &lt;h2&gt; tags. The declaration block contains the key-value pair telling that the color will be red. Note that each key-value pair must end with a semicolon. 8.2.2 CSS selectors CSS selectors are a wide topic, as there are many combinations of things you might want to select inside an HTML page. The first type of selectors are the “standard” ones name, id, or class. These refer to the elements composing an HTML tag: for example, with &lt;h2 id = \"tileone\" class = \"standard\"&gt;One&lt;/h2&gt;, the name is h2, the id tileone, and the class standard31. To select these three elements in CSS: Write the name as-is: h2 Prefix the id with #: #tileone Prefix the class with .: .standard You can also combine these elements, for example h2.standard will select all the h2 tags with a class standard, and h2,h3 will select the h2 and the h3. You can build more complex selectors: for example div.standard &gt; p will select all the &lt;p&gt; tags that are contained inside a div of class standard (CSS combinator), or a:hover, which dictates the style of the a tags when they are hovered by the mouse (CSS pseudo-class), div.standard::first-letter, that select the first letter of the div of class standard (CSS pseudo-elements), and h2[data-value=\"hey\"], which selects all the h2 with a data-value attribute set to \"hey\" (CSS attribute selector). As you can see, lots of complex selectors can be built with CSS, to target very specific elements of your UI. But mastering these complex selectors is not the main goal of this chapter, hence we will just be using standard selectors in the rest of the examples in this book. 8.2.3 CSS properties Now that you have selected elements, it is time to apply some styles! Between the brackets of the declaration block, you will have to define a series of key-value elements defining the properties of the style: the key here is the css property, followed by its value. For example, color: red; or text-align: center; define that for the selected HTML elements, the color will be red, or the text centered. We will not cover all the possible properties, as there hundreds of them. Feel free to refer to the CSS Reference page from Mozilla for an exhaustive list of available properties. 8.3 Integrate CSS files to your Shiny App Now you have got a grasp on how to getting started writing your own CSS, how do you integrate it inside your Shiny Application? There are three methods that can be used: writing it inline, integrating it inside a tags$script() straight into you application UI code, or by writing it into an external file. Note that the good practice is considered to be the integration of an external file. 8.3.1 Inline CSS If you need to add style to one specific element, you can write it straight inside the HTML tag: library(shiny) ui &lt;- function(request){ tagList( h2(style = &quot;color:red;&quot;, &quot;This is red&quot;) ) } server &lt;- function( input, output, session ){ } shinyApp(ui, server) Shiny applications not supported in static R Markdown documents But this method loses all the advantages of CSS, notably the possibility to apply style to multiple elements. Use it with caution. 8.3.2 Writing in a tags$style() If you had a tags$style() somewhere inside your UI code (generally at the very beginning of your UI), you can then add CSS code straight to your application. Here is an example: library(shiny) ui &lt;- function(request){ tagList( tags$style( &quot;h2{ color:red; }&quot; ), h2(&quot;This is red&quot;) ) } server &lt;- function( input, output, session ){ } shinyApp(ui, server) Shiny applications not supported in static R Markdown documents This works, but should not be considered as the best option: indeed, if you have a large amount of CSS code to insert to your app, it can make the code harder to read as it adds a large amount of visual noise. The best solution then is to go with the alternative of writing the CSS inside a separate file: it allows to separate things and to make the UI code lighter, as it is easier to maintain a separate CSS file than CSS written straight into R code. 8.3.3 Including External Files To include an external CSS file, you will have to use another tags: tags$link(). What this tag will contain is these three elements: rel=\"stylesheet\" type=\"text/css\" href=\"www/custom.css\" The first two are standard: you do not need to change them, they are necessary to indicate to the HTML page that you are creating a stylesheet, with the type being text/css. The href is the one you will need to change: this path points to where your style file is located. If you are building your application with {golem}, the good news is that this file creation and linking is transparent: if you call golem::add_css_file(\"name\"), a file will be created at inst/app/www, and this file will be automatically linked inside your UI thanks to the bundle_resouces() function. 8.3.4 Using R packages If you want to use external CSS template, there are several packages that exist that can do implement new custom UI designs for your application. Here are some: {resume}, provides an implementation of the Bootstrap Resume Template. {nessy}, a port of NES CSS. {skeleton}, Skeleton CSS. {shinyMobile}, shiny API for Framework7 (IOS/android). {shinydashboardPlus}, extensions for shinydashboard. {bs4Dash}, Bootstrap 4 shinydashboard using AdminLTE3. {fullPage}, fullPage.js, pagePiling.js and multiScroll.js for shiny. And all the amazing things done at RinteRface. 8.4 External resources If you want to learn more about CSS, there are three places where you can get started: FreeCodeCamp, with contains many hours of course around HTML and CSS. W3 Schools CSS Tutorial Learn to style HTML using CSS "],
["setting-up-for-success-with-golem.html", "Chapter 9 Setting up for success with {golem} 9.1 Create a {golem} 9.2 Setting things up with dev/01_start.R 9.3 Setting infrastructure for prototyping", " Chapter 9 Setting up for success with {golem} Before starting to prototype and build anything, initialize a {golem} project! This will help you put your application on solid ground, and once the project is ready to be filled, you can start prototyping right inside it! The general workflow for “prototype and build” is the following: the project manager sets up a {golem} project, where the first steps are filled, the general structure, with Shiny module, is set, and then the project is registered to the version control system. Once we have this structure, package and modules combined, we can start prototyping the UI inside the module, work on the CSS and JavaScript elements that might be needed, and prototyping the back-end functionalities inside Rmarkdown files. And then, once this two prototyping sides are finished, we work on the integration of everything inside the reactive context. In this chapter and in chapter 11, we will be presenting the {golem} package in more depth. {golem} is a framework that standardize the process of building production-ready Shiny Applications. 9.1 Create a {golem} Once {golem} is installed and available on your computer, you can got to File &gt; New Project… in RStudio, and choose “Package for Shiny App Using golem” input. If you want to do it through command line, you can use: golem::create_golem(path = &quot;path/to/package&quot;) Once you have got that, a new project will be launched. Here is the structure of this project: fs::dir_tree(&quot;golex&quot;) [01;34mgolex[0m ├── DESCRIPTION ├── NAMESPACE ├── [01;34mR[0m │ ├── [32mapp_config.R[0m │ ├── [32mapp_server.R[0m │ ├── [32mapp_ui.R[0m │ └── [32mrun_app.R[0m ├── [01;34mdev[0m │ ├── [32m01_start.R[0m │ ├── [32m02_dev.R[0m │ ├── [32m03_deploy.R[0m │ └── [32mrun_dev.R[0m ├── [01;34minst[0m │ ├── [01;34mapp[0m │ │ └── [01;34mwww[0m │ │ └── favicon.ico │ └── golem-config.yml └── [01;34mman[0m └── run_app.Rd If you are already familiar with R packages, most of these files will appear very familiar to you. That’s because a {golem} app IS a package. For more details about these files, please refer to the Understanding {golem} app structure part of this book. In this part, we will go through the dev/01_start.R and dev/02_dev.R scripts. 9.2 Setting things up with dev/01_start.R Once you have created your project, the first file that opens is dev/01_start.R. This file contains a series of commands to run once, at the start of the project. These are the commands you will be using at 9.2.1 Fill the DESCRIPTION and set options First, fill the DESCRIPTION file by adding information about the package that will contain your app: golem::fill_desc( pkg_name = &quot;ipsumapp&quot;, # The Name of the package containing the App pkg_title = &quot;PKG_TITLE&quot;, # The Title of the package containing the App pkg_description = &quot;PKG_DESC.&quot;, # The Description of the package containing the App author_first_name = &quot;AUTHOR_FIRST&quot;, # Your First Name author_last_name = &quot;AUTHOR_LAST&quot;, # Your Last Name author_email = &quot;AUTHOR@MAIL.COM&quot;, # Your Email repo_url = NULL # The URL of the GitHub Repo (optional) ) Then, call the golem::set_golem_options() function, it will add information to the golem-config.yml file, and notably set the {here} package root sentinel. 9.2.2 Set common Files If you want to use the MIT license, add README, a code of conduct, a lifecycle badge, and NEWS usethis::use_mit_license( name = &quot;Golem User&quot; ) # You can set another license here usethis::use_readme_rmd( open = FALSE ) usethis::use_code_of_conduct() usethis::use_lifecycle_badge( &quot;Experimental&quot; ) usethis::use_news_md( open = FALSE ) It’s also where you will be invited to use git usethis::use_git() 9.2.3 Init Tests Create a template for tests: golem::use_recommended_tests() 9.2.4 Use Recommended Elements golem::use_recommended_tests() and golem::use_recommended_deps() sets a default testing infrastructure and adds dependencies to the application. 9.2.5 Add utilitary functions These two functions add a file with various functions that can be used along the process of building your app. See each file in details for a description of the functions. golem::use_utils_ui() golem::use_utils_server() 9.2.6 If you want to change the default favicon golem::use_favicon( path = &quot;path/to/favicon&quot;) You’re now set! You’ve successfully initiated the project and can go to dev/02_dev.R. 9.3 Setting infrastructure for prototyping 9.3.1 Add modules in dev/02_dev.R The golem::add_module() functions creates a module in the R folder. The file and the modules will be named after the name parameter, by adding mod_ to the R file, and mod_*_ui and mod_*_server to the UI and server functions. golem::add_module(name = &quot;my_first_module&quot;) # Name of the module [32m✔[39m File created at R/mod_my_first_module.R [31m●[39m Go to R/mod_my_first_module.R The new file will contain: #&#39; my_first_module UI Function #&#39; #&#39; @description A shiny Module. #&#39; #&#39; @param id,input,output,session Internal parameters for {shiny}. #&#39; #&#39; @noRd #&#39; #&#39; @importFrom shiny NS tagList mod_my_first_module_ui &lt;- function(id){ ns &lt;- NS(id) tagList( ) } #&#39; my_first_module Server Function #&#39; #&#39; @noRd mod_my_first_module_server &lt;- function(input, output, session){ ns &lt;- session$ns } ## To be copied in the UI # mod_my_first_module_ui(&quot;my_first_module_ui_1&quot;) ## To be copied in the server # callModule(mod_my_first_module_server, &quot;my_first_module_ui_1&quot;) Note that in order not to make errors when putting these into your app, the end of the file will contain code that has to be copied and pasted inside your UI and server functions. This is where you will be adding the core of your app. In a first time, these modules will contain prototyped UI for the application, and once the application is ready to be integrated, you will add the core logic here. 9.3.2 Add CSS &amp; JS Files Adding some infrastructure for JavaScript and CSS file from the very beginning can also formalize the set-up: you are giving the rest of your team a golem::add_js_file( &quot;script&quot; ) Will generate the following file: $( document ).ready(function() { }); Here, you will have an infrastructure for launching JavaScript code once the application is ready (this code is standard jQuery format: we will be back to JavaScript at the end of this book). golem::add_js_handler( &quot;handlers&quot; ) Will generate the following file: $( document ).ready(function() { Shiny.addCustomMessageHandler(&#39;fun&#39;, function(arg) { }) }); As you can see, there is already a skeleton for building Shiny JavaScript handlers. More on that structure in the last chapter of the book! golem::add_css_file( &quot;custom&quot; ) Will create a blank CSS file inside the inst/app/www folder. Note that if you are building your application with {golem}, these files will be linked automatically to your application. "],
["stepprotopype.html", "Chapter 10 Building an “ipsum-app” 10.1 Prototyping is crucial 10.2 Prototyping Shiny 10.3 Building with RMarkdown", " Chapter 10 Building an “ipsum-app” 10.1 Prototyping is crucial 10.1.1 Prototype, then polish Prototyping first may help keep you from investing far too much time for marginal gains. http://www.catb.org/~esr/writings/taoup/html/ch01s06.html#rule_of_optimization And yet another Rule from the Art of Unix Programming: “Rule of Optimization: Prototype before polishing. Get it working before you optimize it.” Getting things to work before trying to optimize the app is always a good approach: Making things work before working on low level optimization makes the whole engineering process easier: having a “minimal viable product” that works, even if slowly and not perfectly, gives a stronger sense of success to the project. For example if you are building a vehicle, it feels more of a success to start with a skateboard than with a wheel: you quickly have a product that can be used to move, not waiting for the end of the project before finally having something useful. Abstraction is hard, and makes the code base harder to work with. You have heard a lot that if you are copying and pasting something more than twice, you should write a function. And with Shiny, if you are writing a piece of the app more than twice, you should write modules. But while these kind of abstractions are elegant and optimized, they can make the software harder to work on while building it. So before focusing on turning something into a function, make it work first. As said in R for Data Science about abstraction with {purrr}: Once you master these functions, you’ll find it takes much less time to solve iteration problems. But you should never feel bad about using a for loop instead of a map function. The map functions are a step up a tower of abstraction, and it can take a long time to get your head around how they work. The important thing is that you solve the problem that you’re working on, not write the most concise and elegant code (although that’s definitely something you want to strive towards!). .right{ text-align: right;} R for Data Science - 21.5 The map functions As a small example, we can refer to the binding module from {hexmake}: this module manipulates namespaces, inputs and session to automatically bind inputs to the R6 object containing the image (see implementation here, here and here). That’s an elegant solution: instead of duplicating content, we use functions to automatically bind events. But that is a higher level of abstraction: we manipulate different levels of namespacing and inputs, making it harder to reason about when you have to change the code base. It’s hard to identify upfront the real bottlenecks of the app. As long as the app is not in a working state, it is very hard to identify the real pieces of code that need to be optimized. Chances are that if you ask yourself upfront what the app bottlenecks will be, you will not aim right. So instead of losing time focusing on specific pieces of code you think need to be optimized, start by having something that works, then optimize the code. In other words, “Make It Work. Make It Right. Make It Fast”, (KentBeck). It’s easier to spot mistakes when you have something the can run. If a piece of software runs, it is straightforward to check if a change in the codebase break the software or not: it either still run or not. 10.1.2 The “UI first” approach Using what can be called a “UI first” approach when building an app is in most cases the safest way to fo. And for two main reasons. 10.1.2.1 Agreeing on specification First of all, it helps everybody involved in the application to agree on what the app is supposed to do, and once the UI is set, there should be no “surprise implementation”. Well, at least, this is the best way to reduce the number of changes in the app, as the sooner we have a global idea of the app the better: it is hard to implement a core new feature once the app is 90% finished, while it would have been way easier to implement it if it has been detected from the very start. Indeed, implementing core feature once the app is very advanced can be critical, as our application might not have been thought to work the way it now needs to work, so adding certain elements might lead to a need for change in the core architecture of the app. Once we agree on what elements compose the app, there should be no sudden “oh the app needs to do that thing now, sorry I hadn’t realized that before”. And we can not blame the person ordering the app for not realizing everything needed: it is really hard to have a mental model of the whole app when we are writing specifications, not to mention when reading them. On the other hand, having a mock application with the UI really helps realizing what the app is doing and how it works, and to agree with the developer that this is actually what we want our application to do (or realizing that this is not something we actually need). Prototyping the UI first should require the least possible computation from the server-side of your application. You focus on the appearance of the app: buttons, figures, tables, graphs… and how they interact with each other. At that stage of the design process, you will not be focusing on correctness of the results or graphs: you will be placing elements on the front-end so that you can be sure that everything is there, even if some buttons do not trigger anything. At that point, the idea is to get the people ordering the app think about what they actually need, and there might be some question rising like “oh, where is the button to download that results in pdf?”. And at that precise moment is the perfect time for a change in specification. 10.1.2.2 Organising work A pre-defined UI allows every person involved in the coding process to know which part of the app they are working on, and to be sure that you do not forget anything. As you might be working on the app as a team, you will need to find a strategy for efficiently splitting the work between every coder. And it’s much easier to work on a piece of the app you can visually identify and integrate in a complete app scenario. In other words, it is easier to be told “you will be working on the ‘Summary’ panel from that mock UI” than “you will be working on bullet point 45 to 78 of the specifications”. 10.2 Prototyping Shiny In the next section, you will be introduced to two packages that can be used when prototyping user interface: {shinipsum} and {fakir}. 10.2.1 Fast UI Prototyping with {shinipsum} When prototyping the UI for an application, we will not be focusing on building the actual computation: what we need is creating a draft with visual components, so that we can have visual clues about the end result. To do that, you can use the {shinipsum} package, which has been designed to generate random shiny elements. If you are familiar with “lorem ipsum”, the fake text generator that is used in software design as a placeholder for text, the idea is the same: generating placeholders for Shiny outputs. You can install this package from GitHub with: remotes::install_github(&quot;Thinkr-open/shinipsum&quot;) In this package, a series of functions that generates random placeholders. For example, random_ggplot() generates random {ggplot2} elements. If we run this code two time, we should get different results32: library(shinipsum) library(ggplot2) random_ggplot() + labs(title = &quot;Random plot&quot;) random_ggplot() + labs(title = &quot;Random plot&quot;) Of course, the idea is to combine this with a Shiny interface, for example random_ggplot() will be used with a renderPlot() and plotOutput(). And as we want to prototype but still be close to what the app might look like, these functions take arguments that can shape the output: for example, random_ggplot() has a type parameter that can help you select a specific geom. library(shiny) library(shinipsum) library(DT) Attaching package: &#39;DT&#39; The following objects are masked from &#39;package:shiny&#39;: dataTableOutput, renderDataTable ui &lt;- fluidPage( h2(&quot;A Random DT&quot;), DTOutput(&quot;data_table&quot;), h2(&quot;A Random Plot&quot;), plotOutput(&quot;plot&quot;), h2(&quot;A Random Text&quot;), tableOutput(&quot;text&quot;) ) server &lt;- function(input, output, session) { output$data_table &lt;- DT::renderDT({ random_DT(5, 5) }) output$plot &lt;- renderPlot({ random_ggplot() }) output$text &lt;- renderText({ random_text(nwords = 50) }) } shinyApp(ui, server) Shiny applications not supported in static R Markdown documents Other {shinipsum} functions include: tables: random_table(nrow = 3, ncol = 10) Sepal.Length Sepal.Width Petal.Length Petal.Width 1 5.1 3.5 1.4 0.2 2 4.9 3.0 1.4 0.2 3 4.7 3.2 1.3 0.2 Species Sepal.Length.1 Sepal.Width.1 Petal.Length.1 1 setosa 5.1 3.5 1.4 2 setosa 4.9 3.0 1.4 3 setosa 4.7 3.2 1.3 Petal.Width.1 Species.1 1 0.2 setosa 2 0.2 setosa 3 0.2 setosa print outputs: random_print(type = &quot;model&quot;) Call: lm(formula = Sepal.Length ~ Sepal.Width, data = datasets::iris) Coefficients: (Intercept) Sepal.Width 6.526 -0.223 … and text, image, ggplotly, dygraph, and DT. 10.2.2 Using {fakir} for fake data generation Generating random placeholder for Shiny might not be enough: maybe you also need example datasets. This can be accomplished using the {fakir} package, which was primarily created to provide fake datasets for R tutorials and exercises, but that can easily be used inside a Shiny application. At the time of writing these lines, the package is only available on GitHub, and can be installed with: remotes::install_github(&quot;Thinkr-open/fakir&quot;) This package contains three “datasets”, that are randomly generated when you call the corresponding functions: fake_base_clients() generates a fake dataset for a ticketing service fake_sondage_answers() is a fake survey about transportation fake_visits() is a fake dataset for the visits on a website library(fakir) fake_visits(from = &quot;2017-01-01&quot;, to = &quot;2017-01-31&quot;) [90m# A tibble: 31 x 8[39m timestamp year month day home about blog [90m*[39m [3m[90m&lt;date&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;int&gt;[39m[23m [3m[90m&lt;int&gt;[39m[23m [3m[90m&lt;int&gt;[39m[23m [3m[90m&lt;int&gt;[39m[23m [90m 1[39m 2017-01-01 [4m2[24m017 1 1 369 220 404 [90m 2[39m 2017-01-02 [4m2[24m017 1 2 159 250 414 [90m 3[39m 2017-01-03 [4m2[24m017 1 3 436 170 498 [90m 4[39m 2017-01-04 [4m2[24m017 1 4 [31mNA[39m 258 526 [90m 5[39m 2017-01-05 [4m2[24m017 1 5 362 [31mNA[39m 407 [90m 6[39m 2017-01-06 [4m2[24m017 1 6 245 145 576 [90m 7[39m 2017-01-07 [4m2[24m017 1 7 [31mNA[39m [31mNA[39m 484 [90m 8[39m 2017-01-08 [4m2[24m017 1 8 461 103 441 [90m 9[39m 2017-01-09 [4m2[24m017 1 9 337 113 673 [90m10[39m 2017-01-10 [4m2[24m017 1 10 [31mNA[39m 169 308 [90m# … with 21 more rows, and 1 more variable:[39m [90m# contact [3m[90m&lt;int&gt;[90m[23m[39m The idea with these datasets is to combine various formats that can reflect “real life” datasets: they contain dates, numeric and character variables, and have missing values. They can also be manipulated with the included {sf} geographical dataset fra_sf allowing for maps creation. Fake datasets created with {fakir} can be used to build light examples on the use of the inputs, for filters or interactive map, or as examples for the internal functions and their corresponding documentation. 10.3 Building with RMarkdown While on one side you are building the user interface, you (or someone from your team) can start working on the back-end implementation. Again, this should be done out of any reactive logic: the back-end should not depend on any reactive context. And because documentation is gold, you should start with writing the back-end documentation straight as package documentation, inside your Vignettes. Or what we call “Rmd-first”. 10.3.1 Define the content of the application Rmarkdown files are the perfect spot to sandbox the back-end of your application: inside the file, you are not disrupted by any reactive behavior, as you are just working with plain old R code: data wrangling operations, multi-parameters based models, summary tables outputs, graphical outputs… And the nice thing is that you can share the output of the rendered file as an HTML or PDF to either your client or boss, or anyone ordering the application. That way, you can drive the discussion to be focused on discussing the core algorithm, not some details like “I want the button to be blue” when what you need to know is if the output of the model is correct-in other words, you are again applying the rule of the separation of concerns, i.e. you help focusing on one part of the application without adding any cognitive load to the person “reading” the outputs. And also, last but not least, if you have to implement changes to the back-end functions, it is way easier to check and to share in a static file than in an application. When doing that, the best way is again to separate things: do not be afraid of writing multiple RMarkdown files, one for each part of the end application. Again, this will help everybody focus on what matter: be it you, your team, or the person ordering the application. Building the back-end in Rmd files is also a good way to make the back-end “application independent”, in the sense that it helps documenting how the algorithms you have been building can be used outside of the application. In many cases, when you are building an application, you are creating functions that contain business logic/domain expertise, and that can in fact be used outside of the application. Writing these functions and how they work together forces you to think about these functions, and also gives a good starting point for anybody familiar with R that would want to start using this back-end toolkit. Of course, as you are building your application as a package, it is way easier now: you can share a package with the application inside it, along with a function to launch the app but also functions that can be used outside. And if you need some data to use as an example, feel free to pick one from {fakir}! (See §10). 10.3.2 Using the Rmd files as a laboratory notebook Rmd can also be used as the place to keep track of what you have in mind while creating the application: most of the time, you will create the functions inside the R/ folder, but it might not be the perfect place to document your thought process. On the other hand, using Markdown as a kind of “software laboratory notebook” to keep track of your idea is a good way to document all the choices you have made about your data wrangling, models, visualization, so that you can use it as a common knowledge-base all along the application life: you can share this with your client, with the rest of your team, or with anybody involved in the project. And also, developing in multiple Rmd files helps the separation of work between multiple developers, and will reduce code conflicts during development. 10.3.3 Rmd, Vignettes, and documentation first Working with the {golem} framework implies that you will build the application as an R package. And of course, an R package implies writing documentation: one of the main goals of the Vignettes, in an R package, is to document how to use the package. And the good news is that when checking a package, i.e when running devtools::check() or R CMD check, the Vignettes are going to be built, and the process will fail if at least one of the Vignette fails to render. That way, you can use the documentation of the back-end as an extra tool for doing unit-testing! One radical approach to the “Rmd first” philosophy is to write everything in an Rmd from the very beginning of your project: write the function code, there roxygen tags, their tests, etc, then move everything to the correct spot in the package infrastructure once you are happy with everything. And of course, when you need to add another feature to your app, open a new markdown and start the process of development and documentation again. "],
["stepbuild.html", "Chapter 11 Building app with {golem} 11.1 Add dependencies 11.2 Submodules and utility functions 11.3 Add tests 11.4 Documentation and Code Coverage 11.5 Using {golem} dev functions", " Chapter 11 Building app with {golem} Now that the application is prototyped inside a {golem} skeleton, you can now work on the integration of the application. This parts encompasses the integration of the back-end/front-end, together, and working on the software engineering task of the application: add and organize dependencies creating and including sub-modules if necessary organize utility functions and link them to the module they are used in adding testing infrastructure linking to CI / CD services check 11.1 Add dependencies Every time you add a new dependency, you can add it with: usethis::use_package(&quot;pkg&quot;) Note that all the explicitly namespaced calls (i.e pkg::fun) can be scraped using the {attachment} package: attachment::att_from_rscripts() 11.2 Submodules and utility functions Two functions can be called to add utility functions: golem::add_fct( &quot;helpers&quot; ) golem::add_utils( &quot;helpers&quot; ) The first will create a R/fct_helpers.R file The second will create a R/utils_helpers.R file 11.3 Add tests You can add more tests to your application with: usethis::use_test(&quot;app&quot;) 11.4 Documentation and Code Coverage 11.4.1 Vignette Whenever you need to add a new Vignette, you can call: usethis::use_vignette(&quot;shinyexample&quot;) The Vignettes can then be compiled with: devtools::build_vignettes() 11.4.2 Code coverage usethis::use_travis() usethis::use_appveyor() usethis::use_coverage() 11.5 Using {golem} dev functions There is a series of tools to make your app behave differently whether it is in dev or prod mode. Notably, the app_prod() and app_dev() function look for the value of options( \"golem.app.prod\" ), or return TRUE if this option does not exist. Setting this options at the beginning of your dev process allows to make your app behave in a specific way when you are in dev mode. For example, printing message to the console with cat_dev(). options( &quot;golem.app.prod&quot; = TRUE) golem::cat_dev(&quot;hey\\n&quot;) options( &quot;golem.app.prod&quot; = FALSE) golem::cat_dev(&quot;hey\\n&quot;) hey You can then make any function being “dev-dependant” with the make_dev() function: log_dev &lt;- golem::make_dev(log) log_dev(10) [1] 2.303 options( &quot;golem.app.prod&quot; = TRUE) log_dev(10) "],
["step-secure.html", "Chapter 12 Build yourself a safety net 12.1 Testing your app 12.2 A reproducible environment", " Chapter 12 Build yourself a safety net “Don’t fuck over Future You” JD Strengthening your app means two things: testing, and locking the application environment. 12.1 Testing your app The process of getting your application production-ready implies that the application is tested. With a robust testing suite, you will develop, maintain, and improve in a safe environment and ensure your project sustainability. What will you be testing? Both sides of the application: the business logic and the user interface. And also, the application load, i.e how much time and memory is required when your application starts being used by a significant amount of users, be it from the user perspective (how many time does it take to complete a full scenario) and from the server perspective (how many memory is needed for my app to run). 12.1.1 Testing the business logic If you have been following the good practices we have listed in previous chapters, your current application has at least these two properties: The business-logic functions are separated from your interactive-logic functions. Your application is inside a package. On top of being a sane organization approach, using this separation inside a package structure allows to leverage all the tooling that has been built for testing “standard” packages. R developers have been developing packages for a long time, and at the time of writing these lines (April 2020), more than 15,000 packages are available on CRAN. To sustain these developments, a lot of tools have been created to secure the development process, and especially tools for creating unit tests for your package. Unit tests are a general concept in software engineering that describes the process of writing a form of assessment to check the validity of your code. A simplified explanation is that if you write a function call meaning_of_life that returns 42, you will expect this function to always return 42, and to be alerted if ever this value changes. Using unit tests is a way to secure your work in the future, be it for future you, for your collaborator, or for anybody wanting to collaborate to the project: if anyone comes and change the code behind the meaning_of_life() function, and the result is no longer 42, the developer working on this piece of code will be able to catch it. The general idea is to detect bugs and breaking changes at the moment they are happening, not once it is too late. There are several packages in R that can be used to implement unit testing, and you can even implement your own tests. One of the most popular right now33 is {testthat} (???).. This testing framework lets you write a series of tests and expectations, which are then launch when calling devtools::test(), either locally or in you CI system. Here is an example of testing that the meaning_of_life will always be 42. test_that(&quot;The meaning of life is 42&quot;, { expect_equal( meaning_of_life(), 42 ) }) Once you have this test skeleton set, you will be able to detect any change to this function. If you want to learn more about how to use {testthat}, you can refer to the following resources: {testthat} online documentation Chapter 10 Testing - R Packages Part 5: Test and Code Coverage - Building a package that lasts — eRum 2018 workshop 12.1.2 Testing the interactive logic Once you have built a solid test suite for your business logic, another side of your app you might want to check is the interactive logic, i.e. the user interface. There are several tools from the web developer world that can be used to do exactly that: mimicking an interactive session where instead of deliberately clicking on the application interface, you let a program do it for you. 12.1.2.1 puppeteer puppeteer is a NodeJS module that drives a Google Chrome headless session and mimic a session on the app. And good news, there is a Google Chrome extension, called Puppeteer Recorder, that allows you to create, while visiting a webpage, the pupepeteer script to reproduce your visit. Here is, for example, a very small script for testing {hexmake}, generated by this extension. const puppeteer = require(&#39;puppeteer&#39;); (async () =&gt; { const browser = await puppeteer.launch() const page = await browser.newPage() await page.goto(&#39;http://localhost:2811/&#39;) await page.setViewport({ width: 1440, height: 766 }) await page.waitForSelector(&#39;.row &gt; .col &gt; .rounded &gt; details:nth-child(3) &gt; summary&#39;) await page.click(&#39;.row &gt; .col &gt; .rounded &gt; details:nth-child(3) &gt; summary&#39;) await page.waitForSelector(&#39;.innerrounded #main_ui_1-left_ui_1-pkg_name_ui_1-package&#39;) await page.click(&#39;.innerrounded #main_ui_1-left_ui_1-pkg_name_ui_1-package&#39;) await browser.close() })() Be aware though that this extension does not record everything, at least with the version used while writing this book (0.7.1). For example, typing inside a text input is not recorded: that is completely doable inside puppeteer, yet not recorded by this extension34. Once you have this piece of code, put it into a NodeJS script, and replay the session as many time as you need. If ever one of the steps can not be replayed as recorded, the script will fail, notifying you of a regression. Several packages in R mimic what puppeteer does (Google Chrome headless orchestration),with notably {crrri} (???) and {chromote} (???). These packages can be used to launch and manipulate a Google Chrome headless session, meaning that you can programmatically navigate and interact with a webpage from R. And to do the tests in a puppeteer spirit, you can refer to the {crrry} package (???), which contains a series of wrapper functions around {crrri}, specifically designed for Shiny. Here is an example: test &lt;- crrry::CrrryOnPage$new( chrome_bin = pagedown::find_chrome(), chrome_port = httpuv::randomPort(), # Note that you will need httpuv &gt;= 1.5.2 url = &quot;https://connect.thinkr.fr/hexmake/&quot;, inspect = FALSE, headless = TRUE ) test$wait_for_shiny_ready() You can then call one of the test object methods: call_js(), that allows you to run JavaScript code shiny_set_input() changes the value of a Shiny Input wait_for() waits for a JavaScript condition to be TRUE click_on_id clicks on a given id Of course, the interesting part is doing “bulk testing” of your application, for example by setting a series of values to an input: for (i in letters[1:5]){ test$shiny_set_input( &quot;main_ui_1-left_ui_1-pkg_name_ui_1-package&quot;, i ) } And once your test is done, do not forget to close the connection! test$stop() 12.1.2.2 Monkey test If you are working on a user-facing software (i.e a software used by external users), there is one rule to live by: every unexpected behavior that can happen, will happen. In other words, if you develop and think “a user will never do that”, just expect a user to eventually do “that”. But how can we get prepared for the unexpected? How can we test the “crazy behavior” that user will adopt? In web development, there exists a methodology called “Monkey testing”, which consists of launching a series of random event on a webpage: random text in input, scrolling, clicking, zooming… and see if the application crashes or not. This software testing method allows to test the robustness of the application, by seeing how much it can handle unexpected behavior. Several JavaScript libraries exist when it comes to monkey testing, one of the most popular (and easy to use) library is called gremlin.js. This library is particularly interesting when it comes to Shiny as it does not need external installation: you can add the library as a bookmark on your browser, navigate to the application, and launch the testing (click on the “Generate Bookmarklet” link on the top of the README). FIGURE 12.1: Example of using gremlins.js on the “prenoms” Shiny application. And if you want to scale this, you can also combine it with {shinyloadtest} (Dipert, Schloerke, and Borges 2020): launch a session recording, run gremlins one or several time inside the recording, then replay it with multiple sessions. With {crrry}, this gremlins test comes for free: test &lt;- crrry::CrrryOnPage$new( chrome_bin = pagedown::find_chrome(), chrome_port = httpuv::randomPort(), url = &quot;https://connect.thinkr.fr/hexmake/&quot;, inspect = FALSE, headless = TRUE ) test$wait_for_shiny_ready() test$gremlins_horde() test$stop() 12.1.2.3 {shinytest} Finally, if you prefer a Shiny specific package, you can go for {shinytest}. This package, created and maintained by RStudio, allows you to do a series of screenshots of your application, and then replays your app and compare the previously taken screenshots to the current state of your application, allowing you to detect any changes in the interface. If you are building your application with {golem}, you will need to add an app.R file at the root of your package, then run shinytest::recordTest(): golem::add_rstudioconnect_file() shinytest::recordTest() Once this function is run, a new window opens: it contains your app, and a “Screenshot” button on the right. Using this button, you can take various recording of your shiny application at different states. Then, you can do some changes in your app, and run: shinytest::testApp() If the {shinytest} package detects a visual change in the application, you will be immediately alerted, with a report of the difference from the snapshots you took and the current state of the application. 12.1.3 Testing the app load 12.1.3.1 {shinyloadtest} {shinyloadtest} tests how an application behaves when one, two, three, twenty, one hundred users connect to the app, and gives you a visual report about the connection and response time of each session. The idea with {shinyloadtest} is to first record a session where you mimic a user behavior, then shinycannon, a command line tool coming with {shinyloadtest}, replays the recording several times. Once the session has been replayed several times mimicking the session you have recorded, you have access to a report of the behavior of your app. library(shinyloadtest) # Starting your app in another process p &lt;- processx::process$new( &quot;Rscript&quot;, c(&quot;-e&quot;, &quot;options(&#39;shiny.port&#39;= 2811);hexmake::run_app()&quot;) ) # Check that the process is alive Sys.sleep(5) # We wait for the app to be ready p$is_alive() browseURL(&quot;http:://localhost:2811&quot;) Record the tests, potentially in a new dir: fs::dir_create(&quot;shinylogs&quot;) withr::with_dir( &quot;shinylogs&quot;, { shinyloadtest::record_session(&quot;http://localhost:2811&quot;, port = 1234) } ) We now have a series of one or more recording(s) inside the shinylogs/ folder: Then, let’s switch to our command line, and rerun the session with shinycannon. The shinycannon command line tools take several argument: the path the .log file, the URL of the app, --workers specify the number of concurrent connections to run, and the --output-dir argument specifies where the report should be written. Then, go to your terminal and run: shinycannon shinylogs/recording.log http://localhost:2811 --workers 10 --output-dir shinylogs/run1 And now, we have new files inside the folder, corresponding to the session recordings. fs::dir_tree(&quot;shinylogs&quot;, recurse = FALSE) [01;34mshinylogs[0m ├── dockerstats.csv ├── recording.log └── [01;34mrun1[0m Good news: we do not have to manually analyze these files—{shinyloadtest} offers a series of wrapper functions to do that. shinyload_runs &lt;- shinyloadtest::load_runs(&quot;5 workers&quot; = &quot;shinylogs/run1&quot;) Warning: `as.tibble()` is deprecated as of tibble 2.0.0. Please use `as_tibble()` instead. The signature and semantics have changed, see `?as_tibble`. [90mThis warning is displayed once every 8 hours.[39m [90mCall `lifecycle::last_warnings()` to see where this warning was generated.[39m We now have a data.frame with dplyr::glimpse(shinyload_runs) Rows: 550 Columns: 13 $ run [3m[90m&lt;ord&gt;[39m[23m 5 workers, 5 workers, 5 wo… $ session_id [3m[90m&lt;int&gt;[39m[23m 0, 0, 0, 0, 0, 0, 0, 0, 0,… $ user_id [3m[90m&lt;int&gt;[39m[23m 0, 0, 0, 0, 0, 0, 0, 0, 0,… $ iteration [3m[90m&lt;int&gt;[39m[23m 0, 0, 0, 0, 0, 0, 0, 0, 0,… $ input_line_number [3m[90m&lt;int&gt;[39m[23m 4, 5, 6, 8, 9, 11, 14, 15,… $ event [3m[90m&lt;chr&gt;[39m[23m &quot;REQ_HOME&quot;, &quot;WS_OPEN&quot;, &quot;WS… $ start [3m[90m&lt;dbl&gt;[39m[23m 0.000, 0.462, 0.539, 1.025… $ end [3m[90m&lt;dbl&gt;[39m[23m 0.461, 0.539, 0.542, 1.219… $ time [3m[90m&lt;dbl&gt;[39m[23m 0.461, 0.077, 0.003, 0.194… $ concurrency [3m[90m&lt;dbl&gt;[39m[23m 0.0, 1.0, 1.0, 1.0, 1.0, 1… $ maintenance [3m[90m&lt;lgl&gt;[39m[23m TRUE, TRUE, TRUE, TRUE, TR… $ label [3m[90m&lt;ord&gt;[39m[23m &quot;Event 1) Get: Homepage&quot;, … $ json [3m[90m&lt;list&gt;[39m[23m [[&quot;REQ_HOME&quot;, 2020-04-10 … Then, {shinyloadtest} comes with a series of plotting functions that can be used to analyse your recording. Here are some examples: slt_session_duration() plots the session duration, with the various types of event that takes computation time: JS and CSS load, R computation… slt_session_duration(shinyload_runs) slt_waterfall() plots the waterfall graph of session durations, ordered by events. slt_waterfall(shinyload_runs) And if you need to bundle everything into an HTML reports, shinyloadtest_report() is what you are looking for. shinyloadtest_report(shinyload_runs) So, to sum up with a step by step guide: If the shiny app is only available on your machine, launch a process with {processx}, or in another R session, that launches the application. You can either set the port with options('shiny.port'= 2811), or let shiny decide for you. Be sure that the process is running. If the app is online, use the online url (and make sure you have access to the app). Run shinyloadtest::record_session(url). You should probably set a different port for {shinyloadtest}, so that it does not try to connect on port 80. Play around with your app, record a scenario of usage Close the tab where the app is running. Return to your terminal, and run the shinycannon command line tool Wait for the process to be terminated Go back to R, and then you can analyse the data from the recordings, either manually or by generating the html report 12.1.3.2 {shinyloadtest}, {crrry}, and {dockerstats} Another thing you might want to monitor is the memory/CPU usage of your application, which {shinyloadtest} does not natively provide: the package records the load from the browser point of view, not from the server one. That’s where {dockerstats} (Fay 2020) can come into play: this package is a wrapper around the command line docker stats, and returns an R data.frame with the stats. You can get the {dockerstats} package from GitHub with: remotes::install_github(&quot;ColinFay/dockerstats&quot;) library(dockerstats) With these stats, we can monitor the load on the app when it is run in a docker container. system(&quot;docker run --name hexmake --rm -p 2811:80 colinfay/hexmake&quot;, wait = FALSE) Let’s say now we want the stats for the hexmake container: dockerstats::dockerstats(&quot;hexmake&quot;) Of course, right now nobody is using the app, so the usage can be pretty small. But let’s push it a little bit an mimic a lot of connections. To do that, we can replay our shinycannon call, with at the same time using the dockerstats_recurse() function, that will recursively call dockerstats() on a regular interval. shinycannon shinylogs/recording.log http://localhost:2811 --workers 10 --output-dir shinylogs/run2 Let’s launch at the same time a dockerstats_recurse() For example, here, we will print, on each loop, the MemUsage of the container, then saving the data inside a dockerstats.csv file. dockerstats_recurse( &quot;hexmake&quot;, callback = function(res){ print( paste(&quot;Mem usage: &quot;, res$MemUsage) ) write.table( res, &quot;dockerstats.csv&quot;, append = TRUE, col.names = FALSE, row.names = FALSE, sep = &quot;,&quot; ) } ) Here is what both these processes look side to side: FIGURE 12.2: {dockerstats} and shinycannon running side-by-side at the same time. As you can see, as the number of connections grow, the memory usage grows. And we now have a csv with the evolution of the docker stats records over time! docker_stats &lt;- read_appended_csv( &quot;shinylogs/dockerstats.csv&quot; ) dplyr::glimpse(docker_stats) Rows: 136 Columns: 14 $ Container [3m[90m&lt;chr&gt;[39m[23m &quot;hexmake&quot;, &quot;hexmake&quot;, &quot;hexmake&quot;,… $ Name [3m[90m&lt;chr&gt;[39m[23m &quot;hexmake&quot;, &quot;hexmake&quot;, &quot;hexmake&quot;,… $ ID [3m[90m&lt;chr&gt;[39m[23m &quot;b5d337941e310cbf4708b95a9cc7556… $ CPUPerc [3m[90m&lt;dbl&gt;[39m[23m 0.09, 15.07, 38.58, 54.94, 20.28… $ MemUsage [3m[90m&lt;chr&gt;[39m[23m &quot;110.9MiB&quot;, &quot;117.2MiB&quot;, &quot;168.3Mi… $ MemLimit [3m[90m&lt;chr&gt;[39m[23m &quot;1.943GiB&quot;, &quot;1.943GiB&quot;, &quot;1.943Gi… $ MemPerc [3m[90m&lt;dbl&gt;[39m[23m 5.57, 5.89, 8.46, 8.73, 8.74, 8.… $ NetI [3m[90m&lt;chr&gt;[39m[23m &quot;586B&quot;, &quot;8.37kB&quot;, &quot;31.6kB&quot;, &quot;62.… $ NetO [3m[90m&lt;chr&gt;[39m[23m &quot;0B&quot;, &quot;433kB&quot;, &quot;1.18MB&quot;, &quot;2.48MB… $ BlockI [3m[90m&lt;chr&gt;[39m[23m &quot;0B&quot;, &quot;0B&quot;, &quot;0B&quot;, &quot;0B&quot;, &quot;0B&quot;, &quot;0… $ BlockO [3m[90m&lt;chr&gt;[39m[23m &quot;0B&quot;, &quot;0B&quot;, &quot;0B&quot;, &quot;0B&quot;, &quot;0B&quot;, &quot;0… $ PIDs [3m[90m&lt;int&gt;[39m[23m 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,… $ record_time [3m[90m&lt;dttm&gt;[39m[23m 2020-04-10 10:39:20, 2020-04-10… $ extra [3m[90m&lt;lgl&gt;[39m[23m NA, NA, NA, NA, NA, NA, NA, NA, … If you need a deeper look into the connection between application actions and the docker stats, you can also combine {dockerstats} with {crrry}, the idea being that you can record the CPU usage at the exact moment the application performs a specific computation. Let’s record the computation of the hexmake container containing the same app as before. system( &quot;docker run -p 2708:80 --rm --name hexmake2 -d colinfay/hexmake&quot;, wait = FALSE ) Sys.sleep(5) # Let&#39;s leave some time for the container to be ready test &lt;- crrry::CrrryOnPage$new( chrome_bin = pagedown::find_chrome(), chrome_port = httpuv::randomPort(), url =&quot;http://localhost:2708&quot;, inspect = FALSE, headless = TRUE ) test$wait_for_shiny_ready() results &lt;- dockerstats::dockerstats(&quot;hexmake2&quot;, extra = &quot;launch&quot;) for (i in letters[1:10]){ test$shiny_set_input( &quot;main_ui_1-left_ui_1-pkg_name_ui_1-package&quot;, i ) results &lt;- rbind( results, dockerstats::dockerstats(&quot;hexmake2&quot;, extra = i) ) } system(&quot;docker kill hexmake2&quot;) And draw a small graph of this evolution: library(dplyr, warn.conflicts = FALSE) results &lt;- results %&gt;% mutate( MemUsage = to_mib(MemUsage), record_time = as.POSIXct(record_time) ) library(ggplot2) ggplot(data = results, aes(x = record_time)) + geom_line(aes(y = MemUsage)) + scale_y_continuous(labels = scales::label_bytes(units = &quot;MiB&quot;)) + geom_vline(aes(xintercept = record_time)) + geom_label(aes(y = max(MemUsage), label = extra)) + labs( title = &quot;MemUsage of setting ten inputs for package name in {hexmake}&quot; ) 12.2 A reproducible environment One of the challenges of building an app that needs to be sent to production is that you will need to work in a reproducible environment. What does this mean? That you are building an application that is to be deployed in another computer (be it a server or another R user’s computer). Once your app is built, there are few chances that you will launch it on your own computer and that external user will connect to your computer. What will happen is that you will either give your users a package (which will be the simplest way to share it: bundle the packaged app to a tar.gz, then let people install it either manually or from a package repository), or a URL where they can connect and use your app. If you follow the {golem} workflow and all the good practices for a solid package, the application you have built should be deployable on another computer that has R. In that second case, you will have to think about how you can create your app in a reproducible environment: in other words, be sure that the app is deployed under the same configuration as your local application—R version, package versions, system requirements, environment variables… To help you achieve that,, we will introduce two tools in the next section: {renv} (???), and Docker. 12.2.1 {renv} 12.2.1.1 About {renv} How do we make sure the package versions we have installed on our machine stays the same in the production environment? And also, how can we be sure that, working as a team, we will be able to work together using the same package versions? From one version to another, functions and behaviors change. Most of the time, new version means new functions, and new features. But from time to time, a new version means breaking changes. Catching that these new versions cause breaking changes can be hard: either because we do not realize that the version is different, or because debugging the error is difficult, especially in Shiny where the trace-back is very deep. And of course, the moment when we discover the error might not be the perfect time for us, as we might not have enough free time on our calendar to debug the application which has stopped running… For example, here is an error from a real life error when pushing an app on a shiny-server: root@westeros-vm:/var/log/shiny-server# cat thewall(...).log *** caught segfault *** [...] address 0x5100004d, cause &#39;memory not mapped&#39; Traceback: 1: rcpp_sf_to_geojson(sf, digits, factors_as_string) 2: sf_geojson.sf(data) 3: geojsonsf::sf_geojson(data) 4: addGlifyPolygons(., data = pol_V1, color = les_couleurs, popup = &quot;val&quot;, opacity = 1) 5: function_list[[i]](value) 6: freduce(value, `_function_list`) 7: `_fseq`(`_lhs`) 8: eval(quote(`_fseq`(`_lhs`)), env, env) [...] 105: captureStackTraces({ while (!.globals$stopped) { ..stacktracefloor..(serviceApp()) Sys.sleep(0.001) }}) 106: ..stacktraceoff..(captureStackTraces({ while (!.globals$stopped) { ..stacktracefloor..(serviceApp()) Sys.sleep(0.001) }})) 107: runApp(Sys.getenv(&quot;SHINY_APP&quot;), port = port, launch.browser = FALSE) An irrecoverable exception occurred. R is aborting now ... Pretty hard to debug, isn’t it? So, what has actually happened? On that specific case, it turned out that the package version from {geojsonsf} was 1.2.1 on our development machine, and the one on the Shiny server was updated to 1.3.0. And there was a breaking change in the package. These kind of bugs are even harder to detect as {geojsonsf} was not a direct dependency of our app, but a dependency of one of our dependency, making it just a little bit more complex to identify. FIGURE 12.3: Breaking changes in {geojsonsf}, a dependency of a dependency of our Shiny application. The same thing could have happen if working as a team: one of the computer has an old version, when another one has updated to a more recent one. How do we prevent that? This is where the {renv} package comes into play: this package allows to have a project-based library, instead of a global one. In other words, instead of having a library that is global to your machine, {renv} allows to specify packages with fixed versions for a project. That means that you can have {geojsonsf} version 1.2.1 in one of your project, and the 1.3.0 in another, with the two not conflicting with each other. 12.2.1.2 Using {renv} Underlying the philosophy of renv is that any of your existing workflows should just work as they did before .right{ text-align: right;} Introduction to renv The first thing to do with {renv} is initiating it with the init() function. library(renv) init() This function does several things: Create/modify the .Rprofile file at the root of your project. Here is an example of what this files may look like inside an empty project: [1] &quot;source(\\&quot;renv/activate.R\\&quot;)&quot; In this example, there is just one call to a script, one located at renv/activate.R. It creates a renv.lock file, which will list all the package dependencies [1] &quot;{&quot; [2] &quot; \\&quot;R\\&quot;: {&quot; [3] &quot; \\&quot;Version\\&quot;: \\&quot;3.6.1\\&quot;,&quot; [4] &quot; \\&quot;Repositories\\&quot;: [&quot; [5] &quot; {&quot; [6] &quot; \\&quot;Name\\&quot;: \\&quot;CRAN\\&quot;,&quot; [7] &quot; \\&quot;URL\\&quot;: \\&quot;https://cran.rstudio.com\\&quot;&quot; [8] &quot; }&quot; [9] &quot; ]&quot; [10] &quot; },&quot; [11] &quot; \\&quot;Packages\\&quot;: {&quot; [12] &quot; \\&quot;attempt\\&quot;: {&quot; [13] &quot; \\&quot;Package\\&quot;: \\&quot;attempt\\&quot;,&quot; [14] &quot; \\&quot;Version\\&quot;: \\&quot;0.3.0\\&quot;,&quot; [15] &quot; \\&quot;Source\\&quot;: \\&quot;Repository\\&quot;,&quot; [16] &quot; \\&quot;Repository\\&quot;: \\&quot;CRAN\\&quot;,&quot; [17] &quot; \\&quot;Hash\\&quot;: \\&quot;9aaae25e273927dba4e279caac478baa\\&quot;&quot; [18] &quot; },&quot; [19] &quot; \\&quot;renv\\&quot;: {&quot; [20] &quot; \\&quot;Package\\&quot;: \\&quot;renv\\&quot;,&quot; [21] &quot; \\&quot;Version\\&quot;: \\&quot;0.9.3\\&quot;,&quot; [22] &quot; \\&quot;Source\\&quot;: \\&quot;Repository\\&quot;,&quot; [23] &quot; \\&quot;Repository\\&quot;: \\&quot;CRAN\\&quot;,&quot; [24] &quot; \\&quot;Hash\\&quot;: \\&quot;c1a367437d8a8a44bec4b9d4974cb20c\\&quot;&quot; [25] &quot; },&quot; [26] &quot; \\&quot;rlang\\&quot;: {&quot; [27] &quot; \\&quot;Package\\&quot;: \\&quot;rlang\\&quot;,&quot; [28] &quot; \\&quot;Version\\&quot;: \\&quot;0.4.5\\&quot;,&quot; [29] &quot; \\&quot;Source\\&quot;: \\&quot;Repository\\&quot;,&quot; [30] &quot; \\&quot;Repository\\&quot;: \\&quot;CRAN\\&quot;,&quot; [31] &quot; \\&quot;Hash\\&quot;: \\&quot;1cc1b38e4db40ea6eb19ab8080bbed3b\\&quot;&quot; [32] &quot; }&quot; [33] &quot; }&quot; [34] &quot;}&quot; As we have initiated an empty project, we do not have any dependencies here. If you run this command in a project that already has scripts and dependencies, {renv} will try to locate them all, and add them to this file. Note that these packages can come from CRAN, Bioconductor, GitHub, Gitlab, Bitbucket, and even local repositories. The renv/ folder contains a series of files that that store your settings and the necessary packages, using a structure that mimics a local repository. fs::dir_tree(&quot;data-raw/renv/&quot;) [01;34mdata-raw/renv/[0m ├── [32mactivate.R[0m └── settings.dcf We will not go into details on this folder, as it is a rather complex structure and chances are that you will never have to update it by hand With {renv}, you can choose to link this “local repository” to a local cache, i.e a folder which is common to all your projects and stores packages and the different versions you already installed (this is the default behavior) or to store the complete packages inside the project, making it portable. When you need a new package, you will have to install it in your local library. The fastest way to install new packages in your {renv} powered project is by using the install.packages function, which is shimmed by {renv}. This shim will search into the local cache to see if the package has already been cached, and if it is not, it will install and link it. install.packages(&quot;attempt&quot;) We will now add a little call to this library now: write(&quot;library(attempt)&quot;, &quot;script.R&quot;) Once you want to update your {renv} Lockfile, call snapshot() renv::snapshot(confirm = FALSE) Note that if you are building an application as a package, use renv::snapshot(type = \"explicit\") (need version &gt; 0.9.3-99): this will only capture the dependencies listed in the DESCRIPTION file. [1] &quot;{&quot; [2] &quot; \\&quot;R\\&quot;: {&quot; [3] &quot; \\&quot;Version\\&quot;: \\&quot;3.6.1\\&quot;,&quot; [4] &quot; \\&quot;Repositories\\&quot;: [&quot; [5] &quot; {&quot; [6] &quot; \\&quot;Name\\&quot;: \\&quot;CRAN\\&quot;,&quot; [7] &quot; \\&quot;URL\\&quot;: \\&quot;https://cran.rstudio.com\\&quot;&quot; [8] &quot; }&quot; [9] &quot; ]&quot; [10] &quot; },&quot; [11] &quot; \\&quot;Packages\\&quot;: {&quot; [12] &quot; \\&quot;attempt\\&quot;: {&quot; [13] &quot; \\&quot;Package\\&quot;: \\&quot;attempt\\&quot;,&quot; [14] &quot; \\&quot;Version\\&quot;: \\&quot;0.3.0\\&quot;,&quot; [15] &quot; \\&quot;Source\\&quot;: \\&quot;Repository\\&quot;,&quot; [16] &quot; \\&quot;Repository\\&quot;: \\&quot;CRAN\\&quot;,&quot; [17] &quot; \\&quot;Hash\\&quot;: \\&quot;9aaae25e273927dba4e279caac478baa\\&quot;&quot; [18] &quot; },&quot; [19] &quot; \\&quot;renv\\&quot;: {&quot; [20] &quot; \\&quot;Package\\&quot;: \\&quot;renv\\&quot;,&quot; [21] &quot; \\&quot;Version\\&quot;: \\&quot;0.9.3\\&quot;,&quot; [22] &quot; \\&quot;Source\\&quot;: \\&quot;Repository\\&quot;,&quot; [23] &quot; \\&quot;Repository\\&quot;: \\&quot;CRAN\\&quot;,&quot; [24] &quot; \\&quot;Hash\\&quot;: \\&quot;c1a367437d8a8a44bec4b9d4974cb20c\\&quot;&quot; [25] &quot; },&quot; [26] &quot; \\&quot;rlang\\&quot;: {&quot; [27] &quot; \\&quot;Package\\&quot;: \\&quot;rlang\\&quot;,&quot; [28] &quot; \\&quot;Version\\&quot;: \\&quot;0.4.5\\&quot;,&quot; [29] &quot; \\&quot;Source\\&quot;: \\&quot;Repository\\&quot;,&quot; [30] &quot; \\&quot;Repository\\&quot;: \\&quot;CRAN\\&quot;,&quot; [31] &quot; \\&quot;Hash\\&quot;: \\&quot;1cc1b38e4db40ea6eb19ab8080bbed3b\\&quot;&quot; [32] &quot; }&quot; [33] &quot; }&quot; [34] &quot;}&quot; And now that you have a reproducible {renv} library, what is next? Of course, if you are either working as a team or deploying to a server, you will have to restore the state of your project, which is now living somewhere else, inside your current project / deployment. And to do that, the function to call is env::restore(), which will update your local project with the dependencies listed inside your Lockfile. So, to sum up, here are the step to follow: Initiate the project with renv::init() Install / remove packages Take a snapshot() of the state of your project renv::restore() the state of your project using renv.lock Share .Rprofile, renv.lock, renv/activate.R and renv/settings.dcf files for reproducibility Of course, renv::restore() comes with another superpower: time traveling! If you decide to update a package in your project, and realize that this package makes the application crash (e.g. an update to {geojsonsf}), you can go back in time to a previous version of your library by calling the restore() function. There are more things you can do with {renv}. If you want to know more, feel free to refer to the official website. 12.2.2 Docker 12.2.2.1 R, Docker, Shiny Docker is a program that allows to download, install, create, launch and stop multiple operating systems, called containers, on a machine, which will be called the host. This host can be your local computer, or the server where you deploy your application(s). Docker was designed for enclosing software environments inside an image that can later be launched. The general idea is that with Docker, you are defining in a Dockerfile all the “rules” that are used to create a given environment, and then you can use this file (and the linked files, for example the R package containing your app) to deploy your application on any given server that can run Docker. That way, if the Dockerfile can compile on your machine and if you can run it, it should work everywhere (of course, it is a little bit more complex than that, but you get the idea). So, why Docker in the context of Shiny apps? Because Docker allows you to abstract away the complexity of managing multiple versions of R and multiple version of the same package, or even different versions of the same system requirement. For example, with our {geojsonsf} example from before, we could safely have had a docker container with the 1.2.1 version, working locally, and changing versions on the server would not have broken our code. By using Docker for your deployment, you can build and deploy an application with the very same version of packages and R as the one from your computer. And of course, you can change them without breaking the rest of the machine: everything that happens in a container stays in a container. That way, if your are building your application with an older version of {shiny}, you are sure that sending it to production will not break everything: the version inside the Docker is the same as the one from your machine. And later, if you update {shiny} and start a new project, you can deploy your app with another version of the package. Same goes for your version of R. 12.2.2.2 Building a Dockerfile for your app Good news! If you are building your app with {golem}, the creation of the Dockerfile is just one function away! If you have a look at the 03_deploy.R file in the dev folder, you will find a series of functions that can create the Dockerfile for your project: either as a generic docker image, or for ShinyProxy or Heroku. For example, to create a Dockerfile for a {golem} project, you can run, from the root of your package: golem::add_dockerfile() Let’s take some time to understand file, or how we could be building it from scratch. FROM FROM rocker/r-ver:3.6.1 This line defines what version of R to use for deploying your application. This FROM line is the one that sets an image to start from: you rarely (if ever) build a Docker image from nothing, but instead you use an existing image on top of which you build your own image. Here, we choose one of the r-ver docker images, based on the output of: R.Version()$version.string [1] &quot;R version 3.6.3 (2020-02-29)&quot; RUN The RUN calls in the file refers to bash calls that are used to build the image. For example, the second line of the Dockerfile installs all the system requirements needed by our application. RUN apt-get update &amp;&amp; apt-get install -y git-core libcurl4-openssl-dev libssh2-1-dev libssl-dev libxml2-dev make zlib1g-dev &amp;&amp; rm -rf /var/lib/apt/lists/* In the subsequent RUN calls, {golem} chooses to call remotes::install_version() to be sure we install the version of the package that matches the one from your computer. RUN Rscript -e &#39;remotes::install_version(&quot;config&quot;,upgrade=&quot;never&quot;, version = &quot;0.3&quot;)&#39; As you can see, it matches the local version: packageVersion(&quot;config&quot;) [1] &#39;0.3&#39; ADD This Docker entry takes a folder or a file, and copies it inside the image. With {golem}, we are adding the current project, containing the app, to a folder called /build_zone. ADD . /build_zone EXPOSE This command defines which port of the container will be available from the outside of the container. EXPOSE 80 CMD This final command is the one that is launched when you run a container. With a {shiny} app, this command is the one that launches the application. EXPOSE 80 12.2.2.3 {dockerfiler} If you want to do everything from the R command line, the {dockerfiler} package (Fay 2019) is here for you! This package allows you to generate a Dockerfile straight from R: library(dockerfiler) my_dock &lt;- Dockerfile$new() my_dock$RUN(&quot;apt-get update &amp;&amp; apt-get install -y git-core libcurl4-openssl-dev&quot;) my_dock$ADD(&quot;.&quot;, &quot;/&quot;) my_dock$RUN(&quot;mkdir /build_zone&quot;) my_dock$ADD(&quot;.&quot;, &quot;/build_zone&quot;) my_dock$WORKDIR(&quot;/build_zone&quot;) my_dock$RUN(r(remotes::install_local(upgrade=&quot;never&quot;))) my_dock$EXPOSE(80) my_dock FROM rocker/r-base RUN apt-get update &amp;&amp; apt-get install -y git-core libcurl4-openssl-dev ADD . / RUN mkdir /build_zone ADD . /build_zone WORKDIR /build_zone RUN R -e &#39;remotes::install_local(upgrade = &quot;never&quot;)&#39; EXPOSE 80 12.2.2.4 Docker &amp; {renv} If you use {renv} to build your Shiny application, it can also be used inside your Docker container. To make those two tools work together, you will have to copy the files produces by {renv} inside the container: .Rprofile, renv.lock, renv/activate.R and renv/settings.dcf files. Then run renv::restore() inside your application, instead of using the calls to remotes::install_version() as they are currently implemented when doing it with {golem}. At the time of writing these lines, there is no native support of {renv} (with or without Docker) in {golem}, but that is something we can expect to happen in future versions of this package. 12.2.2.5 Develop inside a Docker container Developers have their own R versions and operating systems. If you want to be able to correctly deploy your application, you will use a Docker container. Then, why not already developing inside a Docker container having the exact same architecture than the one you will deploy ? This is possible to use “rocker” containers to build your application inside the container, using the embedded Rstudio Server or directly an exported R console. You can even combine developing in a Docker container with the use of {renv}. 12.2.2.6 Read more about Docker An Introduction to Docker for R Users An Introduction to Rocker: Docker Containers for R The Rockerverse: Packages and Applications for Containerization with R References "],
["secure.html", "Chapter 13 Version Control 13.1 Using Version Control with git 13.2 Git integration 13.3 Automated testing", " Chapter 13 Version Control 13.1 Using Version Control with git “Friends do not let friends work on a coding project without version control.” You might have heard this before, without really considering what this means. Or maybe you are convinced about this saying, but have not had the opportunity to use git, GitHub or Gitlab for versioning your applications. If so, now is the time for a workflow change! 13.1.1 Why Version Control? Have you ever experienced a piece of code disappearing? Or the unsolvable problem of integrating changes when several people have been working on the same piece of code? Or the inability to find back something you have written a while back? If so, you might have been missing Version Control (also shortened as VC). In this chapter, we’ll be focusing on git, but you should be aware that other VC system exist, but they are less popular than git, hence we will not cover them here. Git was designed to handle collaboration on code projects35 where potentially a lot of people have to interact and make changes to the codebase. Git might feel a little bit daunting at first, and even seasoned developers still misuse it, or do not understand it completely, but getting at ease with the basics will significantly improve the way you build software, so do not give up: the benefits from learning it really outweigh the (apparent) complexity. There are many advantages to VC, including: You can get back in time. With a VC system like git, every change is recorded (well, every committed change), meaning that you can potentially get back in time to a previous version of a project, and see the complete history of a file. This feature is very important: if you accidentally made changes that break your application, or if you deleted a feature you thought you would never need, you can go back to where you were a few hours, a few days, a few months back. Several people can work on the same file. Git relies on a system of branches. Within this branch pattern, there is one main branch, called “master”, which contains the stable, main version of the code-base. By “forking” this branch (or any other branch), developers will have a copy of the base branch, where they can safely work on changing (and breaking) things, without impacting the origin branch. This allows to try things in a safe environment, without touching what works. You can safely track changes. Every time a developer records something to git, changes are listed. In other words, you can see what changes are made to a specific file in your codebase. It centralizes the codebase. You can use git locally, but its strength also relies on the ability to synchronize your local project with a distant server. This also means that several people can synchronize with this server and collaborate on a project. That way, changes on a branch on a server can be downloaded (it is called pull in git terminology) by all the members of the team, and synchronized locally, i.e if someone makes changes to a branch and send them to the main server, all the other developers can retrieve these change on their machine. 13.1.2 Git basics: add - commit - push - pull These are the four main actions you will be performing in git: if you just need to learn the minimum to get started, they are the four essential ones. 13.1.2.1 add When using add, you are choosing which elements of your project you want to track, be it new files or modifications of an already versionned file. This action does not save the file in the git repository, but flags the changes to be added to the next commit. 13.1.2.2 commit A commit is a photography of a codebase at a given moment in time. Each commit is associated with two things: a sha1, which is a unique reference in the history of the project and that allows you to identify this precise state when you need to get back in time, and a message, which is a piece of text that describes the commit36. Note that messages are mandatory, you can not commit without them, and that the sha1 are automatically generated by git. Do not overlook these messages: they might seem like a constraint at first but they are a life saver when you need to understand the history of a project. There is no strict rule about what and when to commit. Keep in mind that commits are what allow you to get back in time, so a commit is a complete state of your codebase to which it would make sense to get back to. A good practice is to state in the commit message which choices you made and why (but not how you implemented these changes), so that other developers (and you in the future) will be able to understand changes. 13.1.2.3 push Once you have get a set of commits ready, you are ready to push it to the server. In other word, you will permanently record these commits (so the series of changes) to the server. Making a push implies three things: Other people in the team will be able to retrieve the changes you have made These changes will be recorded permanently in the project history Do not modify commits that were sent to the server37 13.1.2.4 pull Once changes have be recorded in the main server, everybody synchronized with the project can pull the commits to their local project. 13.1.3 About branches Branches are git way to organize work and ideas, notably when several people are collaborating on the same project (which might be the case when building large web applications with R). How does it work? When your start a project, you are in the main branch, which is called the “master”. In a perfect world, you never work directly on this branch: it should always contain a working, deployable version of the application. One Other branches are to be thought as work areas, where developers fix bugs or add features. The modifications made in these development branches will then be transferred (directly or indirectly) to the master branch. FIGURE 13.1: Branches in git. In practice, you might want to use a workflow where each branch is designed to fix a small issue or implement a feature, so that it is easier to separate each small part of the work. Even when working alone. 13.1.4 Issues If you are working with a remote tool with a graphical interface like Gitlab, GitHub or Bitbucket, there is a good change you will be using issues. Issues are “notes” or “tickets” that can be used to track a bug or to suggest a feature. This tool is crucial when it comes to project management: they are the perfect spot for organizing and discussing ideas, but also to have an overview of what has been done, what is currently done and what is left to be done. Issue can also be used as a discussion medium with beta testers, clients or sponsors. One other valuable feature of issues is that they can be referenced inside commits using a hashtag and its number: #123. . In other words, when you send code to the centralized server, you can link this code to one or more issues and corresponding commits appear in the issue discussions. 13.2 Git integration 13.2.1 With RStudio Git is very well integrated to the Rstudio IDE, and using git can be as simple as clicking on a button from time to time. If you are using RStudio, you will find a pull/push button, a stage &amp; commit interface, a tool for visualizing differences in files. Everything you need to get started is there. 13.2.2 As part of a larger world Git is not reserved for team work: even if you are working alone on a project, using git is definitely worth the effort. Using git, and particularly issues, helps you organize your train of thoughts, especially upfront when you need to plan what you will be doing. And of course, remember that git is not reserved to Shiny Applications: it can be used for any other R related projects, and at the end of the day for any code related projects, making it a valuable skill to have in your toolbox, whatever language you will be working with in 10 years! 13.2.3 About git-flow There are a lot of different ways and methodologies to organize your Git workflow. One of the most popular one is called git flow, and we will give you here a quick introduction to how you can manage your work using this approach. Please note that this is a quick introduction, not a complete guide: we will link to some further reading just at the end of this section. So, here are the key concept of git flow: The master branch only contains stable code: most of the time is matches a tagged, fixed version (v0.0.1, 0.1.0, v1.0.0, etc). A very small subset of developers involved in the project have writing access to the master branch, and no developer should ever push code straight to this branch: new code to master only comes either from the dev branch, or from a hotfix branch. For an app in production, the last commit of this branch should be the version that is currently in production. The dev branch, on the other hand, is the “Work in progress” branch: the one that contains the latest changes before they are merged into master. This is the common working branch for every developers. Most of the time, developers do not push code into these branch either: they make merge/pull request (MR) to dev from one of their feature branch. A feature branch is one branch, forked from dev, that implements one of the feature of the application. To keep a clean track of what each branch is doing, a good practice is to use issue-XXX, where XXX is the corresponding issue you plan to solve in this branch. A hot fix branch is a branch to correct a critical issue in master. If forks from master, and is merged straight into master using a MR. Here is a summary of this process: FIGURE 13.2: Presentation of a git flow (Vincent Driessen, http://nvie.com). From a software engineer point of view, here is how daily work goes: Identify an issue to work on Fork dev into issue-XXX Develop feature inside the branch Regularly, run git stash, git rebase dev, and git stash apply to include the latest changes from dev to stay synchronized with dev38 Make a pull request to dev so that the feature is included Once the PR is accepted by the project manager, notify the rest of the team that there have been changes to dev, so they can rebase it to the branch they are working on Start working on a new feature Of course, there are way more subtleties to this flow of work, but this gives you a good starting point. Generally speaking, a good communication between developers is essential for a successful collaborative development project. 13.2.4 Further readings on git Git can be used in different ways and different approaches exist. The comprehensiveness of the different possible approaches is beyond the scope of this book, and other resources exist as well. You can find more under these links: https://happygitwithr.com/ https://git-scm.com/book https://www.git-tower.com/blog/git-cheat-sheet/ 13.3 Automated testing We have seen in the chapter 12 how to build a testing infrastructure for your app, notably using the {testthat} package. What we have described is a way to build it locally, before running your test on your own machine. But there is a big flaw to this approach: you have to remember to run the tests, be it regularly or before making a pull request/pushing to the server. To do this kind of job, you will be looking for a tool to do automated testing at the repository level: in other words, a software that can test your application whenever a piece of code is pushed/moved on the repository. To do this, various tools are available, each with there own features. Here is a non exhaustive list of the one you can choose: travis-ci is a software that can be synced with your git repositories (GitHub or Bitbucket), and whenever something happens on the repo, the events described in the travis configuration file (.travis.yml) are executed. If they exit with a code 0, the test passes. If they do not, the integrated tests have failed. This Travis CI integration can be used internally and externally: internally, in the sense that before merging any pull request, the project manager have access to a series of tests that are automatically launched. Externally, as a “health check” before installing a software: if you visit a GitHub repository that has Travis badges included, you can check if the current state of the package/software is stable, i.e. if it passes the automated tests. Travis CI can do a lot more than just testing your app: it can be used to build documentation, deploy to production, or to run any other scripts you want to be run before/after the tests have passed. And the nice thing is that you can test for various versions of R, so that you are sure that you are supporting current, future and previous versions of R. All of this is defined in the .travis.yml file, which is to be put at the root of your source directory, a file that is automatically generated when calling usethis::use_travis(). Here is an example of one of this file, for the {golem} package: # R for travis: see documentation at https://docs.travis-ci.com/user/languages/r language: R sudo: false cache: packages r_github_packages: - ThinkR-open/golem # pre-install to avoid vignette package errors # build matrix; turn on vdiffr only on r release matrix: include: - r: devel - r: release env: VDIFFR_RUN_TESTS=true before_cache: - Rscript -e &#39;remotes::install_cran(&quot;pkgdown&quot;)&#39; - Rscript -e &#39;remotes::install_github(&quot;ThinkR-open/thinkrtemplate&quot;)&#39; deploy: provider: pages skip-cleanup: true github-token: $GITHUB_PAT keep-history: true local-dir: docs on: branch: master skip_cleanup: true - r: oldrel - r: 3.3 - r: 3.4 before_install: - Rscript -e &#39;update.packages(ask = FALSE)&#39; after_success: - Rscript -e &#39;covr::codecov()&#39; - Rscript -e &#39;pkgdown::build_site()&#39; Note that Travis CI can run tests on GNU/Linux or MacOS operating systems. Appveyor has the same functionnalities as Travis CI. This service can integrate with GitHub, GitHub Enterprise, Bitbucket, GitLab, Azure Repos, Kiln, Gitea. It supports Windows, Linux and macOS. GitHub actions serve a related purpose: defining actions to be performed as responses to events on the GitHub repository. Testing, building documentation, push to another repository, deploy on the server… all these actions can be automatically performed. As with Travis CI, these actions are defined in a yaml file. Examples for these configuration can be find at r-lib/actions. If you are working with GitLab, you can use the integrated GitLab CI service: it serves the same purpose, with the little difference that it is completely docker-based: you define a yaml with a series of stages that are performed (concurrently or sequentially), and they are all launched inside a docker container. To help you with this, the colinfay/r-ci-tidyverse docker image comes with pre-installed packages for testing: {remotes}, {testthat}, {vdiffr}, {config}… and is available for several R versions. This docker image can be used as the source image for your GitLab CI yaml file. Here is an example of one of these files image: colinfay/r-ci-tidyverse:3.6.0 cache: paths: - ci/ stages: - test - document building: stage: test script: - R -e &quot;remotes::install_deps(dependencies = TRUE)&quot; - R -e &#39;devtools::check()&#39; documenting: stage: document allow_failure: true when: on_success only: - master script: - Rscript -e &#39;install.packages(&quot;DT&quot;)&#39; - Rscript -e &#39;covr::gitlab(quiet = FALSE)&#39; artifacts: paths: - public Automated testing, continuous integration and continuous deployment is a vast topic that can not be covered in a few pages inside this book, but spending some time learning about this methodologies is definitely worth the time spent: the more you can automate these processes, and the more you test, the more your application will be resilient, easy to maintain and easy to enhance: the more you check, the quicker you will discover bugs. And the quicker you detect bugs, the easier it is to correct them! If you want to modify some code or have to get back in time, the best way to do it is to create a new commit with these changes or use adequate git commands.↩︎ "],
["deploy-golem.html", "Chapter 14 Deploy your application 14.1 Before deployment Check-list 14.2 Sharing your app as a package 14.3 Deploying Apps with {golem} 14.4 RStudio Environments 14.5 Docker", " Chapter 14 Deploy your application Your deploys should be as boring, straightforward, and stress-free as possible. .right{ text-align: right;} How to Deploy Software - Zach Holman Once your app is built, you are ready to deploy it! In other words, your software is now ready to be used by other users, be it from There are two main ways to share your application and make it available to others: by creating a package and making it installable, or by sending it to a remote server. We’ll see in this part how you can do that using {golem}. 14.1 Before deployment Check-list Here is a quick checklist of things to think about once your application is read, and before sending it to production devtools::check(), run from the command line, returns 0 errors, 0 warnings, 0 notes Everything is fully documented Test coverage is good The contact and debugging processes are clear and everybody involves in the project management knows what to expect (If relevant) The server it is deployed on has all the necessary software installed (Docker, Connect, Shiny Server…) to make the application run The server has all the system requirements needed (i.e the system libraries), and if not, they are installed with my application (if it’s dockerized) The application, if deployed on a server, will be deployed on a port which will be accessible by the users (If relevant) The environment variables from the production server are managed inside the application (If relevant) The app is launched on the correct port, or at least this port can be configured via environment variable (If relevant) The server where the app is deployed have access to the data sources (database, API…) If the app record data, there are backups for these data. 14.2 Sharing your app as a package 14.2.1 Install on your machine A Shiny application built with golem is by definition created as an R package. So before sending it to a remote server or sharing it to the world, the first step is testing if the package can be installed on your own computer. To do that, when you are in the project corresponding to the golem you built, you can call remotes::install_local() to install the application on your computer. If you are using the RStudio IDE, you can also click on the Build tab, then click on the Install and Restart button. This should restart your R session, and call library(yourpackagename). Then, try the run_app() function to check that the app can be launched. 14.2.2 Share as built package Building an app as a package also means that this app can be bundled into an archive, and then shared, either as is or using a package repository like the CRAN. To do that, you first need an bundled version of your app, which can be created using the pkgbuild::build() function in the same working directory as your application. Calling this function wîll create a .tar.gz file that is called mygolem_0.0.1.tar.gz (of course with the name of your package). Once you have this tar.gz, you can send it to your favorite package repository. You can also share the file as is with others. If you do so, they will have to install the app with remotes::install_local(\"path/to/tar.gz\"), that will take care of doing a full installation of the app, including installing the required dependencies. Then, they can do library(yourpackagename) and run_app() on their machine. 14.3 Deploying Apps with {golem} The other way to make your application available to others is by sending it to a remote server that can serve Shiny applications. When using {golem}, you can open the dev/03_deploy.R and find the functions for server deployment. At the time of writing this book, there are two main ways to deploy a shiny app on a server: Rstudio’s solutions A docker based solution 14.4 RStudio Environments Rstudio proposes three services to deploy Shiny application : shinyapps.io, an on-premise solution that can serve Shiny application (freemium) Shiny-server, a software you have to install on your own server, and that can be used to deploy multiple applications (you can find either an open source or a professional edition) Rstudio connect, a server-based solution that can deploy Shiny applications and markdown documents (and other kind of content) Each of these platforms has its own function to create an app.R file that is to be used as a launch script of each platform. golem::add_rstudioconnect_file() golem::add_shinyappsio_file() golem::add_shinyserver_file() What these app.R files do is calling a pkgload::load_all() function, that will mimic the launch of your package, and then call the run_app() function from your packaged app. Note that if you need to configure the way your app is launched on these plateforms (for example if you need to pass arguments to the run_app() function), you will have to edit this file. Another way to deploy your {golem} based app to Shiny server and to Connect is to link these two software to a local repository (for example an RStudio Package Manager), and then to only use mypackage::run_app() to the app.R. 14.5 Docker Docker is an open source software used to build and deploy applications in containers. Docker has become an core solution in the DevOps world and a lot of server solution are based on it. See the “Strengthen” chapter for a more complete introduction to Docker. You will find the function for creating a Dockerfile for your {golem} app inside the 03_deploy.R file, which contains a series of 3 functions: golem::add_dockerfile() golem::add_dockerfile_shinyproxy() golem::add_dockerfile_heroku() The first function creates a “generic” Dockerfile, in the sense that it is not specific to any platform, and would work out of the box for your local machine. The second one is meant for ShinyProxy, an open source solution for deploying containarized Shiny application, and the third for Heroku, an online service that can serve containerized applications (not specific to Shiny). Note that the Dockerfile creation in {golem} tries to replicate your local environment as precisely as possible, notably by matching your R version, and the version of the packages you have installed on your machine. System requirements are also added when they are found on &lt;sysreqs.r-hub.io&gt;). Otherwise you might have to add them manually. "],
["when-optimize.html", "Chapter 15 The Need for Optimization 15.1 Build first, then optimize 15.2 Tools for profiling", " Chapter 15 The Need for Optimization The most powerful optimization technique in any programmer’s toolbox is to do nothing. .right{ text-align: right;} The Art of Unix Programming 15.1 Build first, then optimize 15.1.1 Identifying bottlenecks As Donald Knuth puts it “Premature optimization is the root of all evil”. What does that means? That focusing on optimizing small portions of your app before making it work fully is the best way to lose time along the way, even more in the context of a production application, where there are deadlines and a limited amount of time to build the application. Why? Here is the general idea: in the following schema below, you can make the circles travel to the bottleneck as fast as you want, the circles will still be slowed by the narrow bottleneck, hence you will just be losing time making the circle movement faster, without actually gaining any time on the global performance. So focus on making the bottleneck larger, before focusing on making the circle travel fast. When? Once the application is ready: here in our example, we can only detect the bottleneck once the bottle is actually built, not while we are “building the circle”. .right{ text-align: right;} Adapted from WikiMedia This is the very thing you should be optimizing: having faster code anywhere else except this bottleneck will not make your app faster: you will just make your app reach the bottleneck faster, but there will still be this part of your app that slows everything down. But this is something you might only realize when the app is fully built: pieces might be fast together, but slow when put together. It is also possible that the test dataset you have been using from the start works just fine, but when you try your app with a bigger, more realistic dataset, the application is actually way slower than it should be. And, maybe you have been using an example dataset so that you do not have to query the database every time you implement a new feature, but actually the SQL query to the database is very slow. This is something you will discover only when the application is fully functional, not when building the parts: and realizing that when you only have 5% of the allocated time for this project left on your calendar is not a good surprise. 15.1.2 Do you need faster functions? Optimizing an app is a matter of trade-offs: of course, in a perfect world, every piece of the app would be tailored to be fast, easy to maintain, and elegant. But in the real world, you have deadlines, limited times and resources, and we are all but humans. That means that at the end of the day, your app will not be completely perfect: a software can always be made better. No piece of code has ever reached complete perfection. Given that, do you want to spend 5 days out of the 30 you have planned optimizing a function so that it runs in a quarter of a second instead of half a second, then realize the critical bottleneck of your app is actually the SQL query and not the data manipulation? Of course a function running two times faster is a good thing, but think about it in context: for example, how many times is this function called ? We can safely bet that if your function is only called once, working on making it twice faster might not be the one function you would want to focus on (well, unless you have unlimited time to work on your project, and in that case lucky you, you can spend a massive amount of time building the perfect software). On the other hand, the function which is called thousands of time in your application might benefit from being optimized. And all of this is basic maths. Let’s assume the following: A current scenario takes 300 seconds to be accomplished on your application One function A() takes 30 seconds, and it’s called once One function B() takes 1 second, and it’s called 50 times If you divide the execution time of A() by two, you would be performing a local optimization of 15 seconds, and a global optimization of 15 seconds. On the other hand, if you divide the execution time of B() by two, you would be performing a local optimization of 0.5 seconds, but a global optimization of 25 seconds. Again, this kind of optimization is hard to detect until the app is functional. An optimization of 15 seconds is way greater that an optimization of 0.5 seconds. Yet you will only realize that once the application is up and running! 15.1.3 Don’t sacrifice readability As said in the last section, every piece of code can be rewritten to be faster, either from R to R or using a lower level language: for example C or C++. You can also rebuild data manipulation code use one package to another39, use complex data structures to optimizing memory usage, etc, etc. But that comes with a price: not keeping thing simple for the sake of local optimization makes maintenance harder, even more if you are using a lesser known language/package. For example, switching some portions of your code to C++ implies that you might be the only person being able to maintain that specific portion of code, or that your colleague taking over the project will have to spend hours learning the tools you have been building, or the language you have chosen to write your functions with. Again, optimization is always a matter of trade-off: is the half-second local optimization worth the extra hours you will have to spend correcting bugs when the app will crash and when you will be the only one able to correct it? Also, are the extra hours/days spent rewriting a working code-base worth the speed gain of 0.5 seconds on one function? For example, let’s compare both these implementations of the same function, one in R, and one in C++. Of course, the C++ function is faster than the R one—this is the very reason of using C++ with R. library(&quot;Rcpp&quot;) cppFunction(&quot; double mean_cpp(NumericVector x) { int j; int size = x.size(); double res = 0; for (j = 0; j &lt; size; j++){ res = res + x[j]; } return res / size; }&quot;) benched &lt;- bench::mark( cpp = mean_cpp(1:100000), native = mean(1:100000), iterations = 1000 ) benched [90m# A tibble: 2 x 6[39m expression min median `itr/sec` mem_alloc `gc/sec` [3m[90m&lt;bch:expr&gt;[39m[23m [3m[90m&lt;bch:&gt;[39m[23m [3m[90m&lt;bch:&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;bch:byt&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [90m1[39m cpp 152µs 456µs [4m2[24m463. 784KB 32.4 [90m2[39m native 440µs 458µs [4m2[24m160. 0B 0 (Note: we’ll come back on bench::mark in the next chapter) Though, how much time gain is worth being sure you will get someone in your team to take over the maintenance if needed? In other words, given that (in our example), we are gaining around -1.7410^{-6} on the execution time of our function, is it worth switching to C++? Using external languages or complex data structures implies that from the start, you will need to think about who and how your code base will be maintain over the years. Chances are that if you plan on using a Shiny application during a span of several years, various R developers will be working on the project, and including C++ code inside your application means that these future developer will either be required to know C++, or they will not be able to maintain this piece of code. So, to sum up, there are three ways to optimize your application &amp; R code, and the bad news is that you can not optimize for all of them: Optimizing for speed Optimizing for memory Optimizing for readability/maintainability Leading a successful project means that you should, as much as possible, find the perfect balance between these three. 15.2 Tools for profiling 15.2.1 Profiling R code 15.2.1.1 Identifying bottlenecks The best way to profile R code is by using the {profvis} package40, a package designed to evaluate how much time each part of a function call take. With {profvis}, you can spot the bottleneck of your function. Without an automated tool to do the profiling, the developers would have to profile by guessing, which will, most of the time, come with bad results: One of the lessons that the original Unix programmers learned early is that intuition is a poor guide to where the bottlenecks are, even for one who knows the code in question intimately. .right{ text-align: right;} The Art of Unix Programming Instead of guessing, it is safe bet to go for a tool like {profvis}, which allows to have a detailed view of what takes a long time to run in your R code. Using this package is quite straightforward: put the code you want to benchmark inside the profvis() function41, wait for the code to run, and… that is it, you now have an analysis of your code running time. Here is an example with 3 nested functions, top(), middle() and bottom(), where top() calls middle() which calls bottom(): library(profvis) top &lt;- function(){ # We use profvis::pause() because Sys.sleep() doesn&#39;t # show in the flame graph pause(0.1) lapply(1:10, function(x){ x * 10 }) middle() } middle &lt;- function(){ pause(0.2) 1e4 * 9 bottom_a() bottom_b() } bottom_a &lt;- function(){ pause(0.5) print(&quot;hey&quot;) } bottom_b &lt;- function(){ pause(2) print(&quot;hey&quot;) } profvis({ top() }) What you see now is what is called a flame graph: it is a detailed timing of how your function has run, with a clear decomposition of the call stack. What you see on top window is the expression evaluated, and on the bottom a detail of the call stack, with what looks like a little bit like a Gantt diagram. This result reads as such: the wider the function call, the more time it has taken R to computer this piece of code. On the very bottom, the “top” function (i.e. the function which is directly called in the console), and the more you go up, the more you enter the nested function calls. Here is how to read this graph: On the x axis, the time spent computing the function. Our top() function being the only one executed, it takes the whole record time. Then, the second level is the first level of what is called inside top(): first, the function pauses, then it does a series of call to FUN (which is the internal anonymous function from lapply()), then calls the middle() function, which spans from around 100 ms to the end of the call. Then, a detail of middle(), which calls bottom_a() and bottom_b(), which each pause() for a given amount of time. If you click on the “Data” tab, you will also find another view of the flame graph, where you can read the hierarchy of calls and the time and memory spent on each function call: If you are working on profiling the memory usage, you can also use the {profmem} package which, instead of focusing on execution time, will record the memory usage of calls. library(profmem) p &lt;- profmem({ x &lt;- raw(1000) A &lt;- matrix(rnorm(100), ncol = 10) }) p Rprofmem memory profiling of: { x &lt;- raw(1000) A &lt;- matrix(rnorm(100), ncol = 10) } Memory allocations: what bytes calls 1 alloc 1048 raw() 2 alloc 272 matrix() 3 alloc 560 matrix() 4 alloc 552 matrix() 5 alloc 1072 matrix() 6 alloc 848 matrix() -&gt; rnorm() 7 alloc 2552 matrix() -&gt; rnorm() 8 alloc 848 matrix() 9 alloc 528 &lt;internal&gt; 10 alloc 1648 &lt;internal&gt; 11 alloc 1648 &lt;internal&gt; 12 alloc 1072 &lt;internal&gt; 13 alloc 256 &lt;internal&gt; 14 alloc 456 &lt;internal&gt; 15 alloc 216 &lt;internal&gt; 16 alloc 256 &lt;internal&gt; total 13832 You can also get the total allocated memory with: total(p) [1] 13832 And extract specific values based on the memory allocation: p2 &lt;- subset(p, bytes &gt; 1000) print(p2) Rprofmem memory profiling of: { x &lt;- raw(1000) A &lt;- matrix(rnorm(100), ncol = 10) } Memory allocations: what bytes calls 1 alloc 1048 raw() 5 alloc 1072 matrix() 7 alloc 2552 matrix() -&gt; rnorm() 10 alloc 1648 &lt;internal&gt; 11 alloc 1648 &lt;internal&gt; 12 alloc 1072 &lt;internal&gt; total 9040 (Example extracted from {profmem} help page). Here it is, now you have a tool to identify bottlenecks! 15.2.1.2 Benchmarking R Code Identifying bottlenecks is a start, but what to do now? In the next chapter about optimization, we will dive deeper into common strategies for optimizing R &amp; Shiny code. But before that, remember this rule: never start optimizing if you can not benchmark this optimization. Why? Because developers are not perfect at identifying bottlenecks and estimating if something is faster or not, and some optimization methods might lead to slower code. Of course, most of the time they will not, but in some cases adopting optimization methods leads to writing slower code, because we have missed a bottleneck in our new code. And of course, without a clear documentation of what we are doing, we will be missing it, relying only on our intuition as an rough guess of speed gain. In other words, if you want to be sure that you are actually optimizing, be sure that you have a basis to compare with. How to do that? One thing that can be done is to keep an RMarkdown file with your starting point: use this notebook to keep track of what you are doing, by noting where you are starting from (i.e, what’s the original function you want to optimize), and compare it with the new one. By using an Rmd, you can document the strategies you have been using to optimize the code, e.g: “switched from for loop to vectorize function”, “changed from x to y”, etc. This will also be helpful for the future: either for you in other projects (you can get back to this document), or for other developers, as it will explain why specific decisions have been made. To do the timing computation, you can use the {bench} package, which compares the execution time (and other metrics) of two functions. This function takes a series of named elements, each containing an R expression that will be timed. Note that by default, the mark() function compares the output of each function, Once the timing is done, you will get a data.frame with various metrics about the benchmark. x &lt;- function(size){ res &lt;- numeric(size) for (i in 1:size){ res[i] &lt;- i * 10 } return(res) } y &lt;- function(size){ (1:size) * 10 } res &lt;- bench::mark( `for` = x(1000), vectorized = y(1000), iterations = 1000 ) res [90m# A tibble: 2 x 6[39m expression min median `itr/sec` mem_alloc [3m[90m&lt;bch:expr&gt;[39m[23m [3m[90m&lt;bch:t&gt;[39m[23m [3m[90m&lt;bch:t&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;bch:byt&gt;[39m[23m [90m1[39m for 54.45µs 58.52µs [4m1[24m[4m7[24m164. 30.4KB [90m2[39m vectorized 4.33µs 5.05µs [4m9[24m[4m5[24m477. 11.8KB [90m# … with 1 more variable: `gc/sec` [3m[90m&lt;dbl&gt;[90m[23m[39m Here, we have an empiric evidence that one code is faster than the other: by benchmarking the speed of our code, we are able to determine which function is the fastest. If you want a graphical analysis, {bench} comes with an autoplot method for {ggplot2}: ggplot2::autoplot(res) And, bonus point, {bench} takes time to check that the two outputs are the same, so that you are sure you are comparing the very same thing, which is another crucial aspect of benchmarking: be sure you are not comparing apple with oranges! 15.2.2 Profiling Shiny You can profile Shiny application using the {profvis} package, just as any other piece of R code. The only thing to note if you want to use this function on an app built with {golem}, you will have to wrap the run_app() function in a print() function. Long story short, what make the app run is not the function itself, but the printing of the function, so the object returned by run_app() itself can not be profiled. See the discussion on this issue on {golem} to learn more about this. One other thing that can be optimized when it comes to the user interface is the webpage rendering performance. To do that, we can use standard web development tools: as said several times, a Shiny application IS a web application, so tools that are language agnostic will work with Shiny. There are thousands of tools available to do exactly that, and going through all of them would probably not make a lot of sense. So, let’s focus on getting started with a basic but powerful tool, that comes for free inside your browser: Google Lighthouse, one of the famous tool for profiling web pages, and which is bundled into recent versions of Google Chrome. The nice thing is that this tool not only cover what you see (i.e. not only what you are actually rendering on your personal computer), but can also audits your app with various configuration, notably on mobile, with low bandwidth and/or mimicking 3G connection. being able to perform audit of our application as seen on a mobile device is a real strength: we are developing application on our computer, and might not be regularly checking how our application is performing on a mobile. Yet a large portion of the web navigation is performed on a mobile or table. Already in 2016, Google wrote that “More than half of all web traffic now comes from smartphones and tablets”. Knowing the exact number of visitors that browse through mobile is hard: the web is vast, and not all website record the traffic they receive. Yet many, if not all, studies around how the web is browsed are reporting the same results: more traffic is performed via mobile than via computer42. And, the pro of running it in your browser is that it can perform the analysis on locally deployed applications: in other word, you can launch your Shiny application in your R console, open the app in Google Chrome, and run the audit. A lot of online services needs an URL to do the audit! Each result from the audit comes with a series of advises and changes you can make on your application to make it better, with links to know more about the specific issue. And of course, last but not least, you also got the results of the metrics you have “passed”, and it is always a good mood booster to see our app passing some audited points! Here is a quick introduction to this tool: Open Chrome in incognito mode, so that the page performance is not influenced by any of the installed extensions in your Google Chrome Open your developer console, either by going to View &gt; Developer &gt; Developer tools, by doing right click &gt; Inspect, or with the keyboard shortcut ctrl/cmd + alt + I Go to the “Audit” tab Configure your report (or leave the default) Click on “Generate Report” Note that you can also install a command line tool with npm install -g lighthouse43, then run lighthouse http://urlto.audit: it will produce either a JSON (if asked) or an HTML report (the default). Google Lighthouse is computing a series of analysis about your webpage. Once the audit is finished, you have some basic but useful indications about your application: Performance. This metric mostly analyzes the rendering time of the page: for example how many time does it take to load the app in full, that is to say how many time it takes from the first byte received to the app being fully ready to be used, the time between the very first call to the server and the very first response, etc. With Shiny, you will probably get low performance here, notably due to the fact that {shiny} is serving external dependencies that you might not be able to control. For example, the report from {hexmake} suggests to “Eliminate render-blocking resources”, and most of them are not controlled by the shiny developer: they come bundled with shiny::fluidPage itself. Accessibility. Google Lighthouse performs a series of tests about accessibility (see our chapter about accessibility for more information). Best practices bundles a list of “misc” best practices around web applications. SEO: search engine optimization, or how will your app perform when it comes to search engine indexation. Progressive Web App (PWA): a PWA is an app that can run on any device, \" reaching anyone, anywhere, on any device with a single codebase\". Google audit your application to see if your application fits with this idea. Profiling web page is a wide topic and a lot of things can be done to enhance the global page performance. That being said, if you have a limited time to invest in optimizing the front-end performance of the application, Google Lighthouse is a perfect tool, and can be your go-to audit tool for your application. And if you want to do if from R, the npm lighthouse module allows to output the audit in JSON, which can then be brought back to R! lighthouse --output json --output-path data-raw/output.json http://localhost:2811 Then, being a JSON file, you can call if from R: lighthouse_report &lt;- jsonlite::read_json(&quot;data-raw/output.json&quot;) lighthouse_report$audits$`speed-index` $id [1] &quot;speed-index&quot; $title [1] &quot;Speed Index&quot; $description [1] &quot;Speed Index shows how quickly the contents of a page are visibly populated. [Learn more](https://web.dev/speed-index).&quot; $score [1] 0.56 $scoreDisplayMode [1] &quot;numeric&quot; $numericValue [1] 5439 $displayValue [1] &quot;5.4 s&quot; Google Lighthouse also comes with a continuous integration tool, so that you can use it as a regression testing tool for your application. To know more, feel free to read the documentation! 15.2.3 More resources about web page performance Why Performance Matters - Google Web Fundamentals Web Performance - Mozilla Web Docs "],
["optim-caveat.html", "Chapter 16 Common Application Caveats 16.1 Reactivity anti-patterns 16.2 R does too much 16.3 Reading data", " Chapter 16 Common Application Caveats 16.1 Reactivity anti-patterns 16.1.1 Reactivity is awesome… until it is not Let’s face it, reactivity is awesome… until it is not. Reactivity is a common source of confusion for beginners, and a common source of bugs and bottlenecks, even for seasoned Shiny developers. Most of the time, issues come from the fact that there is too much reactivity, i.e. we build apps where too much things happen, and some things are updated way more often than they should, and computations are performed when they should not. Of course, it is a nice feature to make everything react instantly to changes, but when building larger apps it is easy to create monsters, i.e complicated, messy reactive graphs where everything is updated too much and too often. Or worse, we generate endless reactive loops, aka “the reactive inferno” where A invalidates B which invalidates C which invalidates A which invalidates B which invalidates C… Let’s take a small example of a reactive inferno: library(shiny) ui &lt;- function(){ tagList( dateInput( &quot;date&quot;, &quot;choose a date&quot; ), selectInput( &quot;year&quot;, &quot;Choose a year&quot;, choices = 2010:2030 ) ) } server &lt;- function( input, output, session ){ observeEvent( input$date , { updateSelectInput( session, &quot;year&quot;, selected = lubridate::year(input$date) ) }) observeEvent( input$year , { updateDateInput( session, &quot;date&quot;, value = lubridate::as_date( sprintf(&quot;%s-01-01&quot;, input$year) ) ) }) } shinyApp(ui, server) Here, we want to handle something pretty common: The user can pick a date and the year input is updated And the other way round: when the year input changes, the date is updated too But if you try to run this in your console, it will end as a reactive inferno: date update year that updates date that updates year… And the more you work on your app, the more complex it gets, and the more you will be likely to end up in the reactive inferno. In this section, we will be speaking a little bit about reactivity and how to have more control on it, and about a way to share data across modules without relying on passing along reactive objects. 16.1.2 observe vs observeEvent One of the most common feature of reactive inferno is the use of observe() in cases where you should use observeEvent. Spoiler: you should try to use observeEvent() as much as possible, and avoid observe()as much as possible. At first, observe() seems easier to implement, and feels like a shortcut as you do not have to think about what to react to: everything gets updated without you thinking about it. But the truth is that this stairway does not lead to heaven. Let’s stop and think about observe() for a minute. This function updates every time a reactive object it contains is invalidated. Yes, this works well if you have a small amount of reactive objects in the observer, but that gets tricky whenever you start adding things a long list of things inside your observe(), as you might be launching a computation 10 times if your reactive scope contains 10 reactive objects that are somehow invalidated in chain. And believe us, we have seen pieces of code where the observe() contains hundreds of lines of code, with reactives objects all over the place, with one observe() context being invalited dozens of times when one input changes in the application. For example, let’s start with that: ## DO NOT DO GLOBAL VARIABLES, IT&#39;S JUST TO SIMPLIFY THE EXAMPLE i &lt;- 0 library(shiny) library(cli) ui &lt;- function(request){ tagList( textInput(&quot;txt&quot;, &quot;txt&quot;) ) } server &lt;- function(input, output, session){ observe({ i &lt;&lt;- i + 1 cat_rule(as.character(i)) print(input$txt) }) } shinyApp(ui, server) Oh, and then, let’s add a small selectInput(): i &lt;- 0 library(shiny) library(cli) ui &lt;- function(request){ tagList( textInput(&quot;txt&quot;, &quot;txt&quot;), selectInput(&quot;tolower&quot;, &quot;casse&quot;, c(&quot;lower&quot;, &quot;upper&quot;)) ) } server &lt;- function(input, output, session){ observe({ i &lt;&lt;- i + 1 cat_rule(as.character(i)) if (input$tolower == &quot;lower&quot;) { print(tolower(input$txt)) } else { print(tolower(input$txt)) } }) } shinyApp(ui, server) And, as time goes by, we add another control flow to our observe(): i &lt;- 0 library(shiny) library(cli) library(stringi) ui &lt;- function(request){ tagList( textInput(&quot;txt&quot;, &quot;txt&quot;), selectInput(&quot;tolower&quot;, &quot;casse&quot;, c(&quot;lower&quot;, &quot;upper&quot;)), checkboxInput(&quot;rev&quot;, &quot;reverse&quot;) ) } server &lt;- function(input, output, session){ observe({ i &lt;&lt;- i + 1 cat_rule(as.character(i)) if (input$rev){ x &lt;- stri_reverse(input$txt) } else { x &lt;- input$txt } if (input$tolower == &quot;lower&quot;){ print(tolower(x)) } else { print(tolower(x)) } }) } shinyApp(ui, server) And it would be nice to keep the selected values into a reactive list, so that we can reuse it elsewhere. And maybe you would like to add a checkbox so that the logs are printed to the console only if checked. i &lt;- 0 library(shiny) library(cli) library(stringi) ui &lt;- function(request){ tagList( textInput(&quot;txt&quot;, &quot;txt&quot;), selectInput(&quot;tolower&quot;, &quot;casse&quot;, c(&quot;lower&quot;, &quot;upper&quot;)), checkboxInput(&quot;rev&quot;, &quot;reverse&quot;) ) } server &lt;- function(input, output, session){ r &lt;- reactiveValues() observe({ i &lt;&lt;- i + 1 cat_rule(as.character(i)) if (input$rev) { r$x &lt;- stri_reverse(input$txt) } else { r$x &lt;- input$txt } if (input$tolower == &quot;lower&quot;){ r$x &lt;- tolower(r$x) } else { r$x &lt;- toupper(r$x) } }) } shinyApp(ui, server) Ok, now can you tell how many potential invalidation points we have got here? Three: whenever input$txt, input$rev or input$tolower change. Of course, three is not that much, but you get the idea. Let’s pause a minute and think about why we use observe() here. To update the values inside r$x, yes. But do we need to use observe() for, say, updating r$x under dozens of conditions, each time the user types a letter? Possibly not. We generally want our observer to update its content under a small, controlled number of inputs, i.e. with a controlled number of invalidation points. And, what we often forget is that users do not type/select correctly on the first try. No, they usually try and miss, restart, change things, amplifying the reactivity “over-happening”. Moreover, long observe() statements are hard to debug, and they make collaboration harder when the trigger to the observe logic can potentially lives anywhere between line one and line 257 of your observe(). That’s why (well, in 99% of cases), it is safer to go with observeEvent, as it allows to see at a glance what are the condition under which the content is invalidated and re-evaluated. Then, if a reactive context is invalidated, you know why. For example, here is where the reactive invalidation can happen observe({ i &lt;&lt;- i + 1 cat_rule(as.character(i)) * if (input$rev) { * r$x &lt;- stri_reverse(input$txt) } else { * r$x &lt;- input$txt } * if (input$tolower == &quot;lower&quot;){ r$x &lt;- tolower(r$x) } else { r$x &lt;- toupper(r$x) } }) Whereas in this code, it is easier to identify where the invalidation can happen: observeEvent( c( * input$rev, * input$txt ) ,{ i &lt;&lt;- i + 1 cat_rule(as.character(i)) if (input$rev) { r$x &lt;- stri_reverse(input$txt) } else { r$x &lt;- input$txt } if (input$tolower == &quot;lower&quot;){ r$x &lt;- tolower(r$x) } else { r$x &lt;- toupper(r$x) } }) 16.1.3 Building triggers and watchers To prevent this, one way to go is to create “flags” objects, which can be thought of as internal buttons to control what you want to invalidate: you create the button, set some places where you want these buttons to invalidate the context, and finally press these These objects are launched with an init function, then these flags are triggered with trigger(), and wherever we want these flags to invalidate a reactive context, we watch() these flags. The idea here is to get a full control over the reactive flow: we only invalidate contexts when we want, making the general flow of the app more predictable. These flags are available using the {gargoyle} package, that can be installed from GitHub with: remotes::install_github(&quot;ColinFay/gargoyle&quot;) gargoyle::init(\"this\") initiate a \"this\" flag: most of the time you will be generating them at the app_server() level gargoyle::watch(\"this\") sets the flag inside a reactive context, so that it will be invalidated every time you trigger(\"this\") this flag. gargoyle::trigger(\"this\") triggers the flags And, bonus, as these functions use the session object, they are available across all modules. That also means that you can easily trigger an event inside a module from another one. This pattern is for example implemented into {hexmake} (though not with {gargoyle}), where the rendering of the image on the right is fully controlled by the \"render\" flag. The idea here is to allow a complete control over when the image is recomputed: only when trigger(\"render\") is called does the app regenerate the image, helping us lower the reactivity of the application. That might seems like a lot of extra work, but that is definitely worth considering on the long run, as it will help optimizing the rendering (fewer computation) and lowering the number of errors that can result from too much reactivity inside an application. 16.1.4 Using R6 as a data storage One pattern we have also been playing with is storing the app business logic inside one or more R6 objects. Why would we want to do that? 16.1.4.1 Sharing data accross module Sharing an R6 object makes it simpler to create data that are shared across modules, but without the complexity generated by reactive objects, and the instability of using global variables. So basically, the idea is to hold the whole logic of your data reading / cleaning / processing / outputting inside an R6 class. An object of this class is then initiated at the top level of your application, and you can pass this object to the sub-modules. Of course, this makes even more sense if you are combining it with the trigger/watch pattern from before! library(shiny) nameui &lt;- function(id){ ns &lt;- NS(id) tagList( # [...] ) } name &lt;- function(input, output, session, obj){ ns &lt;- session$ns output$that &lt;- renderThis({ obj$compute() trigger(&quot;plot&quot;) }) } name2ui &lt;- function(id){ ns &lt;- NS(id) tagList( # [...] ) } name2 &lt;- function(input, output, session){ ns &lt;- session$ns output$plot &lt;- renderThis({ watch(&quot;plot&quot;) obj$plot() }) } ui &lt;- function(request){ tagList( nameui(&quot;nameui&quot;), name2ui(&quot;name2ui&quot;) ) } server &lt;- function( input, output, session ){ obj &lt;- MyDataProcess$new() callModule(name, &quot;nameui&quot;, obj) callModule(name2, &quot;name2ui&quot;, obj) } shinyApp(ui, server) 16.1.4.2 Get sure it is tested During the process of building a robust Shiny app, we strongly suggest that you test as many things as you can. This is where using an R6 for your business logic of your app makes sense: this allows you to build the whole testing of your application data logic outside of any reactive context: you simply build unit tests just as any other function. 16.2 R does too much 16.2.1 Rendering UI from server side There are many reasons we would want to change things on the UI based on what happens in the server: changing the choices of a selectInput() based on the columns of a table which is uploaded by the user, showing and hiding pieces of the app according to an environment variable, allow the user to create an indeterminate amount of inputs, etc. Chances are that to do that, you have been using the uiOutput() &amp; renderUI() functions from {shiny}. Even if convenient, and the functions of choice in some specific context, this couple makes R do a little bit too much: you are making R regenerate the whole UI component instead of changing only what you need. Plus, you will create a code that is harder to reason about, as we are used to have the UI parts in the UI functions (but that is not related to performance). Here are three strategies to code without uiOutput() &amp; renderUI(). 16.2.1.1 Implement UI events in JavaScript Mixing languages is better than writing everything in one, if and only if using only that one is likely to overcomplicate the program. .right{ text-align: right;} Applying the Unix Philosophy - The Art of Unix Programming We will see in the last chapter of this book how you can integrate JS inside your Shiny app, and how even basic functions can be useful for making your app server lighter. For example, compare: library(shiny) ui &lt;- function(){ tagList( actionButton( &quot;change&quot;, &quot;show/hide graph&quot;, onclick = &quot;$(&#39;#plot&#39;).toggle()&quot; ), plotOutput(&quot;plot&quot;) ) } server &lt;- function( input, output, session ){ output$plot &lt;- renderPlot({ cli::cat_rule(&quot;Rendering plot&quot;) plot(iris) }) } shinyApp(ui, server) Shiny applications not supported in static R Markdown documents to library(shiny) ui &lt;- function(){ tagList( actionButton(&quot;change&quot;, &quot;show/hide graph&quot;), plotOutput(&quot;plot&quot;) ) } server &lt;- function( input, output, session ){ r &lt;- reactiveValues(plot = iris) observeEvent( input$change , { if (input$change %% 2 == 0){ r$plot &lt;- iris } else { r$plot &lt;- NULL } }) output$plot &lt;- renderPlot({ cli::cat_rule(&quot;Rendering plot&quot;) req(r$plot) plot(r$plot) }) } shinyApp(ui, server) Shiny applications not supported in static R Markdown documents The result is the same, but the first version is shorter and easier to reason about: we have one button, and the behavior of the button is contained into itself. And, on top of being harder to maintain, the second solution redraws the plot every time the reactiveValues is updated, making R compute way more than it should, whereas with the JavaScript only solution, the plot is not recomputed every time you need to show it. At a local level, the improvements described in this section will not make your application way faster: for example, rendering UI elements (let’s says rendering a simple title) will not be computationally heavy. But at a global level, lighter UI computation from the server side helps the general rendering of the app: let’s say you have an output that takes 3 seconds to run, then if the whole UI + output is to be rendered on the server side, the whole UI stays blank. Compare: library(shiny) ui &lt;- function(){ tagList( uiOutput(&quot;caption&quot;) ) } server &lt;- function( input, output, session ){ output$caption &lt;- renderUI({ Sys.sleep(3) tagList( h3(&quot;test&quot;), p(&quot;caption&quot;) ) }) } shinyApp(ui, server) Shiny applications not supported in static R Markdown documents to library(shiny) ui &lt;- function(){ tagList( h3(&quot;test&quot;), textOutput(&quot;caption&quot;) ) } server &lt;- function( input, output, session ){ output$caption &lt;- renderText({ Sys.sleep(3) &quot;Lorem ipsum dolor&quot; }) } shinyApp(ui, server) Shiny applications not supported in static R Markdown documents In the first example, the UI will wait for the server to have rendered, while in the second we will first see the title, then the rendered text after a few seconds. That approach makes the user experience better: they know that something is happening, while completely blank page is confusing. Also, R being single threaded, manipulating DOM elements from the server side make R busy doing these DOM manipulation while it could be computing something else. And let’s imagine it takes a quarter of a second to render the DOM element, that is a full second for rendering four of them, while R should be busy doing something else! 16.2.1.2 update* inputs Almost every Shiny inputs, even the custom ones from packages, come with an update_ function that allows to change the input values from the server side, instead of recreating the UI entirely. For example, here is a way to update the content of a selectInput from the server side: library(shiny) ui &lt;- function(){ tagList( selectInput(&quot;species&quot;, &quot;Species&quot;, choices = NULL), actionButton(&quot;update&quot;, &quot;Update&quot;) ) } server &lt;- function( input, output, session ){ observeEvent( input$update , { spc &lt;- unique(iris$Species) updateSelectInput( session, &quot;species&quot;, choices = spc, selected = spc[1] ) }) } shinyApp(ui, server) Shiny applications not supported in static R Markdown documents This switch to updateSelectInput makes the code easier to reason about as the selectInput is where it should be: inside the UI, instead of another pattern where we would use renderUI() and uiOutput(). Plus, with the update method, we are only changing what is needed, not re-generating the whole input. 16.2.1.3 insertUI and removeUI Another way to dynamically change what is in the UI is with insertUI() and removeUI(). It is more global than the solution we have seen before with setting the reactiveValue to NULL or to a value, as it allows to target a larger UI element: we can insert or remove the whole input, instead of having the DOM element inserted but empty. This method allows to have a lighter DOM: &lt;div&gt; which are not rendered are not generated empty, they are simply not there. Two things to note concerning this method, though: Removing an element from the app will not delete the input from the input list. In other word, if you have selectInput(\"x\", \"x\"), and that you remove this input using removeUI(), you will still have input$x in the server. For example, in the following example, the input$val value will not be removed once you have called removeUI(selector = \"#val\"). library(shiny) ui &lt;- function(){ tagList( textInput(&quot;val&quot;, &quot;Value&quot;, &quot;place&quot;), actionButton(&quot;rm&quot;, &quot;Remove UI&quot;) ) } server &lt;- function( input, output, session ){ observeEvent( input$rm , { removeUI(selector = &quot;#val&quot;) }) observe({ invalidateLater(1000) print(input$val) }) } shinyApp(ui, server) Shiny applications not supported in static R Markdown documents Both these functions take a jQuery selector to select the element in the UI. We will introduce these selectors in the last chapter of this book. 16.2.2 Too Much Data in Memory If you are building a Shiny application, there is a great chance you are building it to analyze data. If you are dealing with large datasets, you should consider deporting the data handling and computation to an external database system: for example to an SQL database. Why? Because these system has been created to handle and manipulate data on disk: in other words it will allow you to perform operation on your data without having to clutter R memory with large dataset. For example, if you have a selectInput() that is used to perform a filter on a dataset, you can do that filter straight inside SQL, instead of bringing all the data to R and then do the filter. That is even more necessary if you are building the app for a large number of users: for example if one Shiny session takes up to 300MB, multiply that by the number of users that will need one session, and you will have a rough estimate of how many RAM you will need. On the contrary, if you lighten the data manipulation so that it is done by the back-end, you will have, let’s say, one database with 300 mo of data, then the database size will remain (more or less constant), and the only RAM used by Shiny will be the data manipulation, not the data storage. That’s even more true now that almost any operation you can do today in {dplyr} would be doable with an SQL backend, and that is the purpose of the {dbplyr} package: translate {dplyr} code into SQL. If using a database as a back-end seems a little bit far-fetched right now, that is how it is done in most programming languages: if you are building a web app with NodeJS or Python for example, and need to interact with data, nothing will be stored in RAM: you will be relying on an external database to store your data. Then your application will be used to make queries to this database backend. 16.3 Reading data Shiny applications are a tool of choice when it comes to analyzing data. But that also means that these data have to be imported/read at some point in time, and reading data can be time consuming. How can we optimize that? In this section, we will have a look at three strategies: including datasets inside your application, using R packages for fast data reading, and when and why you should move to an external database system. 16.3.1 Including Data in your Application If you are building your application using the {golem} framework, you are building your application as a package. R packages provide a way to include internal datasets, that can then be used as objects inside your app. This is the solution you should go for if your data are never to rarely updated: the datasets are created during package development, then included inside the build of your package. The plus side of this approach is that it makes the data fast to read, as they are serialized as R native objects. To include data inside your application, you can use the usethis::use_data_raw( name = \"my_dataset\", open = FALSE ) command which is inside the 02_dev.R script inside the dev/ folder of your source application (if you are building the app with {golem}). This will create a folder called data-raw at the root of your application folder, with a script to prepare your dataset. Here, you can read the data, modify it if necessary, and then save it with usethis::use_data(my_dataset). Once this is done, you will have access to the my_dataset object inside your application. This is for example what is done in the {tidytuesday201942} application, at data-raw/big_epa_cars.R: the csv is read there, and then used as an internal dataset inside the application. 16.3.2 Reading External Datasets Other applications use data that are not available at build time: they are created to analyse data that are uploaded by users, or maybe they are fetch on an external service while using the app (for example by calling an API). When you are building an application for the “user data” use case, the first thing you will need is to provide users a way to upload their dataset: shiny::fileInput(). One crucial thing to keep in mind when it comes to using user-uploaded files is that you have to be (very) strict with the way you handle files: Always specify what type of file you want: shiny::fileInput() has an accept parameter that allows you to set one or more MIME types or extension. When using this argument (for example with text/csv, .csv, or .xslx), the user will only be able to select a subset of files from their computer: the ones that matches the type. Always perform checks once the file is uploaded, even more if it is tabular data: column type, naming, empty rows… The more you check the file for potential errors, the less your application is likely to fail to analyze this uploaded dataset. If the data reading takes a while, do not forget to add a visual progression cue: be it a shiny::withProgress() or tools from the {waiter} package. Why do we do that? Because whenever you offer a user the possibility to upload anything, you can be sure that at some point, they will upload a file that will make the app crash. By setting a specific MIME type and by doing a series of check once the file is uploaded, you will make your application more stable. Finally, having a visual cue that “something is happening” is very important for the user experience, as “something is happening” is better than not knowing what is happening, and it may also prevent the user from clicking again and again on the upload button. Now we have our fileInput() set, how do we read these data as fast as possible? There are several options depending on the type of data you are reading. Here are some packages that can make the file reading faster: For tabular, flat dataset (typically csv, tsv, or text), {vroom} can read data at a 1.40 GB/sec/sec speed. {data.table}, and its fread() function, is also fast at reading delimited files. For JSON files, {jsonlite} If you need to read Excel files inside your app, {readxl} offers a binding to the RapidXML C++ library, reading Excel files fast. 16.3.3 Using External DataBases Another type of data analysed in a shiny application is one contained inside an external database. Database are wildly used in the data science world, and in the software engineering as a whole. Being a widely used source of data, databases come with API and drivers that help retrieving and transferring data: be it SQL, NoSQL, or even graph. Using a database is one of the solution for making your app lighter, and more efficient in the long run, notably if you need to scale your app to thousands of visitors. Indeed, if you plan on having your app scale to numerous people, that will mean that a lot of R processes will be triggered. And if your data is contained in your app, this will mean that each R process will take a significant amount of RAM if the dataset is large. For example, if your dataset alone takes ~300 mb of RAM, that means that if you want to launch the app 10 times, you will need ~3gb of RAM. On the other hand, if you decide to switch these data to an external database, it will lower the global RAM need: the DB will takes these 300mb of data, and each shiny application will make request to the database. So, schematically, if the database needs 300mb, and one shiny app 50mb, then 10 app will be 300mb + 50 * 1b mo. Of course, it is not as simplistic as that, and other things are to be considered: making database requests can be computationally expensive, and might need some network adjustments, but you get the idea. Covering all the available type of databases and the packages associated with each is a very, very large topic: there are dozens of database systems, and as many (if not more) packages to interact with them. For a more extensive coverage of using databases in R, please follow these resources: Databases using R, the official RStudio documentation around databases and R colinfay/r-db, a docker image, with an companion guide, that bundles the toolchain for a lot of database systems for R CRAN Task View: Databases with R: the official task view from CRAN with a series of packages for database manipulation 16.3.4 Reminder How to choose between these three methodologies: Choice Update Size Package data Never to very rare Low to medium Reading files Uploaded by Users Preferably low External DataBase Never to Streaming Low to Big "],
["optimizing-shiny-code.html", "Chapter 17 Optimizing Shiny Code 17.1 Optimizing R code 17.2 Caching elements 17.3 Asynchronous in Shiny", " Chapter 17 Optimizing Shiny Code 17.1 Optimizing R code In its core, Shiny runs R code on the server side. So to be efficient, the R code computing your values and returning results also has to be optimized. Optimizing R code is such a broad topic that it would be possible to write a full book about it, and in fact a lot of books about R already cover this topic. Instead of re-writing these books, we will try to point to some crucial resources you can refer to if you want get started optimizing your R code. Efficient R programming, by Colin Gillespie and Robin Lovelace, has a series of methods you can quickly put into practice for more efficient R code. Advanced R, by Hadley Wickham, has a chapter about optimizing R code (number 24). In the rest of this chapter, we will be focusing on how to optimize Shiny specifically. 17.2 Caching elements 17.2.1 What is caching? Caching is the process of storing resources intensive results so that when they are needed again, your program can reuse the result another time without having to redo the computation again. How does it work? Let’s make a brief parallel with the human brain, and imagine that you know that you will need to use a phone number many time in the day, and for the purpose of this thought experiment you are completely unable to remember it44. What are you going to do? There are two solutions here: either you look in the phone book or in your phone contact list every time you need it, which takes a couple of seconds every time, or you use a post-it that you put on your computer screen with the number of it, so that you have direct access to it when you need it. It takes a couple of seconds the first time you look for the number, but it is almost instantaneous the next times you need it. This is what caching do: keep the result of computation so when they are needed in the very same context, they are quickly accessible. The downside being that you only have limited space on your screen: when your screen is covered by sticky notes, you can not store any more notes45. In the context of an interactive application in a framework like Shiny, it makes much sense to cache data structures: users tend to repeat what they do, or go back and forth between parameters. For example, if you have a graph that take 2 seconds to render (which is quite common in Shiny, notably when relying on {ggplot2}), you do not want these 2 seconds to be repeated over and over again when users switch from one parameter to another and back to the first, as the two graphs will be the same for the same parameter. Same goes for queries to a database: if a query is done with the same parameters, and you know that they will return the same result, there is no need to ask the database again and again—ask the cache to retrieve the data. 17.2.2 Native Caching in R At least two packages in R implement caching of functions (also called memoization): {memoize}, and {memoise}. They both more or less work the same way: you will call a memoization function on another function, and cache is created for this function output, based on the arguments value. Then every time you call this function again with the same parameters, the cache is returned instead of computing the function another time. So for example, if computing your data once takes 5 seconds with the parameter n = 50, the next time you will be calling this function with n = 50, instead of recomputing, R will go and fetch the value stored in cache. Here is a simple example with {memoise}: library(memoise) library(tictoc) fct &lt;- function(sleep = 1){ Sys.sleep(sleep) return(Sys.time()) } mfct &lt;- memoise(fct) tic() mfct(2) [1] &quot;2020-04-24 19:36:26 UTC&quot; toc() 2.136 sec elapsed tic() mfct(2) [1] &quot;2020-04-24 19:36:26 UTC&quot; toc() 0.025 sec elapsed Let’s try with another example that might look more like what we can find in a Shiny App: connecting to a database con &lt;- DBI::dbConnect( RSQLite::SQLite(), dbname = &quot;:memory:&quot; ) # Writing a large dataset to the db DBI::dbWriteTable( con, &quot;diams&quot;, dplyr::bind_rows( purrr::rerun(10, ggplot2::diamonds) ) ) # Do a query to the SQL db fct_sql &lt;- function(SQL, con){ DBI::dbGetQuery( con, SQL ) } mfct &lt;- memoise(fct_sql) tic() res_a &lt;- mfct(&quot;SELECT * FROM diams WHERE cut = &#39;Ideal&#39;&quot;, con) toc() 0.613 sec elapsed tic() res_b &lt;- mfct(&quot;SELECT * FROM diams WHERE cut = &#39;Ideal&#39;&quot;, con) toc() 0.025 sec elapsed all.equal(res_a, res_b) [1] TRUE tic() res_c &lt;- mfct(&quot;SELECT * FROM diams WHERE cut = &#39;Good&#39;&quot;, con) toc() 0.173 sec elapsed setequal(res_a, res_c) [1] FALSE Note that you can change where the cache is stored by {memoise}. Here, we will save it in a temp directory (but do not do this in production). tpd &lt;- fs::path(paste(sample(letters, 10), collapse = &quot;&quot;)) tpd &lt;- fs::dir_create(tpd) dfs &lt;- cache_filesystem(tpd) mfct &lt;- memoise(fct_sql, cache = dfs) res_a &lt;- mfct(&quot;SELECT * FROM diams WHERE cut = &#39;Ideal&#39;&quot;, con) res_b &lt;- mfct(&quot;SELECT * FROM diams WHERE cut = &#39;Good&#39;&quot;, con) fs::dir_tree(tpd) [01;34mcinrauwhbg[0m ├── 556488dff60a10a8 └── a2e57bf2d93fd8cd As you can see, we now have two cache objects inside the directory we have specified as a cache_filesystem. 17.2.3 Caching Shiny At the time of writing this page (April 2020), {shiny} has one caching function: renderCachedPlot(). This function behaves more or less like the renderPlot() function, except that it is tailored for caching. The extra arguments you will find are cacheKeyExpr and sizePolicy: the former is the list of inputs and values that allow to cache the plot—every time these values and inputs are the same, they produce the same graph, so {shiny} will be fetching inside the cache instead of computing the value another time. sizePolicy is a function that returns a width and an height, and which are used to round the plot dimension in pixels, so that not every pixel combination are generated in the cache. The good news is that converting existing renderPlot() functions to renderCachedPlot() is pretty straightforward in most cases: take your current renderPlot(), and add the cache keys46. Here is an example: library(shiny) ui &lt;- function(request){ tagList( selectInput(&quot;tbl&quot;, &quot;Table&quot;, c(&quot;iris&quot;, &quot;mtcars&quot;, &quot;airquality&quot;)), plotOutput(&quot;plot&quot;) ) } server &lt;- function( input, output, session ){ output$plot &lt;- renderCachedPlot({ plot( get(input$tbl) ) }, cacheKeyExpr = { input$tbl }) } shinyApp(ui, server) If you try this app, the first rendering of the three plots will take a little bit of time, but every subsequent rendering of the plot is almost instantaneous. And if we apply what we have just seen with {memoise}: con &lt;- DBI::dbConnect( RSQLite::SQLite(), dbname = &quot;:memory:&quot; ) DBI::dbWriteTable( con, &quot;diams&quot;, dplyr::bind_rows( purrr::rerun(100, ggplot2::diamonds) ) ) fct_sql &lt;- function(cut, con){ # NEVER EVER SPRINTF AN SQL CODE LIKE THAT # IT&#39;S SENSITIVE TO SQL INJECTIONS, WE&#39;RE # DOING IT FOR THE EXAMPLE DBI::dbGetQuery( con, sprintf( &quot;SELECT * FROM diams WHERE cut = &#39;%s&#39;&quot;, cut ) ) %&gt;% head() } db &lt;- cache_filesystem(&quot;cache/&quot;) fct_sql &lt;- memoise(fct_sql, cache = db) ui &lt;- function(request){ tagList( selectInput(&quot;cut&quot;, &quot;cut&quot;, unique(ggplot2::diamonds$cut)), tableOutput(&quot;tbl&quot;) ) } server &lt;- function( input, output, session ){ output$tbl &lt;- renderTable({ fct_sql(input$cut, con) }) } shinyApp(ui, server) You will see that the first time you run this piece of code, it will take a couple of seconds to render the table for a new input$cut value. But if you re-select this input a second time, the output will show instantaneously. Caching is a nice way to make your app faster: even more if you expect your output to be stable over time: if the plot created by a series of inputs stays the same all along your app lifecycle, it is worth thinking about implementing an on-disk caching. At the time of writing these lines (April 2020), you can also use remote caching, in the form of Amazon S3 storage or with Google Cloud Storage. To do that, you will need the development version of {memoise} (version 1.1.0). If your application needs “fresh” data every time it is used, for example because data in the SQL database are updated every hour, cache will not be of much help here, on the contrary: the same inputs on the function will render different output depending on when they are called. One other thing to remember is that, just like our computer screen from our phone number example from before, you do not have unlimited space when it come to storing cache: storing a large amount of cache will take space on your disk. For example, from our stored cache from before: fs::dir_info(tpd)[, &quot;size&quot;] 462.31K 1.86M Managing cache at a system level is out of scope for this book, but note that the most commonly accepted rule for deleting cache is called LRU, for Least Recently Used. The underlying principle of this approach is that users tend to need what they have needed recently: hence the more a piece of data has been used recently, the more likely it is that it will be needed soon. And this can be retrieved with: fs::dir_info(tpd)[, &quot;access_time&quot;] [1] &quot;2020-04-24 19:36:29 UTC&quot; &quot;2020-04-24 19:36:35 UTC&quot; Hence, when using cache, it might be interesting to periodically removed the oldest used cache, so that you can regain some space on the server running the application. 17.3 Asynchronous in Shiny One of the drawbacks of Shiny is that as it is running on top of R, it is single threaded: meaning that each computation is run in sequence, one after the other. Well, at least natively, as methods have emerged to run pieces of code in parallel. 17.3.1 How to To launch code blocks in parallel, we will use a combination of two packages, {future} and {promises}, and a reactiveValue(). {future} is an R package which main purpose is to allow users to send code to be run elsewhere, i.e in another session, thread, or even on another machine. {promises}, on the other hand, is a package providing structure for handling asynchronous programming in R47. 17.3.1.1 Asynchronous for Cross-sessions Availability The first type of asynchronous programming in Shiny is the one that allow non-blocking programming at a cross-session context. In other words, it is a programming method which is useful in the context of running one Shiny session which is accessed by multiple users. Natively, in Shiny, if user1 comes and launches a 15 second computation, then user2 has to wait for this computation to finish, before launching their own 15 second computation, and user 3 has to wait the 15 seconds of user1 plus the 15 seconds for user, etc. With {future} and {promises}, each long computation is sent to be run somewhere else, so when user1 launches their 15 second computation, they are not blocking the R process for user2 and user3. How does it work48? {promises} comes with two operators which will be useful in our case, %...&gt;% and %...!%: the first being “what happens when the future() is solved?” (i.e. when the computation from the future() is completed), and the second is “what happens if the future() fails?” (i.e. what to do when the future() returns an error). Here is an example of using this skeleton: library(future) library(promises) plan(multisession) # We&#39;re opening several R session (future specific) future({ Sys.sleep(3) return(rnorm(5)) }) %...&gt;% ( function(result){ print(result) } ) %...!% ( function(error){ stop(error) } ) If you run this in your console, you will see that you have access to the R console directly after launching the code. And a couple of seconds later (a little bit more than 3), the result of the rnorm(5) will be printed to the console. Note that you can also write one-line function with . as a parameter, instead of building the full anonymous function (we will use this notation in the rest of the chapter): library(future) library(promises) plan(multisession) # We&#39;re opening several R session (future specific) future({ Sys.sleep(3) return(rnorm(5)) }) %...&gt;% print(.) %...!% stop(.) Let’s port this to Shiny: library(shiny) ui &lt;- function(request){ tagList( verbatimTextOutput(&quot;pr&quot;) ) } server &lt;- function( input, output, session ){ output$pr &lt;- renderPrint({ future({ Sys.sleep(3) return(rnorm(5)) }) %...&gt;% print(.) %...!% stop(.) }) } shinyApp(ui, server) If you have run this, that does not seem like a revolution: but trust us, the Sys.sleep() is not blocking as it allows other users to launch the same computation at the same moment. 17.3.1.2 Inner-session Asynchronousity In the previous section we have implemented cross-session asynchronousity, meaning that the code is non-blocking but for when two or more users access the same app: the code is still blocking at an inner-session level. Let’s have a look at this code: library(shiny) ui &lt;- function(request){ tagList( verbatimTextOutput(&quot;pr&quot;), plotOutput(&quot;plot&quot;) ) } server &lt;- function( input, output, session ){ output$pr &lt;- renderPrint({ future({ Sys.sleep(3) return(rnorm(5)) }) %...&gt;% print(.) %...!% stop(.) }) output$plot &lt;- renderPlot({ plot(iris) }) } shinyApp(ui, server) Here, you would expect the plot to be available before the rnorm(), but it is not: {promises} is still blocking at an inner-session level, so elements are still rendered sequentially. To bypass that, we will use a reactiveValue() structure. library(shiny) library(promises) library(future) plan(multisession) ui &lt;- function(request){ tagList( verbatimTextOutput(&quot;pr&quot;), plotOutput(&quot;plot&quot;) ) } server &lt;- function( input, output, session ) { rv &lt;- reactiveValues( res = NULL ) future({ Sys.sleep(5) rnorm(5) }) %...&gt;% (function(e){ rv$res &lt;- e }) %...!% (function(e){ rv$res &lt;- NULL warning(e) }) output$pr &lt;- renderPrint({ req(rv$res) }) output$plot &lt;- renderPlot({ plot(iris) }) } shinyApp(ui, server) Let’s detail this code step by step: rv &lt;- reactiveValues creates a reactiveValue() that will contains NULL, and which will serve the content of renderPrint() when the future() is resolved. It is initiated as NULL so that the renderPrint() is silent at launch. %...&gt;% rv() %...!% is the {promises} structure we have seen before. %...!% (function(e){ rv$res &lt;- NULL ; warning(e) }) is what happens when the future({}) fails: we are setting the rv$res value back to NULL so that the renderPrint() does not fails and print an error in case of failure. 17.3.1.3 Potential Pitfalls of Asynchronous Shiny There is one thing to be aware of if you plan on using this async methodology: that you are not in a sequential context anymore. What that implies is that the first future({}) you will send is not necessary the first you will get back. For example, if you send SQL requests to be run asynchronically and that each call takes between 1 an 10 seconds to return, there is a chance that the first request to return will be the last one you have sent. To handle that, we can adopt two different strategies, depending on what we need: We need only the last expression sent. In other words, if we send three expressions to be evaluated somewhere, we only need to get back the last one. To handle that, the best way is to have an id that is also sent to the future, and when the future comes back, we check that this id is the one we are expecting. If it is, we update the reactiveValues(). If it is not, we ignore it. library(shiny) library(promises) library(future) plan(multisession) ui &lt;- function(request){ tagList( actionButton(&quot;go&quot;, &quot;go&quot;), verbatimTextOutput(&quot;pr&quot;), plotOutput(&quot;plot&quot;) ) } server &lt;- function( input, output, session ) { rv &lt;- reactiveValues( res = NULL, last_id = 0 ) observeEvent( input$go , { rv$last_id &lt;- rv$last_id + 1 last_id &lt;- rv$last_id future({ if (last_id %% 2 == 0){ Sys.sleep(3) } list( id = last_id, res = rnorm(5) ) }) %...&gt;% (function(e){ cli::cat_rule( sprintf(&quot;Back from %s&quot;, e$id) ) if (e$id == rv$last_id){ rv$res &lt;- e } }) %...!% (function(e){ rv$res &lt;- NULL warning(e) }) cli::cat_rule( sprintf(&quot;%s sent&quot;, rv$last_id) ) }) output$pr &lt;- renderPrint({ req(rv$res) }) output$plot &lt;- renderPlot({ plot(iris) }) } shinyApp(ui, server) We need to treat the outputs in the order they are received. In that case, instead of waiting for the very last input, you will need to build a structure that will receive the output, check if this output is the “next in line”, store it if it is not, or return it if it is and see if there is another output in the queue. This type of implementation is a little bit more complex so we will not detail it all inside this chapter, but here is a small implementation using {liteq}. library(promises) library(future) plan(multisession) library(liteq) Attaching package: &#39;liteq&#39; The following object is masked from &#39;package:purrr&#39;: is_empty db &lt;- tempfile() q &lt;- ensure_queue(&quot;jobs&quot;, db = db) for (i in 1:5){ future({ Sys.sleep(sample(1:5, 1)) return(rnorm(5)) }) %...&gt;% (function(res){ publish(q, title = as.character(i), message = paste(res, collapse = &quot;,&quot;)) }) %...!% stop(.) } "],
["optimjs.html", "Chapter 18 Using JavaScript 18.1 A quick introduction to JavaScript 18.2 Client-side JavaScript 18.3 JavaScript &lt;-&gt; Shiny communication 18.4 About {golem} js functions 18.5 Learn more about JavaScript", " Chapter 18 Using JavaScript At its core, building a Shiny app is building a JavaScript application that can talk with an R session. This process is invisible to most Shiny developers, who usually do everything in R, and in the end, this is the case: most of the Shiny apps out there are 100% written with R. In fact, when you are writing UI elements in Shiny, what you are actually doing is building a series of HTML tags. For example, this simple {shiny} code returns a series of HTML tags: fluidPage( h2(&quot;hey&quot;), actionButton(&quot;act&quot;, &quot;Validate&quot;) ) &lt;div class=&quot;container-fluid&quot;&gt; &lt;h2&gt;hey&lt;/h2&gt; &lt;button id=&quot;act&quot; type=&quot;button&quot; class=&quot;btn btn-default action-button&quot;&gt;Validate&lt;/button&gt; &lt;/div&gt; Later on, when the app is launched, {shiny} binds events to UI elements, and these JavaScript events will communicate with R, in the sense that they will send data to R, and receive data from R. Most of the time, when the JavaScript side of the websocket receives on of these events, the page the user sees is modified (for example, a plot is drawn). On the R end of the websocket, i.e when R receives data from the web page, a value is fetched, and something is computed. What happens under the hood is a little bit complex and out of scope for this book, but the general idea is: R talks to your browser through a web socket (that you can imagine as a small “phone line” with both software modules listening at each end49), and this browser talks to R through the same web socket. // TODO: create here a simple Flowchart // R -&gt; (Web Socket) -&gt; JS // R &lt;- (Web Socket) &lt;- JS It’s important to note here that the communication happens in both ways: from R to JavaScript, and from JavaScript to R. In fact, when we write a piece of code like sliderInput(\"plop\", \"this\", 1, 10, 5), what we are doing is creating a binding between JavaScript and R, where the JavaScript runtime (in the browser) listens to any event happening on the slider with the id \"plop\", and whenever it detects that something happens to this element, something (most of the time its value) is sent back to R, and R does computation based on that value. With output$bla &lt;- renderPlot({}), what we are doing is making the two communicate the other way around: we are telling JavaScript to listen to any incoming data from R for the id \"bla\", and whenever JavaScript sees incoming data from R, it puts it into the proper HTML tag (here, JavaScript inserts the image received from R in the &lt;img&gt; tags with the id bla). So even if everything is written with R, we are writing a web application, i.e. HTML, CSS and JavaScript elements. Once you have realized that, the possibilities are endless: in fact almost anything doable in a “classic” web app can be done in Shiny with a little bit of tweaking. What this also implies is that getting (even a little bit) better at writing HTML, CSS, and especially JavaScript will make your app better, lighter, and more user-friendly, as JavaScript is a language that has been designed to interact with a web page: change element appearances, hide and show things, click somewhere, show alerts and prompts… Knowing just enough JavaScript can improve the quality of your app: especially when you have been using R to render some complex UIs: think conditional panels, simulating a button click from the server, hide and show elements… All these things are good examples of where you should be using JavaScript instead of building more or less complex renderUI or insertUI patterns in your server. Moreover, the number of JavaScript libraries available on the web is tremendous ; and the good news is that Shiny has everything it needs to bundle external JavaScript libraries inside your application50. This is what this section of the book aims at: giving you just enough JavaScript knowledge to lighten your Shiny App, in order to improve the global user and developer experience. In this chapter, we will first review some JavaScript basics which can be used “client-side” only, i.e. only in your browser. Then, we will talk about making R &amp; JS communicate with each other, and explore some common patterns for JavaScript in Shiny. Finally, we will quickly present some of the functions available in {golem} that can be used to launch JavaScript. Note that this chapter does not try to be a comprehensive JavaScript course. External resources are linked all throughout this chapter and at the end if you want to dive deeper into JavaScript. 18.1 A quick introduction to JavaScript JavaScript is a programming language which has been designed to work in the browser51. There are three ways to include the JavaScript code inside your web app: As an external file, which is served to the browser alongside your main application page Inside a &lt;script&gt; HTML tag inside your page Inline, on a specific tag, for example by adding an onclick event straight on a tag Note that the good practice when it comes to include JavaScript is to add the code inside an external file. If you are working with {golem}, including a JavaScript file is achieved via two functions: golem::add_js_file(\"name\"), which adds a standard JavaScript file, i.e. one which is not meant to be used to communicate with R. We’ll see in the first part of this chapter how to add JavaScript code there. golem::add_js_handler(\"name\"), which creates a file with a skeleton for Shiny handlers. We’ll see this second type of elements in the JavaScript &lt;-&gt; Shiny communication part. OK, good, but what do we do now? Note that in this chapter, we will not be covering basic JavaScript object and manipulation. Feel free to refer to the first chapter of JavaScript 4 Shiny - Field Notes for a detailed introduction to objects and object manipulation. 18.1.1 Understanding html, class, and id You have to think of a web page as a tree, where the top of the webpage is the root node, and every element in the page is a node in this tree (this tree is called a DOM, for Document Object Model). You can work on any of these HTML nodes with JavaScript: modify it, bind events to it and/or listen to events, hide and show… But first, you have to find a way to identify these elements: either as a group of elements or as a unique element inside the whole tree. That is what HTML semantic elements, classes, and ids are made for. Consider this piece of code: library(shiny) fluidPage( titlePanel(&quot;Hello Shiny&quot;), actionButton(&quot;go&quot;, &quot;go&quot;) ) &lt;div class=&quot;container-fluid&quot;&gt; &lt;h2&gt;Hello Shiny&lt;/h2&gt; &lt;button id=&quot;go&quot; type=&quot;button&quot; class=&quot;btn btn-default action-button&quot;&gt;go&lt;/button&gt; &lt;/div&gt; This {shiny} code creates a piece of HTML code containing three nodes: a div with a specific class (a BootStrap container), an h2, which is a level-two header, and a button which has an id and a class. Both are included in the div. Let’s detail what we have got here: HTML tags, which are the building blocks of the “tree”: here div, h2 and button are HTML tags. The button has an id, which is short for “identifier”. This id has to to be unique: this reference allows to refer to this exact element, and more specifically, it allows JavaScript and R to talk to each other: if you click on a button, you have to be sure you are referring to this specific button, and only that one. Elements can have a class which can apply to multiple elements. This can be used in JavaScript, but it is also very useful for styling elements in CSS. 18.1.2 Querying in Vanilla JavaScript In “Vanilla” JavaScript (i.e without any external plugin installed), you can query these elements using methods from the document object. For example: // Given &lt;div id = &quot;pouet&quot; name=&quot;plop&quot; class = &quot;plouf&quot;&gt;Wesh&lt;/div&gt; // Query with the ID document.querySelector(&quot;#pouet&quot;) document.getElementById(&quot;pouet&quot;) // With the class document.querySelectorAll(&quot;.plouf&quot;) document.getElementsByClassName(&quot;plouf&quot;) // With the name attribute document.getElementsByName(&quot;plop&quot;) // Using the tag name document.getElementsByTagName(&quot;div&quot;) Note that some of these methods have been introduced with ES6, which is a version of JavaScript that came out in 2015. This version of JavaScript is supported by most browser since mid-2016 (and June 2017 for Firefox) (see JavaScript Versions from W3Schools). Most of your users should now be using a browser version that is compatible with ES6, but that is something that you might want to keep in mind: browser version matters when it comes to using JavaScript. 18.1.3 About DOM events When users navigate a webpage, they are generating events on this page: clicking, hovering elements, pressing keys… all these are listened to by the JavaScript runtime, plus some events that are not generated by the users: for example, there is a “ready” event generated when the webpage has finished loading. Most of these events are linked to a specific node in the tree: for example, if you click on something, you are clicking on a node in the DOM. That is where JavaScript events come into play: when an event is triggered in JavaScript, you can link to it a “reaction”, in other word a piece of JavaScript code that is executed when this event occurs. Here are some examples of events: click / dblclick focus keypress, keydown, keyup mousedown, mouseenter, mouseleave, mousemove, mouseout, mouseover, mouseup scroll For a full list, please refer to https://developer.mozilla.org/fr/docs/Web/Events. Once you have this list in mind, you can then select elements in the DOM, then adding addEventListener to them, and defining a callback function: what happens when the event is triggered. For example, the code below adds an event to the input when a key is pressed, showing a native alert() to the user. &lt;input type=&quot;text&quot; id = &quot;plop&quot;&gt; &lt;script&gt; document.getElementById(&quot;plop&quot;).addEventListener(&quot;keypress&quot;, function(){ alert(&quot;pouet&quot;) }) &lt;/script&gt; Note also that Shiny also generates events, meaning that you can customize the behavior of your application based on these events. Here is a code that launches an alert when Shiny is connected: $(document).on(&#39;shiny:connected&#39;, function(event) { alert(&#39;Connected to the server&#39;); }); But wait, what is this weird $()? That’s jQuery, and we will discover it in the very next section! 18.1.4 About jQuery &amp; jQuery selectors The jQuery framework is natively included in Shiny. jQuery is a fast, small, and feature-rich JavaScript library. It makes things like HTML document traversal and manipulation, event handling, animation, and Ajax much simpler with an easy-to-use API that works across a multitude of browsers. .right{ text-align: right;} jQuery home page jQuery is a very popular JavaScript library which is designed to manipulate the DOM, its events and its elements. It can be used to do a lot of things, like hide and show, change class, click somewhere… And to be able to do that, it comes with the notion of selectors, which will be put between $(). You can use, for example: $(\"#plop\") to refer to the element with the id plop $(\".pouet\") to refer to element(s) of class pouet $(\"button:contains('this')\") to refer to the buttons with a text containing 'this' You can also use special HTML attributes, which are specific to a tag. For example, the following HTML code: &lt;a href = &quot;https://thinkr.fr&quot; data-value = &quot;panel2&quot;&gt;ThinkR&lt;/a&gt; contains the href &amp; data-value attributes. You can refer to these with [] after the tag name. $(\"a[href = 'https://thinkr.fr']\") refers to link(s) with href being https://thinkr.fr $('a[data-value=\"panel2\"]') refers to link(s) with data-value being \"panel2\" These and other selectors are used to identify one or more node(s) in the big tree which is a web page. Once we have identified these elements, we can either extract or change data contained in these nodes, or invoke methods contained within these nodes. Indeed JavaScript, as R, can be used as a functional language, but most of what we do is done in an object-oriented way. In other words, you will interact with objects from the web page, and these objects will contain data and methods. Note that this is not specific to jQuery: elements can also be selected with standard JavaScript. jQuery has the advantage of simplifying selections and actions and to be cross-platform, making it easier to ship applications that can work on all major browsers. And it comes with Shiny for free! Choosing jQuery or vanilla JavaScript is up to you: and in the rest of this chapter we will try to mix both syntax, and put both when possible, so that you can choose the one you are the most comfortable with. 18.2 Client-side JavaScript It is hard to give an exhaustive list of what you can do with JavaScript inside Shiny. As a Shiny app is part JavaScript, part R, once you have a good grasp of JavaScript you can quickly enhance any of your applications. That being said, a few common things can be done that would allow you to immediately optimize your application: i.e. small JavaScript functions that will prevent you from writing complex algorithmic logic in your application server. 18.2.1 Common patterns $('#id').show(); and $('.class').hide(); show and hide one or more elements that match the given selector. For example, this can be use to replace: output$ui &lt;- renderUI({ if (this){ tags(...) } else { NULL } }) Note that this will not drastically improve the performance of your application. Though it will help making it lighter in term of code and easier to grasp in term of readability: everything that can be created in the UI stays in the UI, and everything that needs to be performed by R is in the server. alert(\"message\") uses the built-in alert-box mechanism from the user’s browser (i.e., the alert() function is not part of jQuery but it is built inside the user’s browser). It works well as it relies on the browser instead of relying on R or on a specific JavaScript library. You can use this functionality to replace a call to {shinyalert}: the result is a little less aesthetically pleasing, but that is easier to implement and maintain. var x = prompt(\"this\", \"that\"); this function opens the built-in prompt, which is a text area where the user can input text. With this code, when the user clicks “OK”, the text is stored in the x variable, which you can then send back to R (see further part down this chapter for more info on how to do that). This can replace something like the following: mod &lt;- function() { modalDialog( tagList( textInput(ns(&quot;info&quot;), &quot;Your info here&quot;) ), footer = tagList( modalButton(&quot;Cancel&quot;), actionButton(ns(&quot;ok&quot;), &quot;OK&quot;) ) ) } observeEvent(input$show, { showModal(mod()) }) observeEvent(input$ok, { removeModal() }) $('#id').css('color', 'green'); / document.getElementById(\"demo\").style.color = \"green\"; changes the CSS attributes of the selected element(s). Here, we are switching to green on the #id element. $(\"#id\").text(\"this) / document.getElementById(\"id\").innerText = \"this\"; changes the text content to “this”. This can be used to replace output$ui &lt;- renderUI({ if (this){ tags$p(&quot;First&quot;) } else { tags$p(&quot;Second&quot;) } }) $(\"#id\").remove(); / var elem = document.querySelector('#some-element'); elem.parentNode.removeChild(elem); completely removes the element from the DOM. It can be used as a replacement for shiny::removeUI(), or as a conditional UI. 18.2.2 Where to put them - Back to JavaScript Events OK, now that we have got some ideas about JS code that can be used in Shiny, where do we put them? HTML and JS have a concept called events, which are… well events that happen when the user manipulates the webpage: when the user clicks, hovers (the mouse goes over an element), presses the keyboard… All these events can be used to trigger a JavaScript function. Here are some examples of adding JavaScript functions to DOM events: +onclick The onclick attribute can be added straight inside the HTML tag when possible: tags$button( &quot;Show&quot; onclick = &quot;$(&#39;#plot&#39;).show()&quot; ) Or with shiny::tagAppendAttributes: plotOutput( &quot;plot&quot; ) %&gt;% tagAppendAttributes( onclick = &quot;alert(&#39;hello world&#39;)&quot; ) Here is for example a small Shiny app that implements this behavior: library(shiny) library(magrittr) ui &lt;- function(request){ fluidPage( plotOutput( &quot;plot&quot; ) %&gt;% tagAppendAttributes( onclick = &quot;alert(&#39;iris plot!&#39;)&quot; ) ) } server &lt;- function(input, output, session){ output$plot &lt;- renderPlot({ plot(iris) }) } shinyApp(ui, server) You can find a real Life example of this tagAppendAttributes in the {tidytuesday201942} app: R/mod_dataviz.R#L109, where the click on the plot generates the creation of a Shiny input (we will see this below) That, of course, works well with very small JavaScript code. For longer JavaScript code, you can write a function inside and external file, and add it to your app. In {golem}, this works by launching the add_js_file(\"name\"), which will create a .js file. The JavaScript file is then automatically linked in your application. This, for example, could be: In inst/app/www/script.js function alertme(id){ // Asking information var name = prompt(&quot;Who are you?&quot;); // Showing an alert alert(&quot;Hello &quot; + name + &quot;! You&#39;re seeing &quot; + id); } Then in R plotOutput( &quot;plot&quot; ) %&gt;% tagAppendAttributes( onclick = &quot;alertme(&#39;plot&#39;)&quot; ) Inside this inst/app/www/script.js, you can also attach a new behavior with jQuery to one or several elements. For example, you can add this alertme / onclick behavior to all plots of the app: function alertme(id){ var name = prompt(&quot;Who are you?&quot;); alert(&quot;Hello &quot; + name + &quot;! You&#39;re seeing &quot; + id); } /* We&#39;re adding this so that the function is launched only when the document is ready */ $(function(){ // Selecting all Shiny plots $(&quot;.shiny-plot-output&quot;).on(&quot;click&quot;, function(){ /* Calling the alertme function with the id of the clicked plot */ alertme(this.id); }); }); Then, all the plots from your app will receive this on-click event52. Note that there is a series of Shiny events which are specific to Shiny but that can be used just like the one we have just seen: function alertme(){ var name = prompt(&quot;Who are you?&quot;); alert(&quot;Hello &quot; + name + &quot;! Welcome to my app&quot;); } $(function(){ // Waiting for Shiny to be connected $(document).on(&#39;shiny:connected&#39;, function(event) { alertme(); }); }); See JavaScript Events in Shiny for the full list of JavaScript events available in Shiny. 18.3 JavaScript &lt;-&gt; Shiny communication Now that we have seen some client-side optimization, i.e. R does not do anything with these events when they happen (in fact R is not even aware they happened), let’s now see how we can make these two communicate with each other. 18.3.1 From R to JavaScript Calling JS from the server side (i.e from R) is done by defining a series of CustomMessageHandler: these are functions with one argument that can then be called using the session$sendCustomMessage() method from the server side. Or if you are using {golem}, using the invoke_js() function. You can define them using this skeleton: $( document ).ready(function() { Shiny.addCustomMessageHandler(&#39;fun&#39;, function(arg) { }) }); This skeleton is the one generated by golem::add_js_handler(\"plop\"). Then, it can be called from server-side with: session$sendCustomMessage(&quot;fun&quot;, list()) # OR golem::invoke_js(&quot;fun&quot;, ...) Note that the list() argument from your function will be converted to JSON, and read as such from JavaScript. In other words, if your have an argument called x, and you call the function with list(a = 1, b = 12), then in JavaScript you will be able to use x.a and x.b. For example: In inst/app/www/script.js: Shiny.addCustomMessageHandler(&#39;computed&#39;, function(mess) { alert(&quot;Computed &quot; + mess.what + &quot; in &quot; + mess.sec + &quot; secs&quot;); }) Then in R: observe({ deb &lt;- Sys.time() # Do the computation for id Sys.sleep( sample(1:5, 1) ) session$sendCustomMessage( &quot;computed&quot;, list( what = &quot;plop&quot;, sec = round(Sys.time() - deb) ) ) }) 18.3.2 From JavaScript to R How to do the other way around (from JavaScript to R)? Shiny apps, in the browser, contain an object called Shiny, which can be used to send values to R, by creating an InputValue. For example, with: Shiny.setInputValue(&quot;rand&quot;, Math.random()) you will bind an input that can be caught from the server side with: observeEvent( input$rand , { print( input$rand ) }) This Shiny.setInputValue can of course be used inside any JavaScript function. Here is a small example wrapping some of the things we have seen previously: In inst/app/www/script.js function alertme(){ var name = prompt(&quot;Who are you?&quot;); alert(&quot;Hello &quot; + name + &quot;! Welcome to my app&quot;); Shiny.setInputValue(&quot;username&quot;, name) } $(function(){ // Waiting for Shiny to be connected $(document).on(&#39;shiny:connected&#39;, function(event) { alertme(); }); $(&quot;.shiny-plot-output&quot;).on(&quot;click&quot;, function(){ /* Calling the alertme function with the id of the clicked plot */ Shiny.setInputValue(&quot;last_plot_clicked&quot;, this.id); }); }); These events (getting the user name and the last plot clicked), can then be caught from the server side with: observeEvent( input$username , { cli::cat_rule(&quot;User name:&quot;) print(input$username) }) observeEvent( input$last_plot_clicked , { cli::cat_rule(&quot;Last plot clicked:&quot;) print(input$last_plot_clicked) }) Which will give: &gt; golex::run_app() Loading required package: shiny Listening on http://127.0.0.1:5495 ── User name: ───────────────────────────────────────────────────── [1] &quot;Colin&quot; ── Last plot clicked: ───────────────────────────────────────────── [1] &quot;plota&quot; ── Last plot clicked: ───────────────────────────────────────────── [1] &quot;plopb&quot; Important note: if you are using modules, you will need to pass the namespacing of the id to be able to get it back from the server. This can be done using the session$ns function, which comes by default in any golem-generated module. In other words, you will need to write something like: $( document ).ready(function() { Shiny.addCustomMessageHandler(&#39;whoareyou&#39;, function(arg) { var name = prompt(&quot;Who are you?&quot;) Shiny.setInputValue(arg.id, name); }) }); mod_my_first_module_ui &lt;- function(id){ ns &lt;- NS(id) tagList( actionButton( ns(&quot;showname&quot;), &quot;Enter your name&quot; ) ) } mod_my_first_module_server &lt;- function(input, output, session){ ns &lt;- session$ns observeEvent( input$showname , { session$sendCustomMessage( &quot;whoareyou&quot;, list( id = ns(&quot;name&quot;) ) ) }) observeEvent( input$name , { cli::cat_rule(&quot;Username is:&quot;) print(input$name) }) } Another thing to note about this id creation is that you can generate id that are not defined in R beforehand. For example, let’s create the code below: library(shiny) ui &lt;- function(){ tagList( h3(&quot;No input in R&quot;) ) } server &lt;- function( input, output, session ){ observeEvent( input$notfromr , { print(input$notfromr) }) } shinyApp(ui, server) Then, going into your developer console and typing Shiny.setInputValue(\"notfromr\", Math.random()) will print a random number in your console, event if this input wasn’t defined in your UI function. 18.4 About {golem} js functions {golem} comes with a series of JavaScript functions that you can call from the server. These functions are added by default with golem::activate_js() in app_ui. Then they are called with golem::invoke_js(\"function\", \"element\"). This element can be one of a series of elements (most of the time scalar elements) which can be used to select the DOM node you want to interact with. It can be a full jQuery selector, an id or a class. Note that you can pass multiple elements, with invoke_js ... parameters 18.4.1 golem::invoke_js() showid &amp; hideid, showclass &amp; hideclass show and hide elements using their id or class golem::invoke_js(&quot;showid&quot;, ns(&quot;plot&quot;)) showhref &amp; hidehref hide and show a link by trying to match the href content golem::invoke_js(&quot;showhref&quot;, &quot;panel2&quot;) clickon click on the element, note that you have to use the full jQuery selector show &amp; hide show and hide elements, using the full jQuery selector See ?golem::activate_js for a full list of built-in functions. 18.5 Learn more about JavaScript If you want to interact straight from R with NodeJS (JavaScript in the terminal), you can try the {bubble} package. Be aware that you will need to have a working NodeJS installation on your machine. It can be installed from GitHub remotes::install_github(&quot;ColinFay/bubble&quot;) You can use in RMarkdown chunks, by setting the {knitr} engine: bubble::set_node_engine() Or straight in the command line with: node_repl() Want to learn more? Here is a list of external resources to learn more about JavaScript: 18.5.1 Shiny &amp; JavaScript We have written an online, freely available book about Shiny &amp; JavaScript: JavaScript 4 Shiny - Field Notes JavaScript for Shiny Users, companion website to the rstudio::conf(2020) workshop. Build custom input objects Packaging JavaScript code for Shiny Communicating with Shiny via JavaScript 18.5.2 JavaScript basics Mozilla JavaScript w3schools JavaScript Free Code Camp JavaScript For Cats Learn JS 18.5.3 jQuery jQuery Learning Center w3schools jQuery 18.5.4 Intermediate / advanced JavaScript Eloquent JavaScript You Don’t Know JS Yet "]
]
